,abstract,Field
0,"The existence of black hole horizons has not been strictly proven
observationally, and indeed it may not be possible to do so. However,
alternatives may be established by the observation of gravitational wave echoes
that probe possible near-horizon structure. These echoes are proposed to be
generated in exotic compact objects that are horizonless and feature a
partially reflecting ""wall"" inside their light rings, creating a cavity in
which gravitational perturbations may echo, while leaking out through the
angular momentum barrier with each pass. The characteristic signature of echoes
is a comb of nearly evenly spaced spectral resonances. While approximately
true, deviations from this simple picture can lead to severe observational
signal losses. In this paper, we explore such subtleties with the latest
results for echo sourcing and geometry. A physically motivated echo model is
then developed as a sum over Lorentzian spectral lines, parametrized by
functions of the horizon frame frequency and the size of the cavity. Our final
spectrum is a function of only the mass and spin of the black hole, as well as
the UV scale of the near-horizon physics. We then apply this model in a search
for echoes in the gravitational wave event with the loudest ringdown signal in
LIGO/Virgo, i.e. GW190521. We interpret our findings as a measurement of the
fractional energy in post-merger echoes equal to $E_{echoes} / E_{GR} = 8.9 \pm
4.5\%$, where the uncertainty range represents the 90% credible region. The
robustness of this result is tested against noise backgrounds and simulated
injections, and we find that a signal persists through modifications to the
model and changes in the data search.",0
1,"It is widely believed that Hawking radiation originates from excitations near
the horizons of black holes. However, Giddings proposed that the Hawking
radiation spectrum that characterizes evaporating semi-classical black holes
originates from a quantum atmosphere, which extends beyond the horizon of a
black hole. Although several research projects have been conducted in this
field, they have not yet taken into account the effect of R\'enyi entropy. In
the present article, we will therefore consider the effect of R\'enyi entropy
on Hawking radiation power. We assume that if the effect of R\'enyi entropy is
very small, we suggest that the Hawking radiation should originate from the
quantum atmosphere which extends beyond the black hole's horizon for finite
dimensions. That is, that Giddings' suggestion is the more likely of the above
possibilities. However, for infinite dimensions, both suggestions are equally
credible. We briefly consider the very large effect of R\'enyi entropy on
Hawking radiation power as well. We find that if the effect of R\'enyi entropy
is very large and {\omega}/T_{BH} is very small, then the power spectral
density S_R is proportional to the power spectral density S_{BH}.",0
2,"Serial Femtosecond Crystallography at the X-ray Free Electron Laser (XFEL)
sources enabled the imaging of the catalytic intermediates of the oxygen
evolution reaction of Photosystem II. However, due to the incoherent transition
of the S-states, the resolved structures are a convolution from different
catalytic states. Here, we train Decision Tree Classifier and K-mean clustering
models on Mn compounds obtained from the Cambridge Crystallographic Database to
predict the S-state of the X-ray, XFEL, and CryoEm structures by predicting the
Mn's oxidation states in the oxygen evolving complex (OEC). The model agrees
mostly with the XFEL structures in the dark S1 state. However, significant
discrepancies are observed for the excited XFEL states (S2, S3, and S0) and the
dark states of the X-ray and CryoEm structures. Furthermore, there is a
mismatch between the predicted S-states within the two monomers of the same
dimer, mainly in the excited states. The model suggests that improving the
resolution is crucial to precisely resolve the geometry of the illuminated
S-states to overcome the noncoherent S-state transition. In addition,
significant radiation damage is observed in X-ray and CryoEM structures,
particularly at the dangler Mn center (Mn4). Our model represents a valuable
tool for investigating the electronic structure of the catalytic metal cluster
of PSII to understand the water splitting mechanism.",0
3,"The compositional, as well as structural asymmetries in Janus transition
metal dichalcogenides (J-TMDs) and their van der Waals heterostructures (vdW
HSs), induce an intrinsic Rashba spin-splitting. We investigate the variation
of band-gaps and Rashba parameters in three different Janus heterostructures
having AB-stacked MoXY/WXY (X, Y = S, Se, Te; X$\neq$Y) geometry with Y-Y
interface, using first-principles calculations. We consider the effect of
external electric field and in-plane biaxial strain in tuning the strength of
the intrinsic electric field, which leads to remarkable modifications of the
band-gap and the Rashba spin-splitting. In particular, it is found that the
positive applied field and compressive in-plane biaxial strain can lead to a
notable increase in the Rashba spin-splitting of the valence bands about the
$\Gamma$-point. Moreover, our \textit{ab-initio} calculations reveal the
existence of a type-II band alignment in these heterostructures, which remains
robust under positive external field and biaxial strain. These suggest novel
ways of engineering the electronic, optical, and spin properties of J-TMD van
der Waals heterostructures holding a huge promise in spintronic and
optoelectronic devices.",0
4,"We revisit the problem of the two-dimensional spatiotemporal linear stability
of viscoelastic slender jets obeying linear Phan-Thien-Tanner (PTT) stress
constitutive equation, and investigate the role of finite stresses on the
elasto-capilliary stability of the Beads-on-a-String (BOAS) structure
(structures which include the formation of very thin filament between drops)
and identify the regions of topological transition of the advancing jet
interface, in the limit of low to moderate Ohnesorge number ($Oh$) and high
values of Weissenberg number ($We$). The Briggs idea of analytic continuation
\{previously elucidated in the nonaffine response regime [Bansal, Ghosh and
Sircar, ``Spatiotemporal linear stability of viscoelastic free shear flows:
Nonaffine response regime'', Phys. Fluids {\bf 33}, 054106 (2021)]\} is
deployed to classify regions of temporal stability and absolute and convective
instabilities, as well as evanescent modes, and the results are compared with
previously conducted experiments for viscoelastic filaments. The impact of the
finite stresses are evident in the form of strain-hardening ensuing in lower
absolute growth rates, relatively rapid drainage and finite-time pinch-off. The
phase diagrams reveal the influence of (a) capillary force stabilization at
infinitesimally small values of $Oh$, and (b) inertial stabilization at
significantly larger values of $Oh$.",0
5,"At the KATRIN experiment, the electron neutrino mass is inferred from the
shape of the $\beta$-spectrum of tritium. Important systematic effects in the
Windowless Gaseous Tritium Source (WGTS) of the experiment include the energy
loss by electron scattering, and the extended starting potential. In the WGTS,
primary high-energy electrons from $\beta$-decay produce an extended secondary
spectrum of electrons through various atomic and molecular processes including
ionization, recombination, cluster formation and scattering. This electron
spectrum plays a role in understanding electron energy loss processes, but also
for the simulation of plasma processes. These simulations will then provide an
insight on the starting potential. Here, a Monte Carlo approach is used to
model the electron spectrum for a given magnetic and electric field
configuration. The spectrum is evaluated at different positions within the
WGTS, which allows for a direct analysis of the spectrum close to rear wall and
detector end of the experiment. Alongside electrons, also ions are tracked by
the simulation.",0
6,"The study of 2-dimensional surfaces of constant curvature constitutes a
beautiful branch of geometry with well-documented ties to the mathematical
physics of integrable systems. A lesser known, but equally fascinating, fact is
its connection to 2-dimensional gravity; specifically Jackiw-Teitelboim (JT)
gravity, where the connection manifests through a coordinate choice that
roughly speaking re-casts the gravitational field equations as the sine-Gordon
equation. In this language many well-known results, such as the JT-gravity
black hole and its properties, were understood in terms of sine-Gordon solitons
and their properties. In this brief note, we revisit these ideas in the context
of some of the recent exciting developments in JT-gravity and, more generally,
low-dimensional quantum gravity and speculate on how some of these new ideas
may be similarly understood.",0
7,"As is common with the collection of astronomical data, signals are frequently
dominated by noise. However, when performing FTs of light curves, re-binning
data can improve the signal-to-noise ratio (SNR) at lower frequencies. Using
data collected from the Kepler space telescope, we sequentially re-binned data
three times to investigate the SNR improvement of lower frequency (< 17
microHz) variability in white dwarf KIC 8626021. We found that the SNR at
approximately 5.8 microHz greatly improved through this process, and we
postulate that this frequency is linked to the rotation of KIC 8626021.",0
8,"We investigate the basic properties of Hawking radiation for spherical
solutions in $D = 4$ double field theory. We give the expression of the Hawking
temperature for the solution and then discuss the results of various limits. We
find that for all these limits only Schwarzschild solution and F-JNW solution
can generate Hawking radiation. Moreover, we obtain the lower bound on greybody
factors $\sigma_l(\omega)$ for the spherical solutions in $D = 4$ double field
theory. In particular, we calculate the bound on greybody factors
$\sigma_l(\omega)$ for F-JNW solution. For F-JNW solution, $\sigma_l(\omega)$
monotonically increases with the increase of $a(b)$ for fixed $b(a)$.",0
9,"The relation between genuine multipartite entanglement in the pure state of a
collection of N qubits and the nonclassical correlations in its two-qubit
subsystems is studied. Quantum discord is used as the quantifier of
nonclassical correlations in the subsystem while the generalised geometric
measure (GGM) [Phys. Rev. A. 81, 012308 (2010)] is used to quantify global
entanglement in the N-qubit state. While no definite discernible dependence
between the two can be found for randomly generated global states, for those
with additional structure like weighted graph states we find that local discord
is indicative of global multipartite entanglement. Global states that admit
efficient classical descriptions like stabilizer states furnish an exception in
which despite multipartite entanglement, nonclassical correlation is absent in
two qubit subsystems. We discuss these results in the context of mixed state
quantum computation where nonclassical correlation is considered a candidate
resource that enables exponential speedup over classical computers.",0
10,"We present the Olsson.wl Mathematica package which aims to find linear
transformations for some classes of multivariable hypergeometric functions. It
is based on a well-known method developed by P. O. M. Olsson in J. Math. Phys.
5, 420 (1964) in order to derive the analytic continuations of the Appell $F_1$
double hypergeometric series from the linear transformations of the Gauss
$_2F_1$ hypergeometric function. We provide a brief description of Olsson's
method and demonstrate the commands of the package, along with examples. We
also provide a companion package, called ROC2.wl and dedicated to the
derivation of the regions of convergence of double hypergeometric series. This
package can be used independently of Olsson.wl.",0
11,"We study the robustness of the synchronization of coupled phase oscillators.
When fluctuations of phase differences in lines caused by disturbance exceed a
certain threshold, the state cannot return to synchrony thus leading to
desynchronization. Our main result is the deviation of explicit formulas of a
variance matrix that characterizes the severity of these fluctuations. We
highlight the utility of these results in two general problems: vulnerable line
identification and network design. We find that the vulnerability of lines can
be encoded by the cycle space of graphs. It is analytically shown that a line
in large-size cycles is more vulnerable than those in small-size cycles and
adding a new line or increasing coupling strength of a line reduces the
vulnerability of the lines in any cycle including this line, while it does not
affect the vulnerability of the other lines.",0
12,"The modelling of human crowd behaviors offers many challenging questions to
science in general. Specifically, the social human behavior consists of many
physiological and psychological processes which are still largely unknown. To
model reliably such human crowd systems with complex social interactions,
stochastic tools play an important role for the setting of mathematical
formulations of the problems. In this work, using the description based on an
exclusion principle, we study a statistical-mechanics-based lattice gas model
for active-passive population dynamics with an application to human crowd
behaviors. We provide representative numerical examples for the evacuation
dynamics of human crowds, where the main focus in our considerations is given
to an interacting particle system of active and passive human groups.
Furthermore, our numerical results show that the communication between active
and passive humans strongly influences the evacuation time of the whole
population even when the ""faster-is-slower"" phenomenon is taken into account.
Finally, to provide an additional inside into the problem, a stationary state
of our model is analyzed via current representations and heat map techniques.",0
13,"Hysteresis is a general phenomenon regularly observed in measurements of
various materials properties such as magnetism, elasticity, capillary pressure,
adsorption, battery voltage etc. Usually, the hysteretic behaviour is an
intrinsic property that cannot be avoided or circumvented in dynamic operation
of the system. Here we show, however, that at least as regards the hysteretic
behaviour of phase-separating battery materials, one can enter (deeply) into
the hysteretic loop in specific, yet realistic, transient operating conditions.
Within the hysteretic loop a (significant) portion of particle population
resides in an intraparticle phase separated state. Interestingly, the
transition to the more conventional interparticle phase separation state found
outside the hysteretic loop is very slow. Further, we establish a direct
interrelation between the intraparticle phase separated electrode state and
altered electric response of the electrode, which significantly impacts DC and
AC characteristics of the battery. The experimental evidence of entering the
hysteretic loop and the resulting altered response of the battery are explained
based on thermodynamic reasoning, advanced modelling and insightful
experiments. We believe that the understanding of this phenomenon will help
optimise the diagnostics and monitoring of batteries, while also providing
pertinent motivation for the enhancement of battery design and performance.",0
14,"In this study, we demonstrate sustainable and green nanotechnology for
room-temperature synthesis of H$_x$WO$_3$ ($0<x<0.5$) via a novel reaction
pathway induced by mechanical energy. A simple mixture of monoclinic WO$_3$
powder and polyolefin (polypropylene) is used to obtain H$_x$WO$_3$
nanoparticles that show high crystallinity even through high-energy ball
milling synthesis. The composite of H$_x$WO$_3$ nanoparticles and nanocarbon
by-products exhibit unique optoelectronic properties along with outstanding
enhancement of photocatalytic performance in the decomposition of azo-dye water
pollutants under visible light. The formation mechanism of the obtained
functional material is also discussed. The findings of this study provide
insights into the limitations for mass production of H$_x$WO$_3$ nanoparticles,
such as a specific setup for electrochemical reactions and precious metal
catalysis.",0
15,"$^{222}Rn$ emanating from environmental dust constitutes an important
background component for many low-energy, low-rate experiments. Radon emanation
rates from dust and rock, thus, are important for experiment planning. In this
paper, we report measured radon emanation fractions (defined here as the ratio
of the transient to the total radon progeny activity of a sample) for five
types of dust differing in grain size and composition. This data was obtained
using high-purity germanium detectors (HPGe), measuring emanated and
non-emanated $^{222}Rn$ progeny activities and their temporal change. The range
of observed radon emanation fractions ranges from $3.5 \pm 1.9\%$ and $16.1 \pm
0.84\%$. The impact of the water, contained in the dust, might have on the
emanation fraction was evaluated and found to be small. The data presented here
do not show a clear correlation between dust particle size and emanation
fraction, as hypothesized when starting this study. Our measurement results are
compared to expectations of radon emanation models.",0
16,"We establish the relation of partition functions of conformal higher spin
fields on Weyl equivalent spaces in $d=4$ dimension. We express the partition
function of Weyl graviton and conformal higher spin fields as an integral over
characters on $S^1\times AdS_3$, $S^4$, and $AdS_4$. We observe that the
partition function of conformal higher spins on hyperbolic cylinders differs
from the partition function on $S^4$ by the `edge' contribution. The
logarithmic coefficient obtained from the character integral of the partition
function of conformal higher spins on $AdS_4$ is the half of that obtained from
the partition function on $S^4$. We evaluate the entanglement entropy and the
conformal dimension of the twist operator from the partition function on the
hyperbolic cylinder. The conformal dimension of the co-dimension two twist
operator enables us to find a linear relation between Hofman-Maldacena
variables which we use to show the non-unitarity of the theory. We observe that
the spectrum of the quasinormal modes of conformal higher spins obtained from
the bulk character contains additional distinct states compared to the spectrum
of unitary massless higher spin fields.",0
17,"We revisit the constraints on primordial black holes (PBHs) in the mass range
$10^{13}-10^{18}$ g by comparing the 100\,keV-5\,GeV gamma-ray background with
isotropic flux from PBH Hawking radiation (HR). We investigate three effects
that may update the constraints on the PBH abundance; i) reliably calculating
the secondary spectra of HR for energy below 5\,GeV, ii) the contributions to
the measured isotropic flux from the Galactic PBH HR and that from annihilation
radiation due to evaporated positrons, iii) inclusion of astrophysical
background from gamma-ray sources. The conservative constraint is significantly
improved by more than an order of magnitude at $2\times10^{16}$g$\lesssim
M\lesssim 10^{17}$g over the past relevant work, where the effect ii is
dominant. After further accounting for the astrophysical background, more than
a tenfold improvement extends to a much wider mass range $10^{15}$g$\lesssim
M\lesssim 2\times 10^{17}$g.",0
18,"Recent proposals towards non-local thermoelectric voltage-based thermometry,
in the conventional dual quantum dot set-up, demand an asymmetric step-like
system-to-reservoir coupling around the ground states for optimal operation
(Physica E, 114, 113635, 2019). In addition to such demand for unrealistic
coupling, the sensitivity in such a strategy also depends on the average
measurement terminal temperature, which may result in erroneous temperature
assessment. In this paper, I propose non-local current based thermometry in the
dual dot set-up as a practical alternative and demonstrate that in the regime
of high bias, the sensitivity remains robust against fluctuations of the
measurement terminal temperature. Proceeding further, I propose a non-local
triple quantum dot thermometer, that provides an enhanced sensitivity while
bypassing the demand for unrealistic step-like system-to-reservoir coupling and
being robust against fabrication induced variability in Coulomb coupling. In
addition, I show that the heat extracted from (to) the target reservoir, in the
triple dot design, can also be suppressed drastically by appropriate
fabrication strategy, to prevent thermometry induced drift in reservoir
temperature. The proposed triple dot setup thus offers a multitude of benefits
and could potentially pave the path towards the practical realization and
deployment of high-performance non-local ``sub-Kelvin range"" thermometers.",0
19,"On the 17th of August, 2017 came the simultaneous detections of GW170817, a
gravitational wave that originated from the coalescence of two neutron stars,
along with the gamma-ray burst GRB170817A, and the kilonova counterpart
AT2017gfo. Since then, there has been much excitement surrounding the study of
neutron star mergers, both observationally, using a variety of tools, and
theoretically, with the development of complex models describing the
gravitational-wave and electromagnetic signals. In this work, we improve upon
our pipeline to infer kilonova properties from observed light-curves by
employing a Neural-Network framework that reduces execution time and handles
much larger simulation sets than previously possible. In particular, we use the
radiative transfer code POSSIS to construct 5-dimensional kilonova grids where
we employ different functional forms for the angular dependence of the
dynamical ejecta component. We find that incorporating an angular dependence
improves the fit to the AT2017gfo light-curves by up to ~50% when quantified in
terms of the weighted Mean Square Error.",0
20,"General covariance is a crucial notion in the study of field theories in
curved spacetime. A field theory defined with respect to a semi-Riemannian
metric is generally covariant if two metrics which are related by a
diffeomorphism produce equivalent physics. From a purely mathematical
perspective, this suggest that we try to understand the quotient stack of
metrics modulo diffeomorphism. We'll use the language of groupoids to do this
concretely. Then we'll inspect the tangent complex of this stack at a fixed
metric, which when shifted up by one defines a differential graded Lie algebra.
By considering the action of this Lie algebra on the observables for the
Batalin-Vilkovisky free scalar field theory, we recover a novel expression of
the stress-energy tensor for that example. We'll describe how this construction
nicely encapsulates but also broadens the usual presentation in the physics
literature and discuss applications of the formalism.",0
21,"Energy distribution of electrons in the plasma sustained by the
electron-cyclotron resonance (ECR) discharge has a complicated shape as a
function of various parameters that still remains unknown. Meanwhile, it is an
important plasma characteristic. Some methods and approaches give the
possibility to estimate or measure the properties of the distributions of the
electrons lost from the plasma. One of them, similar to the ion mass
spectrometry, was used in this work to obtain such distributions in the
non-classical continuous ECR ion source with high (up to $ 50-100$ W/cm$ ^ 3 $)
energy input for the first time along with bremsstrahlung spectra. For certain
parameters, a threshold-like regime was discovered, which comprised of the
bursts of energetic electrons, bremsstrahlung and, supposedly, the development
of kinetic instabilities.",0
22,"This is the third paper on hadronic light front wave functions (LFWFs). We
derive a light front Hamiltonian from first principles, using the key features
of the QCD vacuum at low resolution. In the first approximation, it gives
transverse oscillator and longitudinal harmonic modes, and yields the correct
Regge trajectories. For heavy quarkonia, we compare its spectrum to that
obtained from the usual Schroedinger equation in the rest frame. We use the
same approach for light quarks, and investigate the role of confinement and
chiral symmetry breaking in the quark-antiquark sector. We then study spin-spin
and spin-orbit mixing, resulting in e.g. quadrupole moments of vector mesons.
For the light mesons, we show how to extend the famed t$^\prime$Hooft
interaction to the light front, which solves the U(1) problem and helps produce
a light pion. We use the ensuing light front wavefunctions, to derive the
pertinent parton distribution functions, parton amplitudes and low energy
constants.",0
23,"We have derived the analytic solutions of dissipative relativistic spin
hydrodynamics with Gubser expansion. Following the standard strategy of
deriving the solutions in a Gubser flow, we take the Weyl rescaling and obtain
the energy-momentum and angular momentum conversation equations in the
$dS_{3}\otimes\mathbb{R}$ space-time. We then derive the analytic solutions of
spin density, spin potential and other thermodynamic in
$dS_{3}\otimes\mathbb{R}$ space-time and transform them back into Minkowski
space-time $\mathbb{R}^{3,1}$. In the Minkowski space-time, the spin density
and spin potential including the information of radial expansion decay as
$\sim\tau^{-1}$ and $\sim\tau^{-1/3}$, with $\tau$ being proper time,
respectively. Surprisingly,in a Gubser flow the spin potential decays slower
than thermal vorticity and shear tensor. It implies that the evolution of spin
potential may be important in the future studies of spin polarization.
Moreover, we observe the non-vanishing spin corrections to the energy density
and other dissipative terms in the Belinfante form of dissipative spin
hydrodynamics. Our results can also be used as test beds for future simulations
of relativistic dissipative spin hydrodynamics.",0
24,"This note gives an explicit description of conditional measures for the
determinantal point process with the Bergman kernel.",0
25,"In Quantum secret sharing we can share both quantum and classical secrets
with a quantum resource. In this article we study the problem of revocation of
quantum secret shared by the dealer with two shareholders in a three party
scenario. In the existing secret sharing protocols there are no means by which
the dealer can retrieve back the secret once he/she finds all the share holders
to be dishonest. Our protocol makes a significant advancement in solving this
problem by designing strategy in bringing back the secret in the worst possible
situation when all the shareholders/receivers are dishonest . In our proposed
strategy the dealer also possesses a quantum share of the secret which empowers
the dealer to bring back the secret even after sharing is done. However the
protocol along with the revocation process also ensures the normal
reconstruction at the share holder's location when they are honest. This
advantage comes with the expense of extra one qubit on dealer's side and
consequently we require a four qubit resource to start with for 1-dealer and
2-share holder's scenario. Here in this article we not only give the
description of our protocol but also give an example where our protocol is
working with the help of a four qubit entangled state. We also explicitly found
out the range of parameter for the input state for which the protocol will be
successful.",0
26,"In quantum chaos, spectral statistics generally follows the predictions of
Random Matrix Theory (RMT). A notable exception is given by scar states, that
enhance probability density around unstable periodic orbits of the classical
system, therefore causing significant deviations of the spectral density from
RMT expectations. In this work, the problem is considered of both RMT-ruled and
scarred chaotic systems coupled to an opening. In particular, predictions are
derived for the spectral density of a chaotic Hamiltonian scattering into a
single- or multiple channels. The results are tested on paradigmatic quantum
chaotic maps on a torus. The present report develops the intuitions previously
sketched in [D. Lippolis, EPL 126 (2019) 10003].",0
27,"We propose a diagrammatic Monte Carlo approach for general spin-boson models,
which can be regarded as a generalization of the strong-coupling expansion for
fermionic impurity models. The algorithm is based on a self-consistently
computed three-point vertex and a stochastically sampled four-point vertex, and
achieves convergence to the numerically exact result in a wide parameter
regime. The performance of the algorithm is demonstrated with applications to a
spin-boson model representing an emitter in a waveguide. As a function of the
coupling strength, the spin exhibits a delocalization-localization crossover at
low temperatures, signaling a qualitative change in the real-time relaxation.
In certain parameter regimes, the response functions of the emitter coupled to
the electromagnetic continuum can be described by an effective Rabi model with
appropriately defined parameters. We also discuss the spatial distribution of
the photon density around the emitter.",0
28,"In search of materials with three-dimensional flat band dispersions, using
{\em ab-initio} computations, we investigate how topological phases evolve as a
function of hydrostatic pressure and uniaxial strain in two types of
superlattices: HgTe/CdTe and HgTe/HgSe. In short-period HgTe/CdTe
superlattices, our analysis unveils the presence of isoenergetic nodal lines,
which could host strain-induced three-dimensional flat bands at the Fermi level
without requiring doping, when fabricated, for instance, as core-shell
nanowires. In contrast, HgTe/HgSe short-period superlattices are found to
harbor a rich phase diagram with a plethora of topological phases. Notably, the
unstrained superlattice realizes an ideal Weyl semimetal with Weyl points
situated at the Fermi level. A small-gap topological insulator with multiple
band inversions can be obtained by tuning the volume: under compressive
uniaxial strain, the material transitions sequentially into a Dirac semimetal
to a nodal-line semimetal, and finally into a topological insulator with a
single band inversion.",0
29,"Along several sight lines within the Milky Way ArH+ has been ubiquitously
detected with only one detection in extragalactic environments, namely along
two sight lines in the red shift z=0.89 absorber towards the lensed blazar PKS
1830-211. Being formed in predominantly atomic gas by reactions between Ar+,
which were initially ionised by cosmic rays and molecular hydrogen, ArH+ has
been shown to be an excellent tracer of atomic gas as well as the impinging
cosmic-ray ionisation rates. In this work, we attempt to extend the
observations of ArH+in extragalactic sources to examine its use as a tracer of
the atomic interstellar medium (ISM) in these galaxies. We report the detection
of ArH+ towards two luminous nearby galaxies, NGC 253 and NGC 4945, and the
non-detection towards Arp 220 observed using the SEPIA660 receiver on the APEX
12 m telescope. In addition, the two sidebands of this receiver allowed us to
observe the NKaKc=1_1,0-1_0,1 transitions of another atomic gas tracer p-H2O+
at 607.227 GHz with the ArH+ line, simultaneously. We modelled the optically
thin spectra of both species and compared their observed line profiles with
that of other well-known atomic gas tracers such as OH+ and o-H2O+ and diffuse
and dense molecular gas tracers HF and CO, respectively. By further assuming
that the observed absorption from the ArH+, OH+, and H2O+ molecules are
affected by the same flux of cosmic rays, we investigate the properties of the
different cloud layers based on a steady-state analysis of the chemistry of
these three species.",0
30,"This work presents an analysis of a hidden charmed pentaquark candidate state
with double strange quark in its quark content. The investigation is performed
in two parts, which provide the mass prediction of the considered state and its
partial decay width for the $P_{css}\rightarrow \Xi^0 J/\psi$ channel. For the
analyses, two-point and three-point QCD sum rule methods are applied to get the
mass and the width, respectively. The mass for this candidate state is obtained
as $m_{P_{css}}=4600 \pm 155$~MeV with corresponding current coupling constant
$\lambda_{P_{css}}=(0.81 \pm 0.21)\times 10^{-3}$~GeV$^6$. These results are
used in the analysis of the partial width of this state for the decay
$P_{css}\rightarrow \Xi^0 J/\psi$. For this decay, the width is obtained as
$\Gamma=9.52\pm 2.85 $~MeV. These results may shed light on the future
experimental searches in which such types of states are probed and may provide
information to discriminate between such possible observations.",0
31,"It is generally accepted that the dynamical mean field theory (DMFT) gives a
good solution of the Holstein model, but only in dimensions greater than two.
Here we show that the DMFT, which becomes exact in the weak coupling and in the
atomic limit, provides an excellent numerically cheap approximate solution for
the spectral function of the Holstein model in the whole range of parameters
even in one dimension. To establish this, we made a detailed comparison with
the spectral functions that we obtained using newly developed momentum-space
numerically exact hierarchical equations of motion (HEOM) method, which yields
electronic correlation functions directly in real time. We crosschecked these
conclusions with our path integral quantum Monte Carlo results and with the
available numerically exact results from the literature.",0
32,"The opportunities afforded by near-term quantum computers to calculate the
ground-state properties of small molecules depend on the structure of the
computational ansatz as well as the errors induced by device noise. Here we
investigate the behavior of these noisy quantum circuits using numerical
simulations to estimate the accuracy and fidelity of the prepared quantum
states relative to the ground truth obtained by conventional means. We
implement several different types of ansatz circuits derived from unitary
coupled cluster theory for the purposes of estimating the ground-state energy
of Sodium Hydride using the variational quantum eigensolver algorithm. We show
how relative error in the energy and the fidelity scale with the levels of
gate-based noise, the inter-molecular configuration, the ansatz circuit depth,
and the parameter optimization methods.",0
33,"We explore the role of the dilaton field on higher derivative supergravity
within the framework of Double Field Theory and use it to fix the Lorentz non
covariant field redefinitions connecting the metric and dilaton fields with the
duality multiplets.",0
34,"The study of the Higgs boson properties offers compelling perspectives for
testing the effects of physics beyond the Standard Model and has deep
implications for the LHC program and future colliders. Accurate determinations
of the Higgs boson properties can provide us with a distinctively precise
picture of the Higgs sector, set tight bounds and predict ranges for the values
of new physics model parameters. In this paper, we discuss the constraints on
supersymmetry which can be derived by a determination of the Higgs boson mass
and couplings. We quantify these constraints by using scans of the 19-parameter
space of the so-called phenomenological minimal supersymmetric Standard Model.
The fraction of scan points that can be excluded by the Higgs measurements is
studied for the coupling measurement accuracies obtained in LHC Run 2 and
expected for the HL-LHC program and $e^+e^-$ colliders and contrasted with
those derived from missing transverse energy searches at the LHC and from dark
matter experiments.",0
35,"Controlling the evolution of nonequilibrium systems to minimize dissipated
heat or work is a key goal for designing nanodevices, both in nanotechnology
and biology. Progress in computing optimal protocols has thus far been limited
to either simple systems or near-equilibrium evolution. Here, we present an
approach for computing optimal protocols based on automatic differentiation.
Our methodology is applicable to complex systems and multidimensional protocols
and is valid arbitrarily far from equilibrium. We validate our method by
reproducing theoretical optimal protocols for a Brownian particle in a
time-varying harmonic trap. We also compute departures from near-equilibrium
behaviour for magnetization reversal on an Ising lattice and for barrier
crossing driven by a harmonic trap, which has been used to represent a range of
biological processes including biomolecular unfolding reactions. Algorithms
based on automatic differentiation outperform the near-equilibrium theory for
far-from-equilibrium magnetization reversal and driven barrier crossing. The
optimal protocol for crossing an energy landscape barrier of 10kT is found to
hasten the approach to, and slow the departure from, the barrier region
compared to the near-equilibrium theoretical protocol.",0
36,"One of the core questions of quantum physics is how to reconcile the unitary
evolution of quantum states, which is information-preserving and
time-reversible, with the second law of thermodynamics, which is neither. The
resolution to this paradox is to recognize that global unitary evolution of a
multi-partite quantum state causes the state of local subsystems to evolve
towards maximum-entropy states. In this work, we experimentally demonstrate
this effect in linear quantum optics by simultaneously showing the convergence
of local quantum states to a generalized Gibbs ensemble constituting a
maximum-entropy state under precisely controlled conditions, while using a new,
efficient certification method to demonstrate that the state retains global
purity. Our quantum states are manipulated by a programmable integrated
photonic quantum processor, which simulates arbitrary non-interacting
Hamiltonians, demonstrating the universality of this phenomenon. Our results
show the potential of photonic devices for quantum simulations involving
non-Gaussian states.",0
37,"Inference of causal relations from data now has become an important field in
artificial intelligence. During the past 16 years, causality analysis (in a
quantitative sense) has been developed independently in physics from first
principles. This short note is a brief summary of this line of work, including
part of the theory and several representative applications.",0
38,"In this work we conduct a numerical search of non-trivial mechanisms, leading
to new tendencies towards long-range ferromagnetic ordering in two-dimensional
materials. For this purpose we employ an original variant of pairwise
infinitesimal spin rotations technique to establish the magnetic transition
temperature as the rigid function of basic crystal's parameters. It favored the
numerical optimization of this function using modified genetic algorithm,
designed to harvest local extrema. It resulted in revealing the moderate
metallicity, accompanied by essential orbital anisotropy, as the prime
configuration, which provides the most favoring conditions to ferromagnetic
ordering, related to double-exchange and superexchange mechanisms.",0
39,"The conventional (numerical) Self-Consistent effective-mass approaches suffer
from convergence failure at ultra-low temperatures (below 4.2 K).
Discontinuities in material properties (e.g., effective-mass, electron
affinity, dielectric constant) can be regarded as the source of such a
shortcoming. This numerical convergence sensitivity limits the application of
Self-Consistent effective-mass approach to study quantum electronic devices
which often operate at ultra-low temperatures. In this article, we develop a
novel Self-Consistent approach based on Cell-Center Finite-Volume (FV-SC)
discretization of the effective-mass Sturm-Liouville Hamiltonian and
generalized Poisson's equation. We apply this approach to simulate the
one-dimensional electron gas (1DEG) formed at the Si-SiO2 interface via a top
gate. We find excellent Self-Consistent convergence from high to extreme low
(as low as 50 mK) temperatures. We further examine the solidity of FV-SC method
by changing external variables such as the electrochemical potential and the
accumulative top gate voltage. Finally, our approach allows for counting
electron-electron interactions and we find that the electron-electron
interactions can affect the subband properties of 1DEG significantly. Our
results demonstrate that our FV-SC approach is a powerful tool to solve
effective-mass Hamiltonian.",0
40,"Two new high-entropy ceramics (HECs) in the weberite and fergusonite
structures, along with unexpected formation of ordered pyrochlore phases with
ultrahigh-entropy compositions and an abrupt pyrochlore-weberite transition,
are discovered in a 21-component oxide system. While the Gibbs phase rule
allows 21 equilibrium phases, nine out of the 13 compositions examined possess
single HEC phases (with ultrahigh ideal configurational entropies: ~2.7kB per
cation or higher on one sublattice in most cases). Notably,
(15RE1/15)(Nb1/2Ta1/2)O4 possess a single monoclinic fergusonite (C2/c) phase
and (15RE1/15)3(Nb1/2Ta1/2)1O7 form a single orthorhombic (C2221) weberite
phase, where 15RE1/15 represents
Sc1/15Y1/15La1/15Pr1/15Nd1/15Sm1/15Eu1/15Gd1/15Tb1/15Dy1/15Ho1/15Er1/15Tm1/15Yb1/15Lu1/15.
Moreover, a series of eight
(15RE1/15)2+x(Ti1/4Zr1/4Ce1/4Hf1/4)2-2x(Nb1/2Ta1/2)xO7 specimens all exhibit
single phases, where a pyrochlore-weberite transition occurs within 0.75 < x <
0.8125. This cubic-to-orthorhombic transition does not change the
temperature-dependent thermal conductivity appreciably, as the amorphous limit
may have already been achieved in the ultrahigh-entropy 21-component oxides.
These discoveries expand the diversity and complexity of HECs, towards
many-component compositionally complex ceramics (CCCs) and ultrahigh-entropy
ceramics.",0
41,"In this paper, we consider the correspondence between the tachyon dark energy
model and the Tsallis holographic dark energy scenario in an FRW universe.We
demonstrate the Tsallis holographic description of tachyon dark energy in an
FRW universe and reconstruct the potential and basic results of the dynamics of
the scalar field which describe the tachyon cosmology. In a flat universe, in
the tachyon model of Tsallis holographic dark energy, independent of the
existence of interaction between dark energy and matter or not, \dot{T}^2 must
always be zero. Therefore, the equation of state {\omega}_D is always -1 in
such a flat universe. For a non-flat universe, \dot{T}^2 cannot be zero so that
{\omega}_D=-1 which cannot be used to explain the origin of the cosmological
constant. \dot{T}^2 monotonically decreases with the increase in \cos(R_h/a)
and \cosh(R_h/a) for different {\delta}'s. In particular, for an open universe,
\dot{T}^2 is always larger than zero, while for a closed universe, \dot{T}^2 is
always smaller than zerowhich is physically invalid. In addition, we conclude
that with the increase in \cos(R_h/a) and \cosh(R_h/a), \dot{T}^2 always
decreases monotonically irrespective of the value of b^2.",0
42,"The electronic and the optical properties of metallic nitride (MN) monolayers
are studied using a DFT formalism. In most of these monolayers, the electron
density of the metallic atoms is much higher than that of the nitride atoms,
and ionic, covalent, and metallic bonds are found in M-N bonds, resulting in
fascinating electronic and optical properties. The optical band gap is varied
from almost $0.0$ to $3.0$~eV for the MN monolayers depending on the bond type
between the metallic and the nitride atoms, as well as the contribution of the
type of orbitals around the Fermi energy. The optical properties such as the
dielectric function, the excitation spectra, the refractive index, the
reflectivity, and the optical conductivity of MN monolayers are calculated. The
excitation energy and static dielectric constant are found to be inversely
proportional to the band gap at low photon energy. The MN monolayers with a
large band gap have good visible light functionality, while the MN monolayers
with a lower band gap are found to be active in the infrared region.
Furthermore, it is shown that the optical properties of MN monolayers show a
strong anisotropy with respect to the polarization of the incoming light.
Consequently, our results for the optical properties of MN monolayers show that
they could be beneficial in optoelectronic device applications.",0
43,"We study phase transitions in five-dimensional Einstein Gravity with a
negative cosmological constant, coupled to a Skyrme matter field. These
transitions are topological generalizations of the Hawking-Page transition
between thermal Anti de Sitter (AdS) spacetime and an AdS black hole. Phases
are characterized by a topological number associated with the Skyrme field
configuration. Depending on that topological number and on the Skyrme coupling
strength, there occur transitions between those phases at two, one, or no
value(s) of the temperature. Through the holographic (AdS/CFT) correspondence,
these solutions are dual to topologically non-trivial states in a conformal
field theory (CFT) with an SU(2)-symmetry, which support either confined or
deconfined (quasi-)particles at strong coupling. We compare to similar known
phase transitions, and discuss potential applications to confinement in
topological phases of condensed matter and the quark-gluon plasma.",0
44,"Large hydrocarbon fuels are used for ground and air transportation and will
be for the foreseeable future. Despite their extensive use, turbulent
combustion of large hydrocarbon fuels, remains relatively poorly understood and
difficult to predict. A key parameter when burning these fuels is the turbulent
consumption speed; the velocity at which fuel and air are consumed through a
turbulent flame front. Such information can be useful as a model input
parameter and for validation of modeled results. In this study, turbulent
consumption speeds were measured for three jet-like fuels using a premixed
turbulent Bunsen burner. The burner was used to independently control
turbulence intensity, unburned temperature, and equivalence ratio. Each fuel
had similar heat releases (within 2%), laminar flame speeds (within 5-15 %),
and adiabatic flame temperatures. Despite this similarity, for constant Re_D
and turbulence intensity, A2 (i.e., jet-A) has the highest turbulent flame
speeds and remains stable (i.e., without tip quenching) at lower {\phi} than
the other fuels evaluated. In contrast the C1 fuel, which contains no
aromatics, has the slowest turbulent flame speeds and exhibits tip quenching at
higher {\phi} then the other fuels. C1 was the most sensitive to the influence
of turbulence, as evidenced by this fuel having the largest ratio of turbulent
to laminar flame speeds. The C1 fuel had the highest stretch sensitivity, in
general, as indicated by calculated Markstein numbers. This work shows that
turbulent flame speeds and tip stability of multi-component large hydrocarbon
fuels can be sensitive to the chemical class of its components. The results
from the current work indicate that caution may be needed when using
alternative or surrogate fuels to replicate conventional fuels, especially if
the alternative fuels are missing chemical classes of fuels that influence
stretch sensitivities.",0
45,"The Hebbian unlearning algorithm, i.e. an unsupervised local procedure used
to improve the retrieval properties in Hopfield-like neural networks, is
numerically compared to a supervised algorithm to train a linear symmetric
perceptron. We analyze the stability of the stored memories, which naturally
maps the problem into a constraint satisfaction problem. Basins of attraction
obtained by the Hebbian unlearning technique are found to be comparable in size
to those obtained in the symmetric perceptron, while the two algorithms are
found to converge in the same region of Gardner's space of interactions, having
followed similar learning paths. A geometric interpretation of Hebbian
unlearning is proposed to explain its optimal performances.",0
46,"The unsteady hydrodynamics of two in-phase pitching foils arranged in
side-by-side (parallel) configurations is examined for a range of Strouhal
number and separation distance. Three distinct vortex patterns are identified
in the Strohual number-separation distance phase maps, which include separated
wake, merged wake, and transitional-merged wake. Furthermore, a novel model is
introduced based on fundamental flow variables including velocity, location,
and circulation of dipole structures to quantitatively distinguish vortex
patterns in the wake. The physical mechanism of wake merging process is also
elucidated. When an oscillating foil experiences the jet deflection phenomenon,
secondary structures shed from the primary street traverse in the other
direction by making an angle with its parent vortex street. For parallel foils,
secondary structures from the vortex street of the lower foil interact with the
primary vortex street of the upper foil under certain kinematic conditions.
This interaction triggers the wake merging process by influencing circulation
of coherent structures in the upper part of the wake. It is unveiled that
merging of the wakes leads to enhancements in propulsive efficiency by
increasing thrust generation without a significant alteration in power
requirements. These are attributed to the formation of a high-momentum jet by
the merged vortex street, which possesses significantly larger circulation due
to the amalgamation of the vortices, and major alterations in the evolution of
leading edge vortices. Thus, flow physics that are thoroughly explored here are
crucial in providing novel insights for future development of flow control
techniques for efficient designs of bio-inspired underwater propulsors.",0
47,"We present a model of the Cassini state of Mercury that comprises an inner
core, a fluid core and a mantle. Our model includes inertial and gravitational
torques between interior regions, and viscous and electromagnetic (EM) coupling
at the boundaries of the fluid core. We show that the coupling between
Mercury's interior regions is sufficiently strong that the obliquity of the
mantle spin axis deviates from that of a rigid planet by no more than 0.01
arcmin. The mantle obliquity decreases with increasing inner core size, but the
change between a large and no inner core is limited to 0.015 arcmin. EM
coupling is stronger than viscous coupling at the inner core boundary and, if
the core magnetic field strength is above 0.3 mT, locks the fluid and solid
cores into a common precession motion. Because of the strong gravitational
coupling between the mantle and inner core, the larger the inner core is, the
more this co-precessing core is brought into an alignment with the mantle, and
the more the obliquity of the polar moment of inertia approaches that expected
for a rigid planet. The misalignment between the polar moment of inertia and
mantle spin axis increases with inner core size, but is limited to 0.007
arcmin. Our results imply that the measured obliquities of the mantle spin axis
and polar moment of inertia should coincide at the present-day level of
measurement errors, and cannot be distinguished from the obliquity of a rigid
planet.",0
48,"Imaging is indispensable for nearly every field of science, engineering,
technology, and medicine. However, measurement noise and stochastic distortions
pose fundamental limits to accessible spatiotemporal information despite
impressive tools such as SIM, PALM/STORM, and STED microscopy. How to combat
this challenge ideally has been an open question for decades. Inspired by a
""virtual gain"" technique to compensate losses in metamaterials, ""active
convolved illumination"" has been recently proposed to significantly improve the
signal-to-noise ratio, hence data acquisition. In this technique, the light
pattern of the object is superimposed with a correlated auxiliary pattern, the
function of which is to reverse the adverse effect of noise and random
distortion based on their spectral characteristics. Despite enormous
implications in statistics, an experimental realization of this novel technique
has been lacking to date. Here, we present the first experimental
demonstration. We find that the active convolved illumination does not only
boost the resolution limit and image contrast, but also the resistance to pixel
saturation. The results confirm the previous theories and opens up new horizons
in a wide range of disciplines from atmospheric sciences, seismology, biology,
statistical learning, and information processing to quantum noise beyond the
fundamental boundaries.",0
49,"Being the most massive binary black hole merger event observed to date,
GW190521 is in a class of its own. The exceptionally loud ringdown of this
merger makes it an ideal candidate to search for gravitational wave echoes, a
proposed smoking gun for the quantum structure of black hole horizons. We
perform an unprecedented multi-pronged search for echoes via two
well-established and independent pipelines: a template-based search for
stimulated emission of Hawking radiation, or Boltzmann echoes, and the
model-agnostic coherent WaveBurst (cWB) search. Stimulated Hawking radiation
from the merger is expected to lead to post-merger echoes at horizon mode
frequency of $\sim 50$ Hz (for quadrupolar gravitational radiation), repeating
at intervals of $\sim 1$ second, due to partial reflection off Planckian
quantum structure of the horizon. A careful analysis using dynamic nested
sampling yields a Bayesian evidence of $ 7\pm 2$ (90% confidence level) for
this signal following GW190521, carrying an excess of $10^{+9}_{-7}\%$ in
gravitational wave energy, relative to the main event. Similarly, the
reconstructed waveform of the first echo in cWB carries an energy excess of
$13^{+16}_{-7}\%$. Accounting for the ""look-elsewhere"" effects, we estimate a
p-value for false detection probability of $5.1 \times 10^{-3}$ (or
2.6$\sigma$) using cWB pipeline, although the verdict on the co-localization of
the post-merger echo and the main event in the sky is inconclusive. While the
current evidence for stimulated Hawking radiation does not reach the gold
standard of $5\sigma$, our findings are in line with expectations for
stimulated Hawking radiation at current detector sensitivities. The next
generation of gravitational wave observatories can thus draw a definitive
conclusion on the quantum nature of black hole horizons.",0
50,"The extreme conditions in Neutron Stars make them ideal test facilities for
fundamental interactions. A Neutron Star can capture Dark Matter via
scattering. As a result of the scattering, Dark Matter kinetic energy is
transferred to the star. An observational consequence of this can be the
warming of old neutron stars to near-infrared temperatures. Different
approximations or simplifications have been applied to previous analyses of the
capture process. In this article, we summarise a significantly improved
treatment of Dark Matter capture, which properly accounts for all relevant
physical effects over a wide range of Dark Matter masses. Among them are
gravitational focusing, a fully relativistic scattering treatment, Pauli
blocking, neutron star opacity and multiple scattering effects. This paper
cites general expressions that allow the capture rate to be computed
numerically, and simplified expressions for particular types of interactions or
mass regimes, which greatly increase the efficiency of computation. As a result
of our method, we are able to model the scattering of Dark Matter from any
neutron star constituent as well as the capture of Dark Matter in other compact
objects. Our results are applied to scattering of Dark Matter from neutrons,
protons, leptons and exotic baryons.",0
51,"Frequency of warm and cold winters in the North Eurasian regions is analyzed
from long-term data, depending on El Nino phenomena of different types.
Frequencies of extremely warm and extremely cold winters for North Eurasian
regions in different phases of El Ni\~no phenomena are compared. Potential
predictability of anomalous winters in the El Ni\~no, La Ni\~na and neutral
phases is estimated.",0
52,"Grover's quantum search algorithm provides a quadratic quantum advantage over
classical algorithms across a broad class of unstructured search problems. The
original protocol is probabilistic, returning the desired result with
significant probability on each query, but in general, requiring several
iterations of the algorithm. We present a modified version to return the
correct result with certainty without having user control over the quantum
search oracle. Our deterministic, two-parameter ""D2p"" protocol utilizes
generalized phase rotations replacing the phase inversions after a standard
oracle query. The D2p protocol achieves a 100% success rate in no more than one
additional iteration compared to the optimal number of steps in the original
Grover's search enabling the same quadratic speed up. We also provide a
visualization using the Bloch sphere for enhanced geometric intuition.",0
53,"We investigate the microscopic dynamics at the initial stage of photoinduced
phase transitions in tetrathiafulvalene-$p$-chloranil by exact diagonalization.
We first show that the one-dimensional extended Peierls-Hubbard model exhibits
a neutral phase with small ionicity and negligible dimerization and an ionic
phase with moderate ionicity and dimerization. Besides these phases, we find a
doubly-ionized phase with strong dimerization that we call the ""dipole"" phase.
These ground state phases are characterized by various order parameters and the
Zak phase, which is relevant to electronic polarization. We further explore the
microscopic dynamics of the three phases triggered by short monocycle optical
pulses. The electronic order parameters and lattice displacement suggest that
the neutral-ionic, ionic-neutral, and dipole-ionic transitions are induced.
Furthermore, clear spectroscopic changes are observed in the time-dependent
spectral density and pump-probe conductivity. A detailed analysis of the
spectroscopy demonstrates the generation of coherent charge-transfer strings
via multiphoton absorption and the crucial roles of the excited states and the
metastable ground state at the new lattice position for the ultrafast dynamics.",0
54,"This work is devoted to study the behavior of massless particles within the
context of curved spacetime. In essence, we investigate the consequences of the
scale factor $C(\eta)$ of the Friedmann-Robertson-Walker metric in the
Einstein-aether formalism to study photon-like particles. To do so, we consider
the system within the canonical ensemble formalism in order to derive the
following thermodynamic state quantities: spectral radiance, Helmholtz free
energy, pressure, entropy, mean energy and the heat capacity. Moreover, the
correction to the \textit{Stefan--Boltzmann} law and the equation of states are
also provide. Particularly, we separate our study within three distinct cases,
i.e., $s=0,p=0$; $s=1,p=1$; $s=2,p=1$. In the first one, the results are
derived \textit{numerically}. Nevertheless, for the rest of the cases, all the
calculations are accomplished \textit{analytically} showing explicitly the
dependence of the scale factor $C(\eta)$ and the Riemann zeta function
$\xi(s)$. Furthermore, our analyses are accomplished in general taking into
account three different regimes of temperature of the universe, i.e., the
inflationary era ($T=10^{13}$ GeV), the electroweak epoch ($T=10^{3}$ GeV) and
the cosmic microwave background ($T=10^{-13}$ GeV).",0
55,"We present our recent high precision calculations (Phys. Rev. D102 (2020)
no.5, 054512 and JHEP 04 (2021) 044, JHEP 21 (2020) 004) of the first moment of
nucleon isovector polarized, unpolarized and transversity distributions, i.e.,
momentum fraction, helicity and transversity moment, respectively. We use the
standard method for the calculation of these moments (via matrix elements of
twist two operators), and carry out a detailed analysis of the sources of
systematic uncertainty, in particular of excited state contributions. Our
calculations have been performed using two different lattice setups
(Clover-on-HISQ and Clover-on-Clover), each with several ensembles. They give
consistent results that are in agreement with global fit analyses.",0
56,"We employ a ghost model of interacting dark energy to obtain the equation of
state \omega for ghost energy density in an FRW universe in complex
quintessence theory. We reconstruct the potential and study the dynamics of the
scalar field that describes complex quintessence cosmology. We perform
\omega-\omega' analysis and stability analysis for both non-interacting and
interacting cases and find that the same basic conclusion as for the real
model, where \omega' = d\omega/dlna. Taking account of the effect of the
complex part and assuming the real part of the quintessence field to be a
""slow-rolling"" field, we conclude that the non-interacting model cannot
describe the real universe since this will lead to fractional energy density
\Omega_D > 1, where \Omega_D can be defined as the ratio of \rho_D to
\rho_{cr}. However, for the interacting case, if we take present \Omega_D =
0.73, then we can determine that b^2 = 0.0849, where b^2 is the interaction
coupling parameter between matter and dark energy. In the real quintessence
model, \Omega_D and b^2 are independent parameters, whereas in the complex
quintessence model, we conclude that there is a relationship between these two
parameters.",0
57,"We report on a search for persistent radio emission from the one-off Fast
Radio Burst (FRB) 20190714A, as well as from two repeating FRBs, 20190711A and
20171019A, using the MeerKAT radio telescope. For FRB 20171019A we also
conducted simultaneous observations with the High Energy Stereoscopic System
(H.E.S.S.) in very high energy gamma rays and searched for signals in the
ultraviolet, optical, and X-ray bands. For this FRB, we obtain a UV flux upper
limit of 1.39x10^-16 erg/cm^-2/s/Amstrong, X-ray limit of ~ 6.6x10^-14
erg/cm^-2/s and a limit on the very-high-energy gamma-ray flux (Phi) (E > 120
GeV) < 1.7 x 10^-12 erg/cm^-2/s. We obtain a radio upper limit of ~15
microJy/beam for persistent emission at the locations of both FRBs 20190711A
and 20171019A, but detect diffuse radio emission with a peak brightness of ~53
microJy/beam associated with FRB 20190714A at z = 0.2365. This represents the
first detection of the radio continuum emission potentially associated with the
host (galaxy) of FRB 20190714A, and is only the third known FRB to have such an
association. Given the possible association of a faint persistent source, FRB
20190714A may potentially be a repeating FRB whose age lies between that of FRB
20121102A and FRB 20180916A. A parallel search for repeat bursts from these
FRBs revealed no new detections down to a fluence of 0.08 Jy ms for a 1 ms
duration burst.",0
58,"This article provides brief guidance for a successful graduate teaching
assistantship at our school. Although the components mentioned in this article
are primarily aimed at mathematics courses designated as the ""Basic Science and
Mathematics"" (BSM) modules with multiple sections, the principle can also be
applied and adapted to other courses and institutions as well.",0
59,"Srinivasa Ramanujan was a great self-taught Indian mathematician, who died a
century ago, at the age of only 32, one year after returning from England.
Among his numerous achievements is the assignment of sensible, finite values to
divergent series, which correspond to Riemann's $\zeta$-function with negative
integer arguments. He hardly left any explanation about it, but following the
few hints that he gave, we construct a direct justification for the best known
example, based on analytic continuation. As a physical application of Ramanujan
summation we discuss the Casimir effect, where this way of removing a divergent
term corresponds to the renormalization of the vacuum energy density, in
particular of the photon field. This leads to the prediction of the Casimir
force between conducting plates, which has now been accurately confirmed by
experiments. Finally we review the discussion about the meaning and
interpretation of the Casimir effect. This takes us to the mystery surrounding
the magnitude of Dark Energy.",0
60,"We propose a transport theory for the kinetic evolution of solar-wind
electrons in the heliosphere. We derive a gyro-averaged kinetic transport
equation that accounts for the spherical expansion of the solar wind and the
geometry of the Parker-spiral magnetic field. To solve our three-dimensional
kinetic equation, we develop a mathematical approach that combines the
Crank--Nicolson scheme in velocity space and a finite-difference Euler scheme
in configuration space. We initialize our model with isotropic electron
distribution functions and calculate the kinetic expansion at heliocentric
distances from 5 to 20 solar radii. In our kinetic model, the electrons evolve
mainly through the combination of the ballistic particle streaming, the
magnetic mirror force, and the electric field. By applying fits to our
numerical results, we quantify the parameters of the electron strahl and core
part of the electron velocity distributions. The strahl fit parameters show
that the density of the electron strahl is around 7% of the total electron
density at a distance of 20 solar radii, the strahl bulk velocity and strahl
temperature parallel to the background magnetic field stay approximately
constant beyond a distance of 15 solar radii, and $\beta_{\parallel s}$ (i.e.,
the ratio between strahl parallel thermal pressure to the magnetic pressure) is
approximately constant with heliocentric distance at a value of about 0.02. We
compare our results with data measured by Parker Solar Probe. Furthermore, we
provide theoretical evidence that the electron strahl is not scattered by the
oblique fast-magnetosonic/whistler instability in the near-Sun environment.",0
61,"Gyroscopic motion explanation in texts is relatively long and requires
reasonable level of comfort with the mathematical tools used. On the other
hand, popular explanation outside academic courses does not explain the
phenomenon and only describes it leaving many to think that it is so weird that
it defies physics. In this paper we offer a simplified and mathematically sound
explanation that can be used in either setting.",0
62,"Over the past decade, fake news and misinformation have turned into a major
problem that has impacted different aspects of our lives, including politics
and public health. Inspired by natural human behavior, we present an approach
that automates the detection of fake news. Natural human behavior is to
cross-check new information with reliable sources. We use Natural Language
Processing (NLP) and build a machine learning (ML) model that automates the
process of cross-checking new information with a set of predefined reliable
sources. We implement this for Twitter and build a model that flags fake
tweets. Specifically, for a given tweet, we use its text to find relevant news
from reliable news agencies. We then train a Random Forest model that checks if
the textual content of the tweet is aligned with the trusted news. If it is
not, the tweet is classified as fake. This approach can be generally applied to
any kind of information and is not limited to a specific news story or a
category of information. Our implementation of this approach gives a $70\%$
accuracy which outperforms other generic fake-news classification models. These
results pave the way towards a more sensible and natural approach to fake news
detection.",0
63,"A set of multipartite orthogonal product states is strongly nonlocal if it is
locally irreducible in every bipartition, which shows the phenomenon of strong
quantum nonlocality without entanglement. It is known that unextendible product
bases (UPBs) can show the phenomenon of quantum nonlocality without
entanglement. Thus it is interesting to investigate the strong quantum
nonlocality for UPBs. Most of the UPBs with the minimum size cannot demonstrate
strong quantum nonlocality. In this paper, we construct a series of UPBs with
different large sizes in $d_A\otimes d_B\otimes d_C$ and $d_A\otimes d_B\otimes
d_C\otimes d_D$ for $d_A, d_B, d_C, d_D\geq 3$, and we also show that these
UPBs have strong quantum nonlocality, which answers an open question given by
Halder \emph{et al.} [Phys. Rev. Lett. \textbf{122}, 040403 (2019)] and Yuan
\emph{et al.} [Phys. Rev. A \textbf{102}, 042228 (2020)] for any possible three
and four-partite systems. Furthermore, we propose an entanglement-assisted
protocol to locally discriminate the UPB in $3\otimes 3\otimes 4$, and it
consumes less entanglement resource than the teleportation-based protocol. Our
results build the connection between strong quantum nonlocality and UPBs.",0
64,"Magnetism is a very relevant subject that permeates our everyday lives.
However, magnetism keeps taking us from surprise to surprise which seems to
indicate that it is a phenomenon not well understood. For example, we found
that bulk amorphous palladium becomes magnetic; so, naturally one should ask,
could defective palladium develop magnetism? In particular, would amorphous
porous palladium become magnetic? Here we show that the answer to that question
is affirmative, this defective topology of Pd is magnetic, with a magnetism
that depends on the amount of sample porosity and on the topology of their
structures. Clearly, if magnetism exists in porous amorphous palladium, this
indicates the possibility of developing light-weight magnets, useful when a
maximized magnetism/weight ratio is demanded, well suited for space and
aeronautical applications.",0
65,"Materials may behave in non-expected ways when subject to unexpected
conditions. For example, when Bi was turned into an amorphous phase
(\textit{a}-Bi) unexpectedly it became a superconductor at temperatures below
$10$ K. We provided an explanation as to why \textit{a}-Bi superconducts and
the crystalline (\textit{c}-Bi) had not been found to do so: we computer
calculated their electronic properties and found that \textit{a}-Bi has a
larger electron density of states, eDoS, at the Fermi surface than
\textit{c}-Bi and this explained the phenomenon. We even predicted an upper
limit for the superconducting $T_c$ of the crystalline phase, which was
experimentally corroborated within the following year. We now decided to
investigate what happens to crystalline (Wyckoff structure) and amorphous Bi
when pressures below the atmospheric are applied (expansion). Here we show that
when expanded, \textit{c}-Bi becomes more metallic, since the eDoS increases
when the volume increases for the Wyckoff structure, while the amorphous eDoS
decreases. If the crystalline structure is maintained its $T_c$ would rise
under expansion, whereas it would diminish for the \textit{a}-Bi. Expansion can
be obtained in the laboratory by chemically etching Bi-based alloys, a process
also known as dealloying, for example.",0
66,"A key feature of integrable systems is that they can be solved to obtain
exact analytical solutions. We show how new models can be constructed through
generalisations of some well known nonlinear partial differential equations
with PT-symmetries whilst preserving integrability. Subsequently, we develop
new methods from well-known ones to obtain exact analytical soliton solutions
for these new systems. The first PT-symmetric generalization we take are
extensions to the complex and multicomplex fields. In agreement with the
reality property present in PT-symmetric non-Hermitian quantum systems, we find
PT-symmetries also play a key role in the reality of conserved charges here. We
then extend our investigations to explore degenerate multi-soliton solutions
for the sine-Gordon and Hirota equations. In particular, we find the usual
time-delays from degenerate soliton solution scattering are time-dependent,
unlike the non-degenerate multi-soliton solutions, and provide a universal
formula to compute the exact time-delay values for the scattering of N-soliton
solutions. Other PT-symmetric extensions of integrable systems we take are of
nonlocal nature, with nonlocalities in space and/or in time, of time crystal
type. Whilst developing new methods for the construction of soliton solutions
for these systems, we find new types of solutions with different parameter
dependence and qualitative behaviour even in the one-soliton solution cases. We
exploit gauge equivalence between the Hirota system with continuous Heisenberg
and Landau-Lifschitz systems to see how nonlocality is inherited from one
system to another and vice versa. Extending investigations to the quantum
regime, we generalize the scheme of Darboux transformations for fully
time-dependent non-Hermitian quantum systems, which allows us to create an
infinite tower of solvable models.",0
67,"Computational hemodynamics is becoming an increasingly important tool in
clinical applications and surgical procedures involving the cardiovascular
system. Aim of this review is to provide a compact summary of state of the art
0D-1D multiscale models of the arterial coronary system, with particular
attention to applications related to cardiac arrhythmias, whose effects on the
coronary circulation remain so far poorly understood. The focus on 0D-1D models
only is motivated by the competitive computational cost, the reliability of the
outcomes for the whole cardiovascular system, and the ability to directly
account for cardiac arrhythmias. The analyzed studies show that cardiac
arrhythmias by their own are able to promote significant alterations of the
coronary hemodynamics, with a worse scenario as the mean heart rate (HR)
increases. The present review can stimulate future investigation, both in
computational and clinical research, devoted to the hemodynamic effects induced
by cardiac arrhythmias on the coronary circulation.",0
68,"As the 125 GeV Higgs becomes disfavored as a portal to the dark sector, one
is motivated to look beyond the SM-Higgs sector, into extended scalar-mediated
portal mechanisms. In this work we consider one interesting possibility of such
extended scalar sector, namely Type X two Higgs doublet with a scalar singlet
dark matter. This model with the advantage of light pseudoscalars also provides
the explanation for the observed anomalous magnetic moment of muon. The dark
matter phenomenology of this model is unique from the other types of two Higgs
doublet models. Therefore, it is quite intriguing to look for signatures
specific to this model at the collider experiments. In this work we take up the
task of finding suitable final states and regions of parameters space that can
be probed at the high-luminosity runs of LHC. We go beyond our rectangular
cut-based approach and use Artificial Neural Network which shows remarkable
improvement in terms of signal significance.",0
69,"In the past decade, tremendous efforts have been made towards understanding
fermionic symmetry protected topological (FSPT) phases in interacting systems.
Nevertheless, for systems with continuum symmetry, e.g., electronic insulators,
it is still unclear how to construct an exactly solvable model with a finite
dimensional Hilbert space in general. In this paper, we give a lattice model
construction and classification for 2D interacting electronic insulators. Based
on the physical picture of $\mathrm{U(1)}_f$-charge decorations, we illustrate
the key idea by considering the well known 2D interacting topological
insulator. Then we generalize our construction to an arbitrary 2D interacting
electronic insulator with symmetry $G_f=\mathrm{U(1)}_f
\rtimes_{\rho_1,\omega_2} G$, where $\mathrm{U(1)}_f$ is the charge
conservation symmetry and $\rho_1, \omega_2$ are additional data which fully
characterize the group structure of $G_f$. Finally we study more examples,
including the full interacting classification of 2D crystalline topological
insulators.",0
70,"Liquid level control is very important in industrial field, where the liquid
level is required, and to prevent overflows. The coupled-tank is a common
system in industrial control processes. The system consists of two tanks
connected together and the liquid flows between them. Tanks contain an inlet
and outlet for each tank. The main principle of controlling this system is to
maintain a constant level of liquid in both tanks when there are an inflow and
outflow of liquid in each tank. To control liquid level in the coupled tank
system, the mathematical model of the system had been derived and evaluated as
a form of linear model. The mathematical model of coupled tank was developed to
apply to both conventional and fuzzy control systems where the dynamic behavior
of the system was considered. When the system had been designed the
corresponding model was implemented in simulation by using Matlab and Simulink
tools.",0
71,"In this work, we consider two heavy Higgs bosons ($H$ and $A$) of about
$2.4\!\sim\!4.8$ TeV in the semi-constrained NMSSM (scNMSSM) in our previous
work. We study them in the $pp\to b\bar{b}H/A \to b\bar{b}t\bar{t}$ channel at
a 100 TeV hardon collider. We calculate their cross section of production and
branching ratios of decay, and compare the signal with the simulation result in
a reference. We finally find that it is hard to detect the heavy Higgs bosons
in this channel at the 100 TeV collider with an integrated luminosity of
$3~{\rm ab}^{-1}$, because their branching ratio to SM particles including
$t\bar{t}$ are seriously suppressed by the decay to SUSY particles. We will add
discussions about detecting in other channels later.",0
72,"The universal fragmentation functions of gluon into the flavored quarkonia
$B_c$ and (polarized) $B_c^*$ are computed within NRQCD factorization
framework, at the lowest order in velocity expansion and strong coupling
constant. It is mandatory to invoke the DGLAP renormalization program to render
the NRQCD short-distance coefficients UV finite in a point-wise manner. The
calculation is facilitated with the sector decomposition method, with the final
results presented with high numerical accuracy. This knowledge is useful to
enrich our understanding toward the large-$p_T$ behavior of $B_c^{(*)}$
production at LHC experiment.",0
73,"In most results concerning bounds on the heat transport in the
Rayleigh-B\'{e}nard convection problem no-slip boundary conditions for the
velocity field are assumed. Nevertheless it is debatable, whether these
boundary conditions reflect the behavior of the fluid at the boundary. This
problem is important in theoretical fluid mechanics as well as in industrial
applications, as the choice of boundary conditions has effects in the
description of the boundary layers and its properties. In fact, different
boundary conditions may inhibit or enhance heat transport. This review presents
a selection of contributions in the theory of rigorous bounds on Nusselt
number, distinguishing and comparing the results for no-slip, free-slip and
Navier-slip boundary conditions.",0
74,"In the Hamiltonian picture, free spin-$1/2$ Dirac fermions on a bipartite
lattice have an $O(4)$ (spin-charge) symmetry. Here we construct an interacting
lattice model with an interaction $V$, which is similar to the Hubbard
interaction but preserves the spin-charge flip symmetry. By tuning the coupling
$V$, we show that we can study the phase transition between the massless
fermion phase at small-$V$ and a massive fermion phase at large-$V$. We
construct a fermion bag algorithm to study this phase transition and find
evidence for it to be second order. Numerical study shows that the universality
class of the transition is different from the one studied earlier involving the
Hubbard coupling $U$. Here we obtain some critical exponents using lattices up
to $L=48$.",0
75,"Quantum computers have an exponential speed-up advantage over classical
computers. One of the most prominent utilities of quantum computers is their
ability to study complex quantum systems in various fields using quantum
computational algorithms. Quantum computational algorithms can be used to study
cosmological systems and how they behave with variations in the different
parameters of the system. Here, we use the variational quantum eigensolver
(VQE) algorithm to simulate the Hawking radiation phenomenon. VQE algorithm is
a combination of quantum and classical computation methods used to obtain the
minimum energy eigenvalue for a given Hamiltonian. Three different custom
ansatzes are used in the VQE algorithm from which the results for the case with
minimum errors are studied. We obtain the plots for temperature and power from
the minimum energy eigenvalue recorded for different values of mass and
distance from the center of the black hole. The final result is then analyzed
and compared against already existing data on Hawking radiation.",0
76,"Light trapping photonic crystal (PhC) patterns on the surface of Si solar
cells provides a novel opportunity to approach the theoretical efficiency limit
of 32.3%, for light-to-electrical power conversion with a single junction cell.
This is beyond the efficiency limit implied by the Lambertian limit of ray
trapping 29%. The interference and slow light effects are harnessed for
collecting light even at the long wavelengths near the Si band-gap. We compare
two different methods for surface patterning, that can be extended to large
area surface patterning: 1) laser direct write and 2) step-&-repeat 5-times
reduction projection lithography. Large area throughput limitations of these
methods are compared with the established electron beam lithography (EBL)
route, which is conventionally utilised but much slower than the presented
methods. Spectral characterisation of the PhC light trapping is compared for
samples fabricated by different methods. Reflectance of Si etched via laser
patterned mask was 7% at visible wavelengths and was comparable with Si
patterned via EBL made mask. The later pattern showed a stronger absorbance
than the Lambertian limit (M.-L. Hsieh et al., Sci. Rep. 10, 11857 (2020)).",0
77,"We uncover a new gravitational-wave production mechanism in cosmological,
first-order, thermal phase transitions. These are usually assumed to proceed
via the nucleation of bubbles of the stable phase inside the metastable phase.
However, if the nucleation rate is sufficiently suppressed, then the Universe
may supercool all the way down the metastable branch and enter the spinodal
region. In this case the transition proceeds via the exponential growth of
unstable modes and the subsequent formation, merging and relaxation of phase
domains. We use holography to follow the real-time evolution of this process in
a strongly coupled, four-dimensional gauge theory. The resulting gravitational
wave spectrum differs qualitatively from that in transitions mediated by bubble
nucleation. We discuss the possibility that the spinodal dynamics may be
preceded by a period of thermal inflation.",0
78,"The prospects of superradiant lasing on the 7.5 kHz wide $^1$S$_0$-$^3$P$_1$
transition in $^{88}$Sr is explored by using numerical simulations of two
systems based on realistic experimental numbers [C. B. Silva et al, Towards a
continuous superradiant laser on the strontium $^1$S$_0$-$^3$P$_0$ transition.
Bulletin of the APS (2021)] [C.-C. Chen et al, Phys. Rev. Applied 12, 044014
(2019)]. One system uses the idea of demonstrating continuous superradiance in
a simple, hot atom beam with high flux [H. Liu et al, Phys. Rev. Lett. 125,
253602 (2020)], and the other system is based on using ultra-cold atoms in a
dipole guide. We find that the hot beam system achieves lasing above 3 $\cdot$
10$^{12}$ atoms/s. It is capable of outputting hundreds of nW and suppressing
cavity noise by a factor of 20-30. The second order Doppler shift causes a
shift in the lasing frequency on the order of 500 Hz. For the cold atom beam we
find that the output power is on the order of hundreds of pW, however the
second order Doppler shift can be neglected, and cavity noise can be suppressed
on the order of a factor 600.",0
79,"We use a laser-based collider to study the interplay between a magnetic
Feshbach resonance and a shape resonance in cold collisions of ultracold Rb
atoms. By exerting control over a parameter space spanned by both collision
energy and magnetic field, we can tune the width of a Feshbach resonance over
several orders of magnitude. We develop a quantum defect theory specialized for
ultracold atomic collisions, and provide a full description and analysis of the
experimental observations.",0
80,"The formation of planetesimals is expected to occur via particle-gas
instabilities that concentrate dust into self-gravitating clumps. Triggering
these instabilities requires the prior pileup of dust in the protoplanetary
disk. Until now, this has been successfully modeled exclusively at the disk's
snowline, whereas rocky planetesimals in the inner disk were obtained only by
assuming either unrealistically large particle sizes or an enhanced global disk
metallicity. However, planetesimal formation solely at the snowline is
difficult to reconcile with the early and contemporaneous formation of iron
meteorite parent bodies with distinct oxidation states and isotopic
compositions, indicating formation at different radial locations in the disk.
Here, by modeling the evolution of a disk with ongoing accretion of material
from the collapsing molecular cloud, we show that planetesimal formation may
have been triggered within the first 0.5 million years by dust pileup at both
the snowline (at approximately 5 au) and the silicate sublimation line (at
approximately 1 au), provided turbulent diffusion was low. Particle
concentration at approximately 1 au is due to the early outward radial motion
of gas and is assisted by the sublimation and recondensation of silicates. Our
results indicate that, although the planetesimals at the two locations formed
about contemporaneously, those at the snowline accreted a large fraction of
their mass (approximately 60 percent) from materials delivered to the disk in
the first few 10,000 yr, whereas this fraction is only 30 percent for the
planetesimals formed at the silicate line. Thus, provided that the isotopic
composition of the delivered material changed with time, these two planetesimal
populations should have distinct isotopic compositions, consistent with
observations.",0
81,"We report the first measurement of the parity-violating elastic electron
scattering asymmetry on 27Al. The 27Al elastic asymmetry is A_PV = 2.16 +- 0.11
(stat) +- 0.16 (syst) ppm, and was measured at <Q^2> =0.02357 +- 0.0001 GeV^2,
<theta_lab> = 7.61 +- 0.02 degrees, and <E_lab> = 1.157 GeV with the Qweak
apparatus at Jefferson Lab. Predictions using a simple Born approximation as
well as more sophisticated distorted-wave calculations are in good agreement
with this result. From this asymmetry the 27Al neutron radius R_n = 2.89 +-
0.12 fm was determined using a many-models correlation technique. The
corresponding neutron skin thickness R_n-R_p = -0.04 +- 0.12 fm is small, as
expected for a light nucleus with a neutron excess of only 1. This result thus
serves as a successful benchmark for electroweak determinations of neutron
radii on heavier nuclei. A tree-level approach was used to extract the 27Al
weak radius R_w = 3.00 +- 0.15 fm, and the weak skin thickness R_wk - R_ch =
-0.04 +- 0.15 fm. The weak form factor at this Q^2 is F_wk = 0.39 +- 0.04.",0
82,"We investigate the monogamy relations related to the concurrence, the
entanglement of formation, convex-roof extended negativity, Tsallis-q
entanglement and R'enyi-{\alpha} entanglement, the polygamy relations related
to the entanglement of formation, Tsallis-q entanglement and R'enyi-{\alpha}
entanglement. Monogamy and polygamy inequalities are obtained for arbitrary
multipartite qubit systems, which are proved to be tighter than the existing
ones. Detailed examples are presented.",0
83,"Mn$_3$Sn is a fascinating magnetic topological system, showing topological
characteristics within the Kagome lattice network due to the non-vanishing
Berry phase in the momentum space. In this study, for the first time, we show a
large pure room-temperature topological Hall effect (THE) in the $xy$-plane
(0001), while the anomalous Hall effect (AHE) has been noticed in the
$zx$-plane (01$\bar{1}$0) of Mn$_3$Sn. With Fe doping, we can induce a giant
$xy$-plane THE in addition to AHE at low temperatures, while still preserving
the pure room-temperature THE in Mn$_{2.8}$Fe$_{0.2}$Sn. Moreover, the AHE in
the $zx$-plane has been increased with Fe doping. Our studies indicate that the
topological properties are highly anisotropic in these systems. Most
importantly, the large room-temperature pure THE observed in Mn$_3$Sn is quite
promising for the realization of room-temperature topotronic-based applications",0
84,"We study the scaling behavior of the (2+1)-flavor QCD crossover region
towards the chiral limit with smaller-than-physical light quark mass gauge
ensembles, generated using the HISQ fermion discretization. At zero chemical
potential, we study the fluctuations of conserved charges and their
correlations with the chiral condensate, towards the chiral limit. We analyse
the role of universal and regular contributions to the above quantities. We
find a preliminary estimate of the leading curvature coefficient of the chiral
phase transition line using scaling arguments.",0
85,"This paper describes three-dimensional numerical simulations and Radio
Frequency (RF) measurements of wave propagation in microwave-heated magnetized
plasmas of ion sources. Full-wave solution of Maxwell's equations has been
addressed through the Finite Element Method (FEM) commercial software COMSOL.
Our numerical model takes into account the strongly inhomogeneous and
anisotropic magnetized ""cold"" plasma medium. The simulations reproduce the main
features of the wave-plasma interaction of the Flexible Plasma Trap (FPT) that
recently came into operations at INFN-LNS. A two-pins RF probe has been ad-hoc
developed and used as a plasmaimmersed antenna for measuring local wave
electric fields in the FPT device. The measurements of plasma electron density
and RF electric field, performed for different external magnetic field
configuration, allowed a direct comparison with the assumed simulation model.",0
86,"Active glassy systems can be thought of as simple model systems that imitate
complex biological systems. Sometimes, it becomes crucial to estimate the
amount of the activity present in such biological systems, such as predicting
the progression rate of the cancer cells or the healing time of the wound. In
this work, we study a model active glassy system to understand a possible
quantification of the degree of activity from the collective, long-range phonon
response in the system. We find that the four-point dynamic susceptibility,
$\chi_4(t)$ at the phonon timescale, grows with increased activity. We then
show how one can estimate the degree of the activity at such a small timescale
by measuring the growth of $\chi_4(t)$ with changing activity. A detailed
finite size analysis of this measurement, shows that the peak height of
$\chi_4(t)$ at this phonon timescale increases strongly with increasing system
size suggesting a possible existence of an intrinsic dynamic length scale that
grows with increasing activity. Finally, we show that this peak height is a
unique function of effective activity across all system sizes, serving as a
possible parameter for characterizing the degree of activity in a system.",0
87,"In this note, we explain how to prove several basic results about finite
index extensions of irreducible local M\""obius covariant nets in the setting of
Connes fusion.",0
88,"Negative head loss coefficients in branched junctions, has been controversial
for long. Herwig et al. showed, that the cause is a diffusive shear work
exchange. Based on their work, a new junction internal model is described,
while the conventional head loss is named external. The latter is obtained
experimentally, while the first cannot. This fact seems to push back reaching a
practical solution. Conventionally two head `loss' coefficients are required.
However the internal model needs three: two `pure' head loss coefficients and a
work coefficient. Based on previous works, the paper shows that the missing
equation comes from the Minimum Energy Dissipation Principle (MinEDP). The
characteristic differential equation of a T-junction is discovered, which
relates the two `pure' head loss coefficients. A particular case is presented
to illustrate how to obtain the internal model from the external one by using
it. The results are applied to empirical and numerical data from the same
branched junction; Zhu's measurements (external) and Herwig's CFD computations
(internal), respectively. Finally, an example illustrates how powerful this new
method is to analyse a new, recently published, exhaust return duct.",0
89,"Atomic nuclei exhibit deformation, pairing correlations, and rotational
symmetries. To meet these competing demands in a computationally tractable
formalism, we revisit the use of general pair condensates with good particle
number as a trial wave function for even-even nuclei. After minimizing the
energy of the condensate, we project out states with good angular momentum with
a fast projection technique, allowing for general triaxial deformations. To
show applicability, we present example calculations from pair condensates in
several model spaces, and compare against projected Hartree-Fock and full
configuration-interaction shell model calculations. This approach successfully
generates spherical, vibrational and rotational spectra, demonstrating
potential for modeling medium- to heavy-mass nuclei.",0
90,"The reduction procedure in Wiener path integrals for a finite-dimensional
mechanical system with symmetry representing the motion of two interacting
scalar particles on a manifold, which is the product of the total space of the
principal fiber bundle and a vector space, is considered for the case of using
dependent coordinates when describing the reduced motion on the orbit space.
The factorization of the measure in the path integral, which is necessary for
the reduction, was based on the application of a nonlinear filtration equation
from the theory of stochastic processes. The non-invariance of the measure in
the path integral under the reduction is shown. The Jacobian of the reduction
is expressed in terms of the mean curvature of the orbit in the principal fiber
bundle.",0
91,"The Lorentzian type IIB matrix model is a promising candidate for a
non-perturbative formulation of superstring theory. In previous studies, Monte
Carlo calculations provided interesting results indicating the spontaneous
breaking of SO(9) to SO(3) and the emergence of (3+1)-dimensional space-time.
However, an approximation was used to avoid the sign problem, which seemed to
make the space-time structure singular. In this talk, we report our results
obtained by using the complex Langevin method to overcome the sign problem
instead of using this approximation. In particular, we discuss the emergence of
continuous space-time in a new phase, which we discovered recently.",0
92,"For the construction of the Lorentzian path integral for gravity one faces
two main questions: Firstly, what configurations to include, in particular
whether to allow Lorentzian metrics that violate causality conditions. And
secondly, how to evaluate a highly oscillatory path integral over unbounded
domains. Relying on Picard-Lefschetz theory to address the second question for
discrete Regge gravity, we will illustrate that it can also answer the first
question. To this end we will define the Regge action for complexified
variables and study its analytical continuation. Although there have been
previously two different versions defined for the Lorentzian Regge action, we
will show that the complex action is unique. More precisely, starting from the
different definitions for the action one arrives at equivalent analytical
extensions. The difference between the two Lorentzian versions is only realized
along branch cuts which arise for a certain class of causality violating
configurations.
  As an application we discuss the path integral describing a finite evolution
step of the discretized deSitter universe. We will in particular consider an
evolution from vanishing to finite scale factor, for which the path integral
defines the no-boundary wave function.",0
93,"We revisited the problem of ""mixing"" in a gravitational N-body system from
the point of view of the ordering of coarse-grained cells in the one-particle
energy space, here denoted {\it energy ranking preservation} (ERP) analysis. We
investigated a subset of the IllustrisTNG cosmological magnetohydrodynamical
simulations, considering both their full and dark-only versions. For each
simulation, we extracted data from the $4$ most massive (""reference"") FOF
haloes at redshift $z=0$, which were analysed separately. The particle energies
in those haloes were sorted and partitioned into bins in terms of quintiles and
deciles. We then identified such particles at higher redshifts, considering
only if they were residing in FOF haloes, and assigned them to the same
initially defined energy bins. We found evidence of ERP, monotonically
declining as a function of redshift, reference halo mass and particle type. Our
results confirm other indications in the literature of a ""mesoscopic""
constraint, operating at a collective level in the energy space, extending now
this effect also to baryons. In a coarse-grained sense, our results indicate
that, roughly, the most (less) gravitationally bounded masses today were
probably the most (less) bounded ones at redshifts even as high as $z \sim 5$.
Our results suggest a relation between the level of ERP and the time since a
major merger.",0
94,"The Land\'e or $g$-factors of charge carriers are decisive for the
spin-dependent phenomena in solids and provide also information about the
underlying electronic band structure. We present a comprehensive set of
experimental data for values and anisotropies of the electron and hole Land\'e
factors in hybrid organic-inorganic (MAPbI$_3$, MAPb(Br$_{0.5}$Cl$_{0.5}$)$_3$,
MAPb(Br$_{0.05}$Cl$_{0.95}$)$_3$, FAPbBr$_3$,
FA$_{0.9}$Cs$_{0.1}$PbI$_{2.8}$Br$_{0.2}$) and all-inorganic (CsPbBr$_3$) lead
halide perovskites, determined by pump-probe Kerr rotation and spin-flip Raman
scattering in magnetic fields up to 10~T at cryogenic temperatures. Further, we
use first-principles DFT calculations in combination with tight-binding and
$\mathbf k \cdot \mathbf p$ approaches to calculate microscopically the Land\'e
factors. The results demonstrate their universal dependence on the band gap
energy across the different perovskite material classes, which can be
summarized in a universal semi-phenomenological expression, in good agreement
with experiment.",0
95,"Deep neural networks (DNNs) are powerful tools for compressing and distilling
information. Due to their scale and complexity, often involving billions of
inter-dependent internal degrees of freedom, exact analysis approaches often
fall short. A common strategy in such cases is to identify slow degrees of
freedom that average out the erratic behavior of the underlying fast
microscopic variables. Here, we identify such a separation of scales occurring
in over-parameterized deep convolutional neural networks (CNNs) at the end of
training. It implies that neuron pre-activations fluctuate in a nearly Gaussian
manner with a deterministic latent kernel. While for CNNs with infinitely many
channels these kernels are inert, for finite CNNs they adapt and learn from
data in an analytically tractable manner. The resulting thermodynamic theory of
deep learning yields accurate predictions on several deep non-linear CNN toy
models. In addition, it provides new ways of analyzing and understanding CNNs.",0
96,"Extensive air showers created by high-energy particles interacting with the
Earth atmosphere can be detected using imaging atmospheric Cherenkov telescopes
(IACTs). The IACT images can be analyzed to distinguish between the events
caused by gamma rays and by hadrons and to infer the parameters of the event
such as the energy of the primary particle. We use convolutional neural
networks (CNNs) to analyze Monte Carlo-simulated images from the telescopes of
the TAIGA experiment. The analysis includes selection of the images
corresponding to the showers caused by gamma rays and estimating the energy of
the gamma rays. We compare performance of the CNNs using images from a single
telescope and the CNNs using images from two telescopes as inputs.",0
97,"Most of crystalline materials develop an hysteresis on their deformation
curve when a mechanical loading is applied in alternating directions. This
effect, also known as the Bauschinger effect, is intimately related to the
reversibile part of the plastic deformation and controls the materials damage
and ultimately their failure. In the present work, we associate mesoscale
Dislocation Dynamics simulations and Finite Element simulations to identify two
original dislocation mechanisms at the origin of the traction/compression
asymmetry and quantify their impacts on the cyclic behaviour of FCC
single-crystals. After demonstrating that no long-range internal stresses can
be measured in the simulations, careful analysis of the dislocation network
show that the Bauschinger effect is caused by an asymmetry in the stability of
junctions formed from segments whose curvature is determined by the applied
stress, and a significant portion of the stored dislocation segments is easily
recovered during the backward motion of dislocations in previously explored
regions of the crystal. These mechanisms are incorporated into a modified
crystal plasticity framework with few parameters quantified from statistical
analysis of Dislocation Dynamics simulations or from the literature. This
strategy has a real predictive capability and the macroscale results are in
good agreement with most of the experimental literature existing on the
Bauschinger and cyclic deformation of FCC single-crystals. This work provides
valuable mechanistic insight to assist in the interpretation of experiments and
the design of structural components to consolidate their life under cyclic
loading.",0
98,"We study the process $\gamma\gamma\to\pi^0\pi^0$ involving the principle
mechanisms, the structure of its cross section and the role of individual
isoscalar-tensor resonances in the saturation of its energy spectrum.",0
99,"Theoretical explorations have revealed that quantum batteries can exploit
quantum correlation to achieve faster charging, thus promising exciting
applications in future technologies. Using NMR architecture, here we
experimentally investigate various aspects of quantum battery with the help of
nuclear spin systems in star-topology configuration. We first carry out
numerical analysis to study how charging a quantum battery depends on the
relative purity factors of charger and battery spins. By experimentally
measuring the polarization of the battery spin undergoing charging, we estimate
the battery energy and establish the theoretically predicted quantum advantage.
We propose using the quantum advantage, which depends on the entanglement among
chargers and battery, as a measure for estimating the size of the entangled
cluster. We develop a simple iterative method to realize asymptotic charging
avoiding oscillatory behaviour of charging and discharging. Finally, we
introduce a load spin and realize a charger-battery-load circuit and
experimentally demonstrate battery energy consumption after varying durations
of battery storage, for up to two minutes.",0
100,"Lattice QCD plays an increasing role in reducing theoretical uncertainties in
kaon decays. Here we focus on kaon decay channels $K\to \ell\nu_\ell \ell'^+
\ell'^-$, which are closely related to radiative kaon decays since the lepton
pairs $\ell'^+ \ell'^-$ come from virtual photon emission: $K\to \ell\nu_\ell
\gamma^* \to \ell\nu_\ell \ell'^+ \ell'^-$. Compared with real photon emission,
these channels involve more complicated form factors due to the off-shell
photon with possible large momentum transfer, which causes a challenge to
lattice-QCD studies. In this work, we introduce a lattice calculation procedure
for their decay width, which can avoid parameterization of form factors. The
systematic errors in our method are found to be controllable. Infinite volume
reconstruction method is adopted to remove the temporal truncation effects and
reduce finite volume effects from kaon intermediate state. This approach sets
up a bridge between lattice QCD calculation and experimental measurement of
decay width.",0
101,"In this paper we extract from a large-$r$ expansion of the vacuum Einstein's
equations a dynamical system governing the time evolution of an infinity of
higher-spin charges. Upon integration, we evaluate the canonical action of
these charges on the gravity phase space. The truncation of this action to
quadratic order and the associated charge conservation laws yield an infinite
tower of soft theorems. We show that the canonical action of the higher spin
charges on gravitons in a conformal primary basis, as well as conformally soft
gravitons reproduces the higher spin celestial symmetries derived from the
operator product expansion. Finally, we give direct evidence that these charges
form a canonical representation of a $w_{1+\infty}$ loop algebra on the
gravitational phase space.",0
102,"We investigate the cosmology of mini Primordial Black Holes (PBHs) produced
by large density perturbations that collapse during a stiff fluid domination
phase. Such a phase can be realized by a runaway-inflaton model that crosses an
inflection point or a sharp feature at the last stage of inflation. Mini PBHs
evaporate promptly and reheat the early universe. In addition, we examine two
notable implications of this scenario: the possible presence of PBH evaporation
remnants in galaxies and a non-zero residual potential energy density for the
runaway inflaton that might play the role of the dark energy. We specify the
parameter space that this scenario can be realized and we find that a transit
PBH domination phase is necessary due to gravitational wave (GW) constraints. A
distinct prediction of the scenario is a compound GW signal that might be
probed by current and future experiments. We also demonstrate our results
employing an explicit inflation model.",0
103,"Ideal Chern insulating phases arise in two-dimensional systems with broken
time-reversal symmetry. They are characterized by having nearly-flat bands, and
a uniform quantum geometry -- which combines the Berry curvature and quantum
metric -- and by being incompressible. In this work, we analyze the role of the
quantum geometry in ideal Chern insulators aiming to describe transport in
presence of external out-of-plane magnetic and electric fields. We firstly show
that in the absence of external perturbations, novel Berry connections appear
in ideal Chern insulating phases. Secondly, we provide a detailed analysis of
the deformation of the quantum geometry once weak out-of-plane magnetic fields
are switched on. The perturbed Berry curvature and quantum metric provide an
effective quantum geometry, which is analyzed in the insulating regime and
provides an application of our novel connections. The conditions under which
the Girvin-MacDonald-Platzman algebra is realized in this situation are
discussed. Furthermore, an investigation of electrical transport due to the new
effective quantum geometry is presented once an electric field is added.
Restricting to the case of two bands in the metallic regime the quantum metric
appears as measurable quantum mechanical correction in the Hall response. Our
findings can be applied, for instance, to rhombohedral trilayer graphene at low
energies.",0
104,"With the recent progress in observations of astrophysical black holes, it has
become more important to understand in detail the physics of strongly
gravitating horizonless objects. If the objects identified in the observations
are indeed horizonless and ultracompact, high curvature effects may come into
play, and their explorations may be intimately related to new physics beyond
General Relativity (GR). In this paper, we revisit the concept of statistical
thermodynamics in curved spacetime, focusing on self-gravitating compact
systems without event horizons. Differently from the previous studies in this
context, we develop a generic framework with no explicit dependence on the
gravitational field equations, which is then applicable to a general theory of
gravity. Defining the global variables directly from the local counterparts,
the conventional thermodynamics follows for a generic curved spacetime. The key
step is the appropriate identification of thermodynamic volume to ensure the
first law of thermodynamics, which is in general different from the geometric
volume. For demonstration, we consider familiar examples of self-gravitating
gas in GR, where the connection to previous studies becomes clear. We also
discuss 2-2-holes in quadratic gravity, a novel example of black hole mimickers
that features super-Planckian curvatures in the interior. When the physical
mass is treated as the total internal energy, interesting connections to black
hole thermodynamics emerge. We find universal high curvature effects in
thermodynamics for these objects, and the dominant effects happen to be
conveniently encoded in the thermodynamic volume.",0
105,"A trivializing map is a field transformation whose Jacobian determinant
exactly cancels the interaction terms in the action, providing a representation
of the theory in terms of a deterministic transformation of a distribution from
which sampling is trivial. Recently, a proof-of-principle study by Albergo,
Kanwar and Shanahan [arXiv:1904.12072] demonstrated that approximations of
trivializing maps can be `machine-learned' by a class of invertible,
differentiable neural models called \textit{normalizing flows}. By ensuring
that the Jacobian determinant can be computed efficiently, asymptotically exact
sampling from the theory of interest can be performed by drawing samples from a
simple distribution and passing them through the network. From a theoretical
perspective, this approach has the potential to become more efficient than
traditional Markov Chain Monte Carlo sampling techniques, where
autocorrelations severely diminish the sampling efficiency as one approaches
the continuum limit. A major caveat is that it is not yet understood how the
size of models and the cost of training them is expected to scale. As a first
step, we have conducted an exploratory scaling study using two-dimensional
$\phi^4$ with up to $20^2$ lattice sites. Although the scope of our study is
limited to a particular model architecture and training algorithm, initial
results paint an interesting picture in which training costs grow very quickly
indeed. We describe a candidate explanation for the poor scaling, and outline
our intentions to clarify the situation in future work.",0
106,"We study helicity polarization through the (3+1) dimensional relativistic
viscous hydrodynamic models at $\sqrt{s_{NN}}=200$GeV Au+Au collisions. Similar
to the local spin polarization, we consider the helicity polarization beyond
global equilibrium and investigate the contributions induced by thermal
vorticity, shear viscous tensor, and the fluid acceleration. We find that the
local helicity polarization induced by thermal vorticity dominates over other
contributions. It also implies that in the low-energy collisions, the the fluid
vorticity as part of thermal vorticity may play the crucial role to the total
helicity polarization. Such a finding could be useful for probing the local
strength of vorticity in rotational quark gluon plasmas by measuring helicity
polarization. Our simulation confirms the strict space reversal symmetry,
whereas we also compare our numerical results with approximated relations
derived from ideal Bjorken flow. Our studies also provide a baseline for the
future investigation on local parity violation through the correlations of
helicity polarization.",0
107,"We study black hole binary mergers in certain cubic Horndeski theories of
gravity, treating them fully non-linearly. In the regime of validity of
effective field theory, the mismatch of the gravitational wave strain between
Horndeski and general relativity (coupled to a scalar field) can be as large as
$10-13\%$ in the Advanced LIGO mass range. The initial data and coupling
constants are chosen such the theory remains in the weakly coupled regime
throughout the evolution. In all cases that we have explored, we observe that
the waveform in the Horndeski theory is shifted by an amount much larger than
the smallness parameter that controls the initial data. This effect is generic
and may be present in other theories of gravity involving higher derivatives.",0
108,"In their target paper, Bruineberg and colleagues provide us with a timely
opportunity to discuss the formal constructs and philosophical implications of
the free-energy principle. I critically discuss their proposed distinction
between Pearl blankets and Friston blankets. I then critically assess the
distinction between inference with a model and inference within a model in
light of instrumentalist approaches to science.",0
109,"We present a fine-grained approach to identify clusters and perform
percolation analysis in a 2D lattice system. In our approach, we develop an
algorithm based on the linked-list data structure and the members of a cluster
are nodes of a path. This path is mapped to a linked-list. This approach
facilitates unique cluster labeling in a lattice with a single scan. We use the
algorithm to determine the critical exponent in the quench dynamics from Mott
Insulator to superfluid phase of bosons in 2D square optical lattices. The
result obtained are consistent with the Kibble-Zurek mechanism. We also employ
the algorithm to compute correlation lengths using definitions based on
percolation theory. And, use it to identify the quantum critical point of the
Bose Glass to superfluid transition in the disordered 2D square optical
lattices. In addition, we also compute the critical exponent of the transition.",0
110,"We propose the implementation of the Swap Test using a charge qubit in a
double quantum dot. The Swap Test is a fundamental quantum subroutine in
quantum machine learning and other applications for estimating the fidelity of
two unknown quantum states by measuring an auxiliary qubit. Our proposal uses a
controlled three-qubit gate which is natural to quantum-dot charge qubits. It
allows us to implement a Swap Test with a circuit depth of six layers, and an
estimated gate time of less than 3 ns, that is below the coherence time of
double quantum dots. This work paves the way for enhancing the toolbox of
quantum machine learning developments in semiconductor qubits.",0
111,"The latest advances of statistical physics have shown remarkable performance
of machine learning in identifying phase transitions. In this paper, we apply
domain adversarial neural network (DANN) based on transfer learning to studying
non-equilibrium and equilibrium phase transition models, which are percolation
model and directed percolation (DP) model, respectively. With the DANN, only a
small fraction of input configurations (2d images) needs to be labeled, which
is automatically chosen, in order to capture the critical point. To learn the
DP model, the method is refined by an iterative procedure in determining the
critical point, which is a prerequisite for the data collapse in calculating
the critical exponent $\nu_{\perp}$. We then apply the DANN to a
two-dimensional site percolation with configurations filtered to include only
the largest cluster which may contain the information related to the order
parameter. The DANN learning of both models yields reliable results which are
comparable to the ones from Monte Carlo simulations. Our study also shows that
the DANN can achieve quite high accuracy at much lower cost, compared to the
supervised learning.",0
112,"In this paper, we introduce a generalized topological quantum field theory
based on the symmetric monoidal category which we call causal network
condensation since it can be regarded as a generalization of spin network
construction of Baez, Turaev-Viro model and causal set theory. In this paper we
introduce some new concepts, including causal network category, causal network
diagrams, and their gauge transformations. Based on these concepts, we
introduce the nerve functor for the symmetric monoidal category. It can be
regarded as a generalization of the nerve functor for simplicial category.",0
113,"Magnetic frustrations and dimensionality play an important role in
determining the nature of the magnetic long-range order and how it melts at
temperatures above the ordering transition $T_N$. In this work, we use
large-scale Monte Carlo simulations to study these phenomena in a class of
frustrated Ising spin models in two spatial dimensions. We find that the
melting of the magnetic long-range order into an isotropic gas-like paramagnet
proceeds via an intermediate stage where the classical spins remain
anisotropically correlated. This correlated paramagnet exists in a temperature
range $T_N < T < T^\ast$, whose width increases as magnetic frustrations grow.
This intermediate phase is typically characterized by short-range correlations,
however the two-dimensional nature of the model allows for an additional exotic
feature -- formation of an incommensurate liquid-like phase with algebraically
decaying spin correlations. The two-stage melting of magnetic order is generic
and pertinent to many frustrated quasi-2D magnets with large (essentially
classical) spins.",0
114,"We study mode-entanglement in the seniority model, derive analytic formulas
for the one-body reduced density matrix of states with seniority $\nu =
0,\;1,\;2$, and $\nu=3$, and also determine the particle number dependence of
the one-body reduced density matrix for arbitrary seniority. We carry out
numerical calculations for the lightest calcium isotopes and for $^{94}{\rm
Ru}$ nucleus, and investigate the structure of their ground and low energy
yrast states. We investigate the fulfillment of several predictions of the
seniority model regarding the behavior of one-mode entropies, which we compare
with the results of configuration interaction (CI) and density matrix
renormalization group (DMRG) computations. For $^{94}{\rm Ru}$, the seniority
model accounts for the $0g_{9/2}$ mode entropies, but seniority mixing is
important for certain yrast states. Interaction induced quantum fluctuations
decrease the occupation of the $0f_{5/2}$, $1p_{3/2}$ and $1p_{1/2}$ shells,
and amount in finite mode entropies on these shells, too, clearly outside the
scope of the simple $(0g_{9/2})^4$ seniority model. The $0f_{7/2}$ shell based
seniority model is more accurate for the light ${\rm Ca}$ isotopes, but
seniority mixing is substantial for some $^{44}{\rm Ca}$ yrast states, too.",0
115,"The dynamical behaviour of the quantum state of different quantum spin
chains, with designed site dependent interaction strengths, is analyzed when
the initial state belongs to the one excitation subspace. It is shown that the
inhomogeneous chains are able to transfer excitations with near perfect
fidelity. This behaviour is found for two very different spin chain
Hamiltonians. The first one is the ferromagnetic Heisenberg Hamiltonian with
nearest neighbor interactions, the second one describes a chain with long range
anisotropic interactions which are ferromagnetic in the $z$ direction and
antiferromagnetic in the $(x,y)$ plane. It is shown that both designed chains
have in common a partially ordered spectrum and well localized eigenvectors.
This physical trait unifies the description of both kind of systems.",0
116,"Astronomical observations reveal that protoplanetary disks around young stars
commonly have ring- and gap-like structures in their dust distributions. These
features are associated with pressure bumps trapping dust particles at specific
locations, which simulations show are ideal sites for planetesimal formation.
Here we show that our Solar System may have formed from rings of planetesimals
-- created by pressure bumps -- rather than a continuous disk. We model the
gaseous disk phase assuming the existence of pressure bumps near the silicate
sublimation line (at $T \sim$1400~K), water snowline (at $T \sim$170~K), and
CO-snowline (at $T \sim$30~K). Our simulations show that dust piles up at the
bumps and forms up to three rings of planetesimals: a narrow ring near 1~au, a
wide ring between $\sim$3-4~au and $\sim$10-20~au, and a distant ring between
$\sim$20~au and $\sim$45~au. We use a series of simulations to follow the
evolution of the innermost ring and show how it can explain the orbital
structure of the inner Solar System and provides a framework to explain the
origins of isotopic signatures of Earth, Mars and different classes of
meteorites. The central ring contains enough mass to explain the rapid growth
of the giant planets' cores. The outermost ring is consistent with dynamical
models of Solar System evolution proposing that the early Solar System had a
primordial planetesimal disk beyond the current orbit of Uranus.",0
117,"We propose a general statistical mechanics framework for the collective
motion of animals. The framework considers the principle of maximum entropy,
the interaction, boundary, and desire effects, as well as the time-delay
effect. These factors provide the ability to describe and solve dynamic and
non-equilibrium problems under this framework. We show that the Vicsek model,
the social force model, and some of their variants can be considered special
cases of this framework. Furthermore, this framework can be extended to the
maximum caliber setting. We demonstrate the potential of this framework for
model comparisons and parameter estimations by applying the model to observed
data from a field study of the emergent behavior of termites. Finally, we
demonstrate the flexibility of the framework by simulating some collective
moving phenomena for birds and ants.",0
118,"Particle physics models with extra dimensions of space (EDS's) and branes
shed new light on naturalness of the electroweak and flavor sectors, with a
rich TeV scale phenomenology. Usually, such models are formulated in the
framework of local effective field theories. The scope of this article is to
propose new model building issues with EDS and branes, arising in the framework
of string-inspired infinite derivative quantum field theories (QFT's), which
are perturbatively exorcized (ghost-free) and intrinsically nonlocal above some
ultraviolet (UV) scale. First, a toy model with bulk and brane scalar fields is
presented. It is shown that a 4D field localized on a $\delta$-like 3-brane is
still delocalized in the bulk on a small distance from the brane position, with
a penetration depth given by the nonlocal length scale. Fields localized on
such distant fuzzy branes are thus allowed to interact directly with suppressed
couplings, which is impossible in a local QFT. It is also argued that above the
nonlocal scale, it should be much more difficult to probe the Kaluza-Klein (KK)
tower in an experiment, because of the UV-opaqueness of such infinite
derivative QFT's. In this way, a shadow EDS is obtained if the nonlocal and
KK-scales are close to each other, such that the effects of the KK-excitations
are screened compared to a local QFT. After that, directions for model building
are given. By using this new possibility of fuzzy branes, a new realization of
split fermions in an EDS is presented, allowing to naturally generate flavor
hierarchies. Moreover, with a warped EDS, the usual warp transmutation of a
brane mass term is revisited, where it is shown that the nonlocal scale is also
redshifted. This may provide a new way to solve or reduce the little hierarchy
problem related to the Higgs boson mass.",0
119,"Q-balls -- whether in the single-field or multi-field context -- are usually
studied in theories containing only one stabilising symmetry. However, this is
not the most general scenario. In this paper, we study a class of theories with
multiple symmetries. We consider both the traditional thin- and thick-wall
limits of these theories, deriving sufficient conditions for existence in the
latter case. Moreover, we also introduce a new state that could exist in this
class of theory -- a cored Q-ball. We show that this new state can be
energetically stable, but leave a detailed phenomenological study to later
work.",0
120,"In this paper the dynamical stability of the Type IIA flow with no source
near its stationary points is established. These stationary points had been
shown previously by the authors to be Ricci-flat K\""ahler metrics on Calabi-Yau
3-folds. The dynamical stability of the Type IIA flow is then applied to prove
the stability under symplectic deformations of the K\""ahler property for
Calabi-Yau 3-folds.",0
121,"Measuring the quasi-normal mode~(QNM) spectrum emitted by a perturbed
black-hole~(BH) --~also known as BH spectroscopy~-- provides an excellent
opportunity to test the predictions of general relativity in the strong-gravity
regime. We investigate the prospects and precision of BH spectroscopy in
massive binary black hole ringdowns, one of the primary science objectives of
the future Laser Interferometric Space Antenna~(LISA) mission. We simulate
various massive binary BH population models, featuring competing prescriptions
for the Delays between galaxy and BH mergers, for the impact of supernova
feedback on massive BH growth, and for the initial population of high redshift
BH seeds (light versus heavy seeds). For each of these scenarios, we compute
the average number of expected events for precision BH spectroscopy using a
Fisher-matrix analysis. We find that, for any heavy seed scenario, LISA will
measure the dominant mode frequency within ${\cal O}(0.1) \%$ relative
uncertainty and will estimate at least 3 independent QNM parameters within $1
\%$ error. The most optimistic heavy seed scenarios produce $\mathcal{O}(100)$
events with $1 \%$ measurability for 3 or more QNM quantities during LISA's
operational time. On the other hand, light seed scenarios produce lighter
merger remnants, which ring at frequencies higher than LISA's sensitivity.
Interestingly, the light seed models give rise to a fraction of mergers in the
band of Einstein Telescope, allowing for the measurement of 3 QNM parameters
with $\sim 10 \%$ relative errors in approximately a few to ten events/yr. More
precise BH spectroscopy in the light seed scenarios would require instruments
operating in the deciHertz band.",0
122,"Most stellar evolution models predict that black holes (BHs) should not exist
above approximately $50-70$ M$_\odot$. However, recent LIGO/Virgo detections
indicate the existence of BHs with masses at and above this threshold. We
suggest that massive BHs, including intermediate mass black holes (IMBHs), can
form in galactic nuclei through collisions between stellar-mass black holes and
the surrounding main-sequence stars. Considering dynamical processes such as
collisions, mass segregation, and relaxation, we find that this channel can be
quite efficient, forming IMBHs as massive as $10^4$ M$_\odot$. Our results
suggest that massive black holes and IMBHs may be ubiquitous in galactic
centres. This formation channel also has implications for observations.
Collisions between stars and BHs can produce electromagnetic signatures, for
example, from x-ray binaries and tidal disruption events. Additionally, formed
through this channel, both black holes in the mass gap and IMBHs can merge with
the supermassive black hole at the center of a galactic nucleus through
gravitational waves. These gravitational wave events are extreme and
intermediate mass ratio inspirals (EMRIs and IMRIs, respectively).",0
123,"Molecular maser lines are signposts of high-mass star formation, probing
excitation and kinematics of very compact regions in the close environment of
young stellar objects and providing useful targets for trigonometric parallax
measurements. Only a few NH$_{3}$ (9,6) masers were known so far, and their
origin is still poorly understood. Here we aim to find new NH$_{3}$ (9,6)
masers to provide a better observational basis to study their role in high-mass
star-forming regions. We carried out NH$_{3}$ (9,6) observations toward Cepheus
A and G34.26$+$0.15 with the Effelsberg-100 m telescope and the Karl G. Janksy
Very Large Array. We discovered new NH$_{3}$ (9,6) masers in Cep A and
G34.26$+$0.25, which increases the number of high-mass star-forming regions
hosting NH$_{3}$ (9,6) masers from five to seven. Long term monitoring (20
months) at Effelsberg shows that the intensity of the (9,6) maser in
G34.26$+$0.25 is decreasing, while the Cep A maser remains stable. Compared to
the Effelsberg data and assuming linear variations between the epochs of
observation, the JVLA data indicate no missing flux. This suggests that the
NH$_3$ (9,6) emission arises from single compact emission regions that are not
resolved by the interferometric measurements. As JVLA imaging shows, the
NH$_{3}$ (9,6) emission in Cep A originates from a sub-arcsecond sized region,
slightly to the west of the peak position of the 1.36\,cm continuum object,
HW2. In G34.26$+$0.25, three NH$_{3}$ (9,6) maser spots are observed: one is
close to the head of the cometary ultracompact \h2 region C and the other two
are emitted from a compact region to the west of the hypercompact \h2 region A.
The newly found (9,6) masers appear to be related to outflows. Higher angular
resolution of JVLA and VLBI observations are needed to provide more accurate
positions and constraints for pumping scenarios.",0
124,"We develop the microscopic theory for the emergence of multiple graviton
modes (GMs) in fractional quantum Hall (FQH) fluids, and show explicitly these
modes are associated with the geometric fluctuation of well-defined conformal
Hilbert spaces (CHS). These Hilbert spaces are hierarchical subspaces within a
single Landau level, each with emergent conformal symmetry and continuously
parameterized by a unimodular metric. We show explicitly the multiple GMs
originate from the spectral weights of the long wavelength GMP modes in each of
the CHSs, corresponding to the quantum fluctuation of each of its metric. For
different FQH phases, whether or not the spectral weight in a certain CHS is
zero can be analytically derived. This leads to a number of statements about
the number of GMs as well as merging and splitting of those modes, which we
verified numerically. We also discuss how the microscopic theory can serve as
the basis for the additional Haldane modes in the effective field theory
description, and their experimental relevance to realistic electron-electron
interaction.",0
125,"We describe a proposal to add a set of very forward detectors to CMS for the
high-luminosity era of the Large Hadron Collider to search for beyond the
standard model long-lived particles, such as dark photons, heavy neutral
leptons, axion-like particles, and dark Higgs bosons. The proposed subsystem is
called FACET for Forward-Aperture CMS ExTension, and will be sensitive to any
particles that can penetrate at least 50 m of magnetized iron and decay in an
18 m long, 1 m diameter vacuum pipe. The decay products will be measured in
detectors using identical technology to the planned CMS Phase-2 upgrade.",0
126,"A relative theory is a boundary condition of a higher-dimensional topological
quantum field theory (TQFT), and carries a non-trivial defect group formed by
mutually non-local defects living in the relative theory. Prime examples are 6d
N=(2,0) theories that are boundary conditions of 7d TQFTs, with the defect
group arising from surface defects. In this paper, we study codimension-two
defects in 6d N=(2,0) theories, and find that the line defects living inside
these codimension-two defects are mutually non-local and hence also form a
defect group. Thus, codimension-two defects in a 6d N=(2,0) theory are relative
defects living inside a relative theory. These relative defects provide
boundary conditions for topological defects of the 7d bulk TQFT. A
codimension-two defect carrying a non-trivial defect group acts as an irregular
puncture when used in the construction of 4d N=2 Class S theories. The defect
group associated to such an irregular puncture provides extra ""trapped""
contributions to the 1-form symmetries of the resulting Class S theories. We
determine the defect groups associated to large classes of both conformal and
non-conformal irregular punctures. Along the way, we discover many new classes
of irregular punctures. A key role in the analysis of defect groups is played
by two different geometric descriptions of the punctures in Type IIB string
theory: one provided by isolated hypersurface singularities in Calabi-Yau
threefolds, and the other provided by ALE fibrations with monodromies.",0
127,"This paper proposes a generalized Bell-like inequality (GBI) for
multiparticle entangled Schr\""{o}dinger-cat--states of arbitrary spin-$s$.
Based on quantum probability statistics the GBI and violation are formulated in
an unified manner with the help of state density operator, which can be
separated to local and non-local parts. The local part gives rise to the
inequality, while the non-local part is responsible for the violation. The GBI
is not violated at all by quantum average except the spin-$1/2$ entangled
states. If the measuring outcomes are restricted in the subspace of spin
coherent state (SCS), namely, only the maximum spin values $\pm s$, the GBI is
still meaningful for the incomplete measurement. With the help of SCS quantum
probability statistics, it is proved that the violation of GBI can occur only
for half-integer spins but not integer spins. Moreover, the maximum violation
bound depends on the number parity of entangled particles, that it is $1/2$ for
the odd particle-numbers while $1$ for even numbers.",0
128,"The coherent oscillation of ultralight dark matter induces changes in
gravitational potential with the frequency in nanohertz range. This effect is
known to produce a monochromatic signal in the pulsar timing residual. Here we
discuss a multi fields scenario that produces a wide spectrum of frequencies,
such that the ultralight particle oscillation can mimic the pulsar timing
signal of stochastic gravitational wave background. We discuss how ultralight
dark matter with various spins produces such a wideband spectrum on pulsar
timing residuals and perform the Bayesian analysis to constrain the parameters.",0
129,"We consider a U(1) Gauged Linear Sigma Model (GLSM) with (2,2) supersymmetry,
leading to a susy vacua of the resolved conifold. It possesses the non-Abelian
global symmetry SU(2)xSU(2). A non-Abelian T-duality can be constructed which
can be described by gauging the global non-Abelian symmetry. This leads to a
dual action, in terms of the dual model Kaehler and superpotential terms, which
include twisted chiral superfields dependence. Comparing the effective
potentials for the U(1) fields on the original and the dual models we determine
the instanton corrections to the dual action. We obtain the supersymmetry vacua
solution of the dual model, in three cases: first in an Abelian direction
inside SU(2)xSU(2), then for an Abelian direction considering instanton
corrections and finally for a semi-chiral non Abelian vector superfield. The
dual geometry for all of these cases (SUSY vacua space) is given by T5xR.
However the instanton corrections restrict the Fayet-Iliopoulos term to be zero
and fix the theta-term, which implies that the duality is established for the
case of the singular conifold. We also discuss the U(1)A R-symmetry in the dual
model and its possible role.",0
130,"A fast algorithm for the large-scale joint inversion of gravity and magnetic
data is developed. It uses a nonlinear Gramian constraint to impose correlation
between density and susceptibility of reconstructed models. The global
objective function is formulated in the space of the weighted parameters, but
the Gramian constraint is implemented in the original space, and the nonlinear
constraint is imposed using two separate Lagrange parameters, one for each
model domain. This combined approach provides more similarity between the
reconstructed models. It is assumed that the measured data are obtained on a
uniform grid and that a consistent regular discretization of the volume domain
is imposed. The sensitivity matrices exhibit a block Toeplitz Toeplitz block
structure for each depth layer of the model domain. Forward and transpose
operations with the matrices can be implemented efficiently using two
dimensional fast Fourier transforms. This makes it feasible to solve for large
scale problems with respect to both computational costs and memory demands, and
to solve the nonlinear problem by applying iterative methods that rely only on
matrix vector multiplications. As such, the use of the regularized reweighted
conjugate gradient algorithm, in conjunction with the structure of the
sensitivity matrices, leads to a fast methodology for large-scale joint
inversion of geophysical data sets. Numerical simulations demonstrate that it
is possible to apply a nonlinear joint inversion algorithm, with $L_p$-norm
stabilisers, for the reconstruction of large model domains on a standard laptop
computer. It is demonstrated, that the p=1 choice provides sparse reconstructed
solutions with sharp boundaries, and $p=2$ provides smooth and blurred models.
Gravity and magnetic data obtained over an area in northwest of Mesoproterozoic
St. Francois Terrane, southeast of Missouri, USA are inverted.",0
131,"In a recent work, a method for the magnetic resonance (MR) measurement of the
true diffusion propagator was introduced, which was subsequently implemented
and validated for free diffusion on a benchtop MR scanner. Here, we provide a
brief theoretical description of the method and discuss various experimental
regimes.",0
132,"This paper provides a complete self-consistent nonlinear theory for electron
plasma waves, within the framework of the adiabatic approximation. The theory
applies whatever the variations of the wave amplitude, provided that they are
slow enough, and it is also valid when the plasma is inhomogeneous and non
stationary. Moreover, it accounts for: (i) the geometrical jump in action
resulting from separatrix crossing; (ii) the continuous change in phase
velocity making the wave frame non-inertial; (iii) the harmonic content of the
scalar potential ; (iv) a non-zero vector potential ; (v) the transition
probabilities from one region of phase space to the other when an orbit crosses
the separatrix ; (vi) the possible change in direction of the wavenumber. The
relative importance of each of the aforementioned effects is discussed in
detail, based on the derivation of the nonlinear frequency shift. This allows
to specify how the general formalism may be simplified, depending on the value
of the wavenumber normalized to the Debye length. Specific applications of our
theory are reported on the companion paper.",0
133,"We use $^{79}$Br nuclear quadrupole resonance (NQR) to demonstrate that ultra
slow lattice dynamics set in below the temperature scale set by the Cu-Cu
super-exchange interaction $J$($\simeq160$ K) in the kagome lattice Heisenberg
antiferromagnet Zn-barlowite. The lattice completely freezes below 50 K, and
$^{79}$Br NQR lineshapes become twice broader due to increased lattice
distortions. Moreover, the frozen lattice exhibits an oscillatory component in
the transverse spin echo decay, a typical signature of pairing of nuclear spins
by indirect nuclear spin-spin interaction. This indicates that some Br sites
form structural dimers via a pair of kagome Cu sites prior to the gradual
emergence of spin singlets below $\sim30$ K. Our findings underscore the
significant roles played by subtle structural distortions in determining the
nature of the disordered magnetic ground state of the kagome lattice.",0
134,"Surface electric (charge) noise influences spin defects due to fluctuation of
the surface charge density and also the electrostatic potential at the crystal
surface. Surprisingly, the two-point correlation function of both the charged
particles' positions and the surface electrostatic potential strongly
influences the power of the polynomial decay of the electric noise spectral
density; this power is not determined solely by the character of the charge
fluctuators. Time-dependent crossover behavior near the correlation time of the
fluctuators, of the spin defect's relaxation and decoherence, provide a
quantitative fingerprint of the diffusive behavior of charged particles at the
surface.",0
135,"We calculate the fiducial and differential cross-sections of $W^{\pm}/Z^0$
production in association with hadronic jets in the presence of electroweak
(EW) corrections through virtual gauge boson self-interactions in the matrix
elements (MEs) of the processes and real partonic cascade emissions. The
calculations are carried out for proton-proton collisions at $\sqrt{s} = 13$
TeV, using Herwig 7 multi-purpose Monte-Carlo event generator with
leading-order or next-to-leading-order MEs that are interfaced with different
parton-shower configurations. The results are compared with experimental
measurements from ATLAS collaboration and with similar computations within the
$k_t$-factorisation framework, providing a test for the validity of the
newly-implemented \textsf{QCD$\oplus$QED$\oplus$EW} parton shower in Herwig 7.
It is shown that the inclusion of EW radiations in the parton shower
simulations improves Herwig's predictions in describing the experimental data.
Additionally, the inclusion of real EW emissions can take precedence over the
incorporation of virtual EW corrections for the simulation of EW-sensitive
events.",0
136,"We propose a way to identify and classify genuine tripartite entanglement in
pure states of three-qubit systems via the Activated Bipartite Entanglement
(ABE). We show that for pure states pertaining to one of the two inequivalent
classes of genuine tripatite entanglement, for which the GHZ and the W states
are the notorious representatives, the ABE is always greater than zero, while
for separable tripartite pure states or bipartite entanglement between two
parts of three-qubit pure states it is always null. In addition we present a
experimental proposal by using linear optical circuits and degrees of freedom
of a single photon to verify the characterization via distributed entanglement.
The circuit simulation showed an excellent accordance with theoretical
prediction for a wide classes of GHZ states.",0
137,"In this work we present a novel, compact, power efficient magnetic field
source design for magnetic field imaging microscopy. The device is based on a
pair of diametrically magnetized permanent magnet cylinders with
electro-mechanical rotation control and ferrite field concentrators. A Hall
probe and NV centers in diamond are used to demonstrate a proof of concept of a
proposed magnetic field setup and to characterise the homogeneity of the
produced magnetic field on a micrometer scale. Numerical simulation results are
compared with experimental results showing good agreement of the distribution
of the magnetic field in the setup. As a result, a magnetic field source with a
tunable field amplitude in the range from 1 mT to 222 mT is demonstrated,
achieving a magnetic field homogeneity of 2 ppm/$\mu$m or 0.5 $\mu$T/$\mu$m at
222 mT in a 25$\times$25 $\mu$m field of view.",0
138,"The Laboratorio Subterra neo de Canfranc (LSC) is the national hub for low
radioactivity techniques and the associated scientific and technological
applications. The concentration of the airborne radon is a major component of
the radioactive budget in the neighborhood of the detectors. The LSC hosts a
Radon Abatement System (RAS), which delivers a radon suppressed air with < 1
mBq/m3 of 222Rn. The radon content in the air is continuously monitored with an
Electrostatic Radon Monitor (ERM). Measurements with the doble beta decay
demonstrators NEXT-NEW and CROSS and the gamma HPGe detectors demonstrate the
important reduction of the radioactive background due to the replaced air in
the vicinity of the detectors. We also discuss the use of this facility in the
LSC current program which includes NEXT-100, low background biology experiments
and radiopure copper electroformation equipment placed in the radon-free clean
room.",0
139,"The structural and dynamic properties of the dark matter halos, though an
important ingredient in understanding large-scale structure formation, require
more conservative particle resolution than those required by halo mass alone in
a simulation. This reduces the parameter space of the simulations, more
severely for high-redshift and large-volume mocks which are required by the
next-generation large sky surveys. Here, we incorporate redshift and cosmology
dependence into an algorithm that assigns accurate halo properties such as
concentration, spin, velocity, and spatial distribution to the sub-resolution
haloes in a simulation. By focusing on getting the right correlations with halo
mass and local tidal anisotropy $\alpha$ measured at $4 \times$ halo radius,
our method will also recover the correlations of these small scale structural
properties with the large-scale environment, i.e., the halo assembly bias at
all scales greater than $5 \times$ halo radius. We find that the distribution
of halo properties is universal with redshift and cosmology. By applying the
algorithm to a large volume simulation $(600 h^{-1}{\rm Mpc})$, we can access
the $30-500$ particle haloes, thus gaining an order of magnitude in halo mass
and two to three orders of magnitude in number density at $z=2-4$. This
technique reduces the cost of mocks required for the estimation of covariance
matrices, weak lensing studies, or any large-scale clustering analysis with
less massive haloes.",0
140,"A variety of novel phenomena and functionalities emerge from lowering the
dimensionality of materials and enriching the degrees of freedom in modulation.
In this work, it is found that the saturation magnetization of LaMnO3 (LMO)
films is largely enhanced by 56% after releasing from a brand-new phase of
tetragonal strontium aluminate buffer layer, and is significantly increased by
92% with bending films to a curvature of 1 mm-1 using a water-assisted
direct-transferring method. Meanwhile, the Curie temperature of LMO films has
been improved by 13 K. High-resolution spherical aberration-corrected scanning
transmission electron microscopy and first-principles calculations
unambiguously demonstrate that the enhanced ferromagnetism is attributed to the
strengthened Mn-O-Mn super-exchange interactions from the augmented
characteristics of the unconventional P21/n structure caused by the
out-of-plane lattice shrinking after strain releasing and increased flexure
degree of freestanding LMO films. This work paves a way to achieve large-scale
and crack-and-wrinkle-free freestanding films of oxides with largely improved
functionalities.",0
141,"In this work, by using the machine learning methods, we study the
sensitivities of heavy pseudo-Dirac neutrino $N$ in the inverse seesaw at the
high-energy hadron colliders. The production process for the signal is $pp \to
\ell^\pm N \to 3 \ell + E_T^{\rm miss}$, while the dominant background is $p p
\to W^\pm Z \to 3 \ell + E_T^{\rm miss}$. We use either the Multi-Layer
Perceptron or the Boosted Decision Tree with Gradient Boosting to analyze the
kinematic observables and optimize the signal/background discrimination. It is
found that the reconstructed $Z$ boson mass and heavy neutrino mass from the
charged leptons and missing transverse energy play crucial roles to separate
the signal/background events. We estimate the prospects of heavy-light neutrino
mixing $|V_{\ell N}|^2$ (with $\ell = e,\,\mu$) using machine learning at the
hadron colliders with $\sqrt{s}=14$ TeV, 27 TeV, and 100 TeV, and find that
$|V_{\ell N}|^2$ can be improved up to ${\cal O} (10^{-6})$ for heavy neutrino
mass $m_N = 100$ GeV and ${\cal O} (10^{-4})$ for $m_N = 1$ TeV.",0
142,"Quantum computing is expected to play an important role in solving the
problem of huge computational costs in various applications by utilizing the
collective properties of quantum states, including superposition, interference,
and entanglement, to perform computations. Quantum mechanical (QM) methods are
candidates for various applications and can provide accurate absolute energy
calculations in structure-based methods. QM methods are powerful tools for
describing reaction pathways and their potential energy surfaces (PESs). In
this study, we applied quantum computing to describe the PES of the bimolecular
nucleophilic substitution (SN2) reaction between chloromethane and chloride
ions. We performed noiseless and noise simulations using quantum algorithms and
compared the accuracy and noise effects of the ansatzes. In noiseless
simulations, the results from UCCSD and k-UpCCGSD are similar to those of full
configurational interaction (FCI) with the same active space, which indicates
that quantum algorithms can describe the PES of the SN2 reaction. In noise
simulations, UCCSD is more susceptible to quantum noise than k-UpCCGSD.
Therefore, k-UpCCGSD can serve as an alternative to UCCSD to reduce quantum
noisy effects in the noisy intermediate-scale quantum era, and k-UpCCGSD is
sufficient to describe the PES of the SN2 reaction in this work. The results
showed the applicability of quantum computing to the SN2 reaction pathway and
provided valuable information for structure-based molecular simulations with
quantum computing.",0
143,"Using ultrashort laser pulses, it has become possible to probe the dynamics
of long-range order in solids on microscopic timescales. In the conventional
description of symmetry-broken phases within time-dependent Ginzburg-Landau
theory, the order parameter evolves coherently, with small fluctuations along
an average trajectory. Recent experiments, however, indicate that some systems
can support a different scenario, named ultrafast inhomogeneous disordering,
where the average order parameter is no longer representative of the state on
the atomic scale. Here we theoretically show that ultrafast disordering can
occur in a paradigmatic model for a Peierls instability if atomic scale
inhomogeneities of both the electronic structure and the charge density wave
order parameter are taken into account. The latter is achieved using a
non-equilibrium generalization of statistical dynamical mean-field theory,
coupled to stochastic differential equations for the order parameter.",0
144,"Hamiltonian Monodromy is the simplest topological obstruction to the
existence of global action-angle coordinates in a completely integrable system.
We show that this property can be studied in a neighborhood of a focus-focus
singularity by a spectral Lax pair approach. From the Lax pair, we derive a
Riemann surface which allows us to compute in a straightforward way the
corresponding Monodromy matrix. The general results are applied to the
Jaynes-Cummings model and the spherical pendulum.",0
145,"Two-dimensional intrinsic antiferromagnetic semiconductors are expected to
stand out in the spintronic field. The present work finds the monolayer
T'-MoTeI is intrinsically an antiferromagnetic semiconductor by using
first-principles calculation. Firstly, the dimerized distortion of the Mo atoms
causes T'-MoTeI to have dynamic stability, which is different from the small
imaginary frequency in the phonon spectrum of T-MoTeI. Secondly, T'-MoTeI is an
indirect-bandgap semiconductor with 1.35 eV. Finally, in the systematic study
of strain effects, there are significant changes in the electronic structure as
well as the bandgap, but the antiferromagnetic ground state is not affected.
Monte Carlo simulations predict that the Neel temperature of T'-MoTeI is 95 K.
The results suggest that the monolayer T'-MoTeI can be a potential candidate
for spintronics applications.",0
146,"Transition metals dichalcogenides (TMDs) are an emergent class of
low-dimensional materials with growing applications in the field of
nanoelectronics. However, efficient methods for synthesizing large
mono-crystals of these systems are still lacking. Here, we describe an
efficient synthetic route for a large number of TMDs that were obtained in
quartz ampoules by sulfuric vapor transport and liquid sulfur. Crystals of
metal sulfides MgS, PdS, PtS2, ReS2, NbS2, TaS2, TaS3, MoS2, WS2, FeS2, CoS2,
NiS2, Cr2S3, VS2, In2S3, Bi2S3, TiS2, ZrS3, HfS3, and pure Au were obtained in
quartz ampoules by chemical vapor transport technique with sulfur vapors as the
transport agent. Unlike the sublimation technique, the metal enters the gas
phase in the form of molecules, hence containing greater amount of sulfur than
the growing crystal. We have investigated the physical properties for a
selection of these crystals and compared them to state-of-the-art findings
reported in the literature. The acquired x-ray photoemission spectroscopy
features demonstrate the overall high quality of single crystals grown in this
work as exemplified by ReS2 and CoS2. This new approach to synthesize
high-quality transition metal dichalcogenides single crystals can alleviate
many material quality concerns and is suitable for emerging electronic devices.",0
147,"We develop a non-Abelian time evolving block decimation (NA-TEBD) approach to
study of open systems governed by Lindbladian time evolution, while exploiting
an arbitrary number of abelian or non-abelian symmetries. We illustrate this
method in a one-dimensional fermionic $SU(2)$ Hubbard model on a semi-infinite
lattice with localized particle loss at one end. We observe a ballistic front
propagation with strongly renormalized front velocity, and a hydrodynamic
current density profile. For large loss rates, a suppression of the particle
current is observed, as a result of the quantum Zeno effect. Operator
entanglement is found to propagate faster than the depletion profile, preceding
the latter.",0
148,"The Integrable Optics Test Accelerator (IOTA) at Fermilab is a small machine
dedicated to a broad frontier accelerator physics program. An important aspect
of this program is to investigate the potential benefits of the resonance free
tune spread achievable with integrable optics to store and accelerate high
intensity proton beams for which space charge is significant. In this context,
a good understanding of proton beam emittance growth and particle loss
mechanisms is essential.
  Assuming nominal design parameters, simulations show that for a bunched beam,
the bulk of emittance growth takes place immediately following injection,
typically within tens of turns. We attempt to account for this growth using a
simplified RMS mismatch theory; some of its limitations and possible
improvements are briefly discussed. We then compare theoretical predictions to
simulations performed using the PIC code pyORBIT. Further exploring ways to
mitigate emittance growth and reduce particle loss, we compare two beam
matching strategies: (1) matching at the injection point (2) matching at the
center of the nonlinear (octupole) insertion region where $\beta_x = \beta_y$.
To observe how nonlinearity affects emittance growth and whether it dominates
growth due to mismatch, we track two different distributions. Finally, we
explore the potential of using octupoles in a quasi-integrable configuration to
mitigate growth using a variety of initial distributions both at reduced and
full intensities.",0
149,"Using two models of opinion dynamics, the $q$-voter model with independence
and the $q$-voter model with anticonformity, we discuss how the change of
disorder from annealed to quenched affects phase transitions on networks. Up
till now, such an analysis has been done only at the mean-field level. To
derive phase diagrams on networks, we develop the pair approximation for the
quenched versions of the models. This formalism can be also applied to other
quenched dynamics of similar kind. The results indicate that such a change of
disorder eliminates all discontinuous phase transitions and broadens ordered
phases. We show that although the annealed and quenched types of disorder lead
to the same result in the $q$-voter model with anticonformity at the mean-field
level, they do lead to distinct phase diagrams on networks. These phase
diagrams shift towards each other as the average node degree of a network
increases, and eventually, they coincide in the mean-field limit. In contrast,
for the $q$-voter model with independence, the phase diagrams move towards the
same direction regardless of the disorder type, and they do not coincide even
in the mean-filed limit. To validate our results, we carry out Monte Carlo
simulations on random regular graphs and Barab\'{a}si-Albert networks. Although
the pair approximation may incorrectly predict the type of phase transitions
for the annealed models, we have not observed such errors for their quenched
counterparts.",0
150,"The type IIB matrix model was proposed as a non-perturbative formulation of
superstring theory in 1996. We simulate a model that describes the late time
behavior of the IIB matrix model by applying the complex Langevin method to
overcome the sign problem. We clarify the relationship between the Euclidean
and the Lorentzian versions of the type IIB matrix model in a recently
discovered phase. By introducing a constraint, we obtain a model where the
spacetime metric is Euclidean at early times, whereas it {\it dynamically}
becomes Lorentzian at late times.",0
151,"Topological Josephson junctions (TJJs) have been a subject of widespread
interest due to their hosting of Majorana zero modes. In long junctions, i.e.
junctions where the junction length exceeds the superconducting coherence
length, TJJs manifest themselves in specific features of the critical current.
Here we propose to couple the helical edge states mediating the TJJ to
additional channels or quantum dots, by which the effective junction length can
be increased by tunable parameters associated with these couplings, so that
such measurements become possible even in short junctions. Besides effective
low-energy models that we treat analytically, we investigate realizations by a
Kane-Mele model with edge passivation and treat them numerically via tight
binding models. In each case, we explicitly calculate the critical current
using the Andreev bound state spectrum and show that it differs in effective
long junctions in the cases of strong and weak parity changing perturbations
(quasiparticle poisoning).",0
152,"The atomically precise method has become an important technique to adjust the
core of thiolate-protected gold nanoclusters to improve physical and chemical
properties. But the doping effect on the structural stability has not been
systematically summarized. In this work, the H2S-nanoalloy molecules with
different doping metal atoms has been investigated to elucidate the impact of
the dopant on the structures. With DFT simulation results, the zinc group atoms
as dopants may be influenced by surrounded gold atoms and the binding of the
thiolate units are enhanced. The simulated zinc group data when combined to the
gold group and plantinum group data can be summarized in the perspective of
balance between the ligand-core binding and core cohesive energies. Most of
dopants drive the modeled nanoclusters away from the balance especially when
the metal atom replaced the gold atom in gold-sulfur bindings. But when cores
of the nanoclusters are dominated by gold atoms, the dopants may achieve
""saturation"" such that the balance in the doped clusters may be corrected. This
work provide a simple profile to understand the internal shift of the structure
introduced by the atomically precise method.",0
153,"In this paper, we study the nonlinear stability of a steady circular flow
created between two rotating concentric cylinders. The dynamics of the viscous
fluid are described by 2D Navier-Stokes equations. We adopt scaling variables.
For the rescaled equations, we prove that the steady flow (Taylor-Couette flow)
is asymptotically stable up to a large perturbation of initial data. Back to
the original 2D Navier-Stokes equations, this implies an improved transition
threshold for the Taylor-Couette flow. The improvement is due to enhanced
dissipation and new observations and constructions of weighted $L^2$ norms,
which capture a hidden structure between the viscosity constant $\nu$ and
(different) rotating speeds and locations of two coaxial cylinders. In
particular, we allow the location of the outer cylinder to tend to infinity,
which renders the initial fluid kinetic energy not uniformly bounded. Due to
enhanced-dissipation effect, we also establish a sharp resolvent estimate,
desired space-time bounds and optimal decaying estimates, which lead to the
proof of nonlinear asymptotic stability of 2D Taylor-Couette flow.",0
154,"In June 2021 the discovery of an unusual comet C/2014 UN271
Bernardinelli-Bernstein has been announced. Its cometary activity beyond Uranus
orbit also has refreshed interest in similar objects, including C/2017K2
PanSTARRS. Another peculiarity of these objects is the long interval of
positional data, taken at large heliocentric distances. These two comets are
suitable candidates for a detailed investigation of their long-term motion
outside the planetary zone. Using the selected orbital solutions, we aim at
estimating the orbital parameters of their orbits at the previous perihelion
passage. This might allow us to discriminate between dynamically old and new
comets. To follow the dynamical evolution of long-period comets far outside the
planetary zone, it is necessary to take into account both the perturbation
caused by the overall Galactic gravitational potential and the actions of
individual stars appearing in the solar neighborhood. To this aim, we applied
the recently published methods based on stellar perturbers ephemerides. For
C/2014 UN271 we obtained a precise orbital solution that can be propagated to
the past and to the future. For C/2017 K2 we have to limit ourselves to study
only the past motion since some signs of nongravitational effects can be found
in recent positional observations. Therefore, we use a specially selected
orbital solution suitable for past motion studies. Using these starting orbits,
we propagated both comets to their previous perihelia. We also investigated the
future motion of C/2014 UN271. Orbital evolution of these two comets appears to
be sensitive to perturbations from several stars that closely approach the Sun.
Unfortunately, some of these stars have 6D data with uncertainties too large to
obtain definitive results for the studied comets; however, it appears that both
comets were probably outside the planetary zone in the previous perihelion.",0
155,"QFP wave trains in the corona have been studied intensively in the past
decade, thanks to the full-disk, high spatiotemporal resolution, and
wide-temperature coverage observations taken by the SDO/AIA. In AIA
observations, QFP wave trains are seen to consist of multiple coherent and
concentric wavefronts emanating successively near the epicenter of the
accompanying flares; they propagate outwardly either along or across coronal
loops at fast-mode magnetosonic speeds from several hundred to more than 2000
km/s, and their periods are in the range of tens of seconds to several minutes.
Based on the distinct different properties of QFP wave trains, they might be
divided into two distinct categories including narrow and broad ones. For most
QFP wave trains, some of their periods are similar to those of quasi-periodic
pulsations (QPPs) in the accompanying flares, indicating that they are probably
different manifestations of the same physical process. Currently, candidate
generation mechanisms for QFP wave trains include two main categories: pulsed
energy excitation mechanism in association with magnetic reconnection and
dispersion evolution mechanism related to the dispersive evolution of
impulsively generated broadband perturbations. In addition, the generation of
some QFP wave trains might be driven by the leakage of three and five minute
oscillations from the lower atmosphere. As one of the new discoveries of SDO,
QFP wave trains provide a new tool for coronal seismology to probe the corona
parameters, and they are also useful for diagnosing the generation of QPPs,
flare processes including energy release and particle accelerations. This
review aims to summarize the main observational and theoretical results of the
spatially-resolved QFP wave trains in extreme ultraviolet observations, and
states briefly a number of questions that deserve further investigations.",0
156,"By combining spectroscopic data from the LAMOST DR7, SDSS DR12, and corrected
photometric data from the Gaia EDR3, we apply the Stellar Color Regression
(SCR; Yuan et al. 2015a) method to recalibrate the SDSS Stripe 82 standard
stars catalog of Ivezi\'c et al. (2007). With a total number of about 30,000
spectroscopically targeted stars, we have mapped out the relatively large and
strongly correlated photometric zero-point errors present in the catalog,
$\sim$2.5 per cent in the $u$ band and $\sim$ 1 per cent in the $griz$ bands.
Our study also confirms some small but significant magnitude dependence errors
in the $z$ band for some charge-coupled devices. Various tests show that we
have achieved an internal precision of about 5 mmag in the $u$ band and about 2
mmag in the $griz$ bands, which is about 5 times better than previous results.
We also apply the method to the latest version of the catalog (V4.2; Thanjavur
et al. 2021), and find modest systematic calibration errors up to $\sim$ 1 per
cent along the R.A. direction and smaller errors along the Dec. direction. The
results demonstrate the power of the SCR method when combining spectroscopic
data and Gaia photometry in breaking the 1 percent precision barrier of
ground-based photometric surveys. Our work paves the way for the re-calibration
of the whole SDSS photometric survey and has important implications for the
calibration of future surveys. Future implementations and improvements of the
SCR method under different situations are also discussed.",0
157,"We present a Bayesian inference scheme for scaled Brownian motion, and
investigate its performance on synthetic data for parameter estimation and
model selection in a combined inference with fractional Brownian motion. We
include the possibility of measurement noise in both models. We find that for
trajectories of a few hundred time points the procedure is able to resolve well
the true model and parameters. Using the prior of the synthetic data generation
process also for the inference, the approach is optimal based on decision
theory. We include a comparison with inference using a prior different from the
data generating one.",0
158,"Solitons in space-time capable of transporting time-like observers at
superluminal speeds have long been tied to violations of the weak, strong, and
dominant energy conditions of general relativity. This trend was recently
broken by a new approach that identified soliton solutions capable of
superluminal travel while being sourced by purely positive energy densities.
This is the first example of hyper-fast solitons satisfying the weak energy
condition, reopening the discussion of superluminal mechanisms rooted in
conventional physics. This article summarizes the recent finding and its
context in the literature. Remaining challenges to autonomous superluminal
travel, such as the dominant energy condition, horizons, and the identification
of a creation mechanism are also discussed.",0
159,"We examine the gravitational wave frequencies from neutron stars during
thermal evolution, adopting the relativistic Cowling approximation. We
particularly focus on the neutron star models, in which the direct Urca (rapid
cooling process) does not work, without the superfluidity and
superconductivity. For such models, the cooling curve hardly depends on the
equation of state (EOS) as well as the mass of neutron star, while we show that
the gravitational wave frequencies strongly depend on the both properties.
Then, we find that the frequencies of the fundamental and the 1st pressure mode
multiplied with the stellar mass are well expressed as a function of the
stellar compactness almost independently of the EOS. We also find that the
frequency of the 1st gravity mode in later phase of the thermal evolution is
strongly correlated with the stellar compactness. In addition, we derive the
empirical formula estimating the threshold mass for the onset of the direct
Urca inside the neutron star as a function of the nuclear saturation parameter.
This formula will give us a constraint on the neutron star properties, if it
would be observationally found that the direct Urca occurs (or does not work)
inside the neutron star.",0
160,"We explore the potential of the $e^+ e^-$ colliders to discover dark matter
and determine its properties such as mass and the spin. For this purpose we
study spin zero and spin one-half cases of dark matter, $D$ which belongs to
$SU(2)$ weak doublet and therefore has the charged doublet partner, $D^+$. For
the case of scalar dark matter we chose Inert Doublet Model, while for the case
of fermion dark matter we suggest the new minimal fermion dark matter model
with only three parameters. We choose two benchmarks for the models under study
which provide the correct amount of observed DM relic density and consistent
with the current DM searches. We focus on the particular process $e^+ e^- \to
D^+ D^- \to D D W^+ W^- \to DD(q \bar{q})(\mu^\pm\nu)$ at 500 GeV ILC collider
which gives rise to the ""di-jet +$\mu$ + missing $E_T$"" signature and study it
at the level of fast detector simulation, taking into account Bremsstrahlung
and ISR effects. We have found that two kinematical observables -- the energy
of the muon, $E_\mu$, and the angular distribution of $W$-boson, reconstructed
from di-jet, $\cos\theta_{jj}$ are very powerful in determination of DM mass
and spin, respectively. In particular we have demonstrated that in case of
fermion DM, the masses can be measured with a few percent accuracy already at
500 fb$^{-1}$ integrated luminosity. At the same time, the scalar DM model
which has about an order of magnitude lower signal, requires about factor of 40
higher luminosity to reach the same accuracy in the mass measurement. We have
found that one can distinguish fermion and scalar DM scenarios with about 2
ab$^{-1}$ total integrated luminosity or less without using the information on
the cross sections for benchmarks under study.",0
161,"We discuss the application of normalizing flows to bosonic lattice field
theories with real-time sign problems. A normalizing flow, once it is found for
such a lattice field theory, is guaranteed to solve its sign problem. We argue
for the existence of normalizing flows for bosonic lattice field theories in
the Schwinger-Keldish formalism in a few ways. We then discuss how this
existence is a specific feature of bosonic theories: such arguments break down
for fermionic systems, whether at finite density or in real-time.",0
162,"Degassing of CO2 and precipitation of calcite to the surface of stalagmites
can strongly impact isotope signals imprinted into the calcite of these
speleothems. Here, we show that in all the variety of conditions occurring in
nature only two distinct types of degassing exist. First, when a thin film of
calcareous solution comes in contact to cave air lower pCO2 value than that of
the aqueous CO2 in the water, molecular CO2 escapes by physical diffusion in
several seconds. In a next step lasting several ten seconds, pH and DIC in the
solution achieve chemical equilibrium with respect to the CO2 in the cave
atmosphere. This solution becomes supersaturated with respect to calcite.
During precipitation for each unit CaCO3 deposited one molecule of CO2 is
generated and escapes from the solution. This precipitation driven degassing is
active during precipitation only. We show that all variations of out gassing
proposed in the literature are either diffusive outgassing or precipitation
driven degassing and that diffusive outgassing has no influence on the isotope
composition of the HCO3 - pool and consequently on that of calcite. Its isotope
imprint is determined solely by precipitation driven degassing in contrast to
most explanations in the literature. We present a theoretical model of d13C and
d18O that explains the contributions of various parameters such as changes in
temperature, changes of pCO2 in the cave atmosphere, and changes in the drip
intervals to the isotope composition of calcite precipitated to the apex of the
stalagmite. We use this model to calculate quantitatively changes of d13C and
d18O observed in field experiments (Carlson et al., 2020) in agreement to their
experimental data. We also apply our model to prior calcite precipitation (PCP)
in the field as reported by Mickler et al. (2019). We discuss how PCP
influences isotope composition signals. ...",0
163,"We investigate Majorana modes in a quantum spin chain with bond-dependent
exchange interactions by studying its dynamics. Specifically, we consider
two-time correlations for the Kitaev-Heisenberg (KH) Hamiltonian close to the
so-called Kitaev critical point. Here, the model coincides with a phase
boundary of two uncoupled instances of Kitaev's model for p-wave
superconductors, together supporting a degenerate ground state characterized by
multiple Majorana modes. In this regime, the real-time dynamics of local spins
reveal a set of strong zero modes, corresponding to a set of protruding
frequencies in the two-time correlation function. We derive perturbative
interactions that map the KH spin chain onto the topological regime of Kitaev's
fermionic model, thus opening up a bulk gap whilst retaining almost degenerate
modes in the mesoscopic regime, i.e., for finite system sizes. This showcases
the emergence of Majorana modes in a chain of effective dimers. Here, the
binding energy within each unit cell competes with the inter-dimer coupling to
generate a finite size energy gap, in analogy with local energy terms in the
transverse-field Ising model. These modes give rise to long coherence times of
local spins located at the system edges. By breaking the local symmetry in each
dimer, one can also observe a second class of Majorana modes in terms of a
beating frequency in the two-time correlations function of the edge spin.
Furthermore, we develop a scenario for realizing these model predictions in
ion-trap quantum simulators with collective addressing of the ions.",0
164,"Recent theoretical research has developed a general framework to understand
director deformations and modulated phases in nematic liquid crystals. In this
framework, there are four fundamental director deformation modes: twist, bend,
splay, and a fourth mode $\Delta$ related to saddle-splay. The first three of
these modes are known to induce modulated phases. Here, we consider modulated
phases induced by the fourth mode. We develop a theory for tetrahedral order in
liquid crystals, and show that it couples to the $\Delta$ mode of director
deformation. Because of geometric frustration, the $\Delta$ mode cannot fill
space by itself, but rather must be accompanied by twist or splay. Hence, it
may induce a spontaneous cholesteric phase, with either handedness, or a splay
nematic phase.",0
165,"The Event Horizon Telescope collaboration has revealed the first direct image
of a black hole, as per the shadow of a Kerr black hole of general relativity.
However, other Kerr-like rotating black holes of modified gravity theories
cannot be ignored, and they are essential as they offer an arena in which these
theories can be tested through astrophysical observation. This motivates us to
investigate asymptotically de Sitter rotating black holes wherein interpreting
the cosmological constant $\Lambda$ as the vacuum energy leads to a deformation
in the vicinity of a black hole -- new Kerr--de Sitter solution, which has a
richer geometric structure than the original one. We derive an analytical
formula necessary for the shadow of the new Kerr--de Sitter black holes and
then visualize the shadow of black holes for various parameters for an observer
at given coordinates $(r_0, \theta_0)$ in the domain $(r_0,\; r_c)$ and
estimate the cosmological constant $\Lambda$ from its shadow observables. The
shadow observables of the new Kerr--de Sitter black holes significantly deviate
from the corresponding observables of the Kerr--de Sitter black hole over an
appreciable range of the parameter space. Interestingly, we find a finite
parameter space for ($\Lambda$, $a$) where the observables of the two black
holes are indistinguishable.",0
166,"We have studied the proximity_induced superconductivity in R7xR7R19.1o Ni
nanoislands by combing scanning tunnelling microscopy_spectroscopy (STM_STS)
with density functional theory (DFT) calculation. Through depositing Ni onto
Pb(111) substrate at 80 K, the monolayer Ni nanoislands with the R7xR7R19.1o
surface structure have been fabricated, where the termination of Ni atoms at
hexagonal close packed (hcp) site is energetically preferred and the electron
filling of 3d orbitals from the charge transfer leads to the vanishing magnetic
moment of Ni atoms. The topographic R7xR7R19.1o lattice as well as the
asymmetric height contrast in atomic unit cell are further corroborated by the
STM simulations. With high spatial and energy resolution, tunneling conductance
spectra have resolved an isotropic superconducting gap with
Delta_Ni_(R7xR7R19.1o)_1.29 meV, which is slightly larger than Delta_Pb_1.25
meV. The temperature dependence of Delta_Ni_(R7xR7R19.1o) supports the
substrate_induced superconducting proximity effect according to the same
transition temperature Tc_7.14 K with the Pb(111). The line spectroscopy has
spatially mapped out the small increase of Delta_Ni_(R7xR7R19.1o), which could
be explained by an enhanced electron_phonon interaction under the framework of
Bardeen_Cooper_Schrieffer (BCS) theory as a manifestation of the hole doping of
Pb(111) from the surface Ni atoms.",0
167,"The vector-like quarks are predicted in many new physics scenarios beyond the
Standard Model~(SM) and could be seen potential signatures of new physics at
the TeV energy scale. In this work, we study single production of exotic
singlet and doublet vectorlike bottom quarks (VLQ-$B$) at future Compact Linear
Collider~(CLIC) via the process $e^{+}e^{-}\to B\bar{b}$ with the decay channel
$B\to bZ$ and two types of modes: $Z\to \ell^{+}\ell^{-}$ and $Z\to
\nu\bar{\nu}$. We calculate the cross sections of signal and relevant SM
backgrounds. After a fast simulation of the signal and background events, the
exclusion limit at 95\% confidence level and $5\sigma$ discovery prospects on
the parameters (the coupling strength $\kappa_{B}$ and the VLQ-$B$ mass) have
been, respectively, presented at the future CLIC with centre of mass energy
$\sqrt{s}=3$ TeV and integrated luminosity of 5~ab$^{-1}$.",0
168,"We report novel flow dynamics at the interface of liquid and gas through
nanofluidic pores without applying any external driving force. Rayleigh
Taylor-instability of water and air in sub-100 nanometer fluidic pores in a
micrometre square domain of water and air are studied. We analyse it in the
context of parameters, such as applied pressure, position to pore size ratio of
the nanofluidic pore, gravity, and density. The paper also verifies the flow
velocity equation with the simulation results and discuss the mass transfer
efficiency of such flow structures. We are the first to report a self-driven
switching mechanism of nanofluidic flow from ON to OFF or vice versa. A highly
nonlinear complex nature of fluid dynamics is observed in nanometric
length-scale, which is also one of the first studies in room temperature.
Self-driven nanofluidics will have a large positive impact on healthcare,
net-zero sustainable energy production, and fundamental physics of fluid
dynamics.",0
169,"We present results for the nucleon structure functions and form factors
obtained from 2+1 flavor lattice QCD with physical light quark masses
($m_{\pi}=135$ MeV) in a large spatial extent of about 10 fm. Our calculations
are performed with the PACS10 gauge configurations generated by the PACS
Collaboration with the six stout-smeared ${\mathscr{O}}(a)$ improved
Wilson-clover quark action and Iwasaki gauge action at $\beta=1.82$ and $2.00$
corresponding to lattice spacings of $0.085$ fm and $0.064$ fm respectively.
The lower moments of structure functions, $\langle x \rangle_{u-d}$ and
$\langle x \rangle_{\Delta u - \Delta d}$ given by the twist-2 operators being
properly renormalized, are evaluated in the $\overline{\rm MS}$ scheme at the
renormalization scale of 2 GeV only at $\beta=1.82$, since the renormalization
factors at $\beta=2.00$ have not yet determined nonperturbatively in the RI/MOM
scheme. Instead, at two lattice spacings, we evaluate appropriate ratios of
$g_{A}/g_{V}$ and $\langle x \rangle_{u-d}/\langle x \rangle_{\Delta u -\Delta
d}$, which are not renormalized in the continuum limit. These quantities thus
can be directly compared with the experimental data without the
renormalization.",0
170,"We consider the production of matter and radiation during reheating after
inflation, restricting our attention solely to gravitational interactions.
Processes considered are the exchange of a graviton, $h_{\mu \nu}$, involved in
the scattering of the inflaton or particles in the newly created radiation
bath. In particular, we consider the gravitational production of dark matter
(scalar or fermionic) from the thermal bath as well as from scattering of the
inflaton condensate. We also consider the gravitational production of radiation
from inflaton scattering. In the latter case, we also derive a lower bound on
the maximal temperature of order of $10^{12}$ GeV for a typical
$\alpha-$attractor scenario from $\phi \phi \rightarrow h_{\mu \nu}
\rightarrow$ Standard Model fields (dominated by the production of Higgs
bosons). This lower gravitational bound becomes the effective maximal
temperature for reheating temperatures, $T_{\rm{RH}} \lesssim 10^9$ GeV. The
processes we consider are all minimal in the sense that they are present in any
non-minimal extension of the Standard Model theory based on Einstein gravity
and can not be neglected. We compare each of these processes to determine their
relative importance in the production of both radiation and dark matter.",0
171,"Turbulence simulation with classical numerical solvers requires very
high-resolution grids to accurately resolve dynamics. Here we train learned
simulators at low spatial and temporal resolutions to capture turbulent
dynamics generated at high resolution. We show that our proposed model can
simulate turbulent dynamics more accurately than classical numerical solvers at
the same low resolutions across various scientifically relevant metrics. Our
model is trained end-to-end from data and is capable of learning a range of
challenging chaotic and turbulent dynamics at low resolution, including
trajectories generated by the state-of-the-art Athena++ engine. We show that
our simpler, general-purpose architecture outperforms various more specialized,
turbulence-specific architectures from the learned turbulence simulation
literature. In general, we see that learned simulators yield unstable
trajectories; however, we show that tuning training noise and temporal
downsampling solves this problem. We also find that while generalization beyond
the training distribution is a challenge for learned models, training noise,
convolutional architectures, and added loss constraints can help. Broadly, we
conclude that our learned simulator outperforms traditional solvers run on
coarser grids, and emphasize that simple design choices can offer stability and
robust generalization.",0
172,"Thermal transport through a Coulomb-blockade quantum dot (QD) coupled to two
metallic leads is studied using five different approaches to the master
equation in which sequential and coutuneling terms are taken into account. In
the presence of intradot Coulomb interactions, a plateau in the
thermo-particle, the heat, and the energy currents is seen. The current plateau
diminishes at a high thermal bias between the leads. It is shown that the
Pauli, the Redfield, the Lindblad-type equation with first order tunneling
rates, and first-order von-Neumann master equations give very similar thermal
transport indicating the conservation of coherency in the electron transport in
sequential tunneling between the QD and leads. In contrast, the thermal
transport is suppressed when coutuneling processes are taken into account via a
second-order von-Neumann master equation. The consideration of second order
effects with respect to the QD-leads coupling brings in a wealth of virtual
processes at the contact to each lead. These virtual processes directly weaken
the effects of the contribution of the first order direct processes to the
overall transport, and introduce important other aspects of the transport, as
level broadening, energy shifts, and lifetimes in the time-domain. As a result
the current plateau formed via the Coulomb interaction diminishes, when second
order and cotunneling processes are considered.",0
173,"We report $^{121}$Sb nuclear quadrupole resonance (NQR) measurements on
kagome superconductor CsV$_3$Sb$_5$ with $T_{\rm c}=2.5$ K. $^{121}$Sb NQR
spectra split after a charge density wave (CDW) transition at $94$ K, which
demonstrates a commensurate CDW state. The coexistence of the high temperature
phase and the CDW phase between $91$ K and $94$ K manifests that it is a first
order phase transition. The CDW order exhibits Tri-Hexagonal deformation with a
lateral shift between the adjacent kagome layers, which is consistent with $2
\times 2 \times 2$ superlattice modulation. The superconducting state coexists
with CDW order and shows a conventional s-wave behavior in the bulk state.",0
174,"Context. Kink waves are routinely observed in coronal loops. Resonant
absorption is a well-accepted mechanism that extracts energy from kink waves.
Nonlinear kink waves are know to be affected by the Kelvin-Helmholtz
instability. However, all previous numerical studies consider linearly
polarized kink waves. Aims. We study the properties of circularly polarized
kink waves on straight plasma cylinders, for both standing and propagating
waves, and compare them to the properties of linearly polarized kink waves.
Methods. We use the code MPI-AMRVAC to solve the full 3D Magnetohydrodynamic
(MHD) equations for a straight magnetic cylinder, excited by both standing and
propagating circularly polarized kink (m = 1) modes. Results. The damping due
to resonant absorption is independent of the polarization state. The morphology
or appearance of the induced resonant flow is different for the two
polarizations, however, there are essentially no differences in the
forward-modeled Doppler signals. For nonlinear oscillations, the growth rate of
small scales is determined by the total energy of the oscillation rather than
the perturbation amplitude. We discuss possible implications and seismological
relevance.",0
175,"We compute an effective potential between two fixed sources in light-front
quantization of a quenched scalar Yukawa theory that models the interaction of
complex scalar fields through the exchange of a neutral scalar. Despite the
breaking of explicit rotational symmetry by the use of light-front coordinates,
the effective potential is rotationally symmetric and matches the standard
Yukawa potential for scalar exchange. The neutral scalar field is represented
by a coherent state, which is obtained nonperturbatively as an eigenstate of
our model Hamiltonian, with the eigenenergy determining the effective
potential. The sources are represented by wave packets that are fixed with
respect to ordinary time, but move in light-front coordinates. The theory is
quenched, to remove pair-production processes that would otherwise cause the
spectrum to be unbounded from below.",0
176,"This study considers a new multi-term urn process that has a correlation in
the same term and temporal correlation. The objective is to clarify the
relationship between the urn model and the Hawkes process. Correlation in the
same term is represented by the P\'{o}lya urn model and the temporal
correlation is incorporated by introducing the conditional initial condition.
In the double-scaling limit of this urn process, the self-exciting negative
binomial distribution (SE-NBD) process, which is a marked point process, is
obtained. In the standard continuous limit, this process becomes the Hawkes
process, which has no correlation in the same term. The difference is the
variance of the intensity function in that the phase transition from the steady
to the non-steady state can be observed. The critical point, at which the power
law distribution is obtained, is the same for the Hawkes and the urn processes.
These two processes are used to analyze empirical data of financial default to
estimate the parameters of the model. For the default portfolio, the results
produced by the urn process are superior to those obtained with the Hawkes
process and confirm self-excitation.",0
177,"The Free Energy Principle (FEP) states that under suitable conditions of weak
coupling, random dynamical systems with sufficient degrees of freedom will
behave so as to minimize an upper bound, formalized as a variational free
energy, on surprisal (a.k.a., self-information). This upper bound can be read
as a Bayesian prediction error. Equivalently, its negative is a lower bound on
Bayesian model evidence (a.k.a., marginal likelihood). In short, certain random
dynamical systems evince a kind of self-evidencing. Here, we reformulate the
FEP in the formal setting of spacetime-background free, scale-free quantum
information theory. We show how generic quantum systems can be regarded as
observers, which with the standard freedom of choice assumption become agents
capable of assigning semantics to observational outcomes. We show how such
agents minimize Bayesian prediction error in environments characterized by
uncertainty, insufficient learning, and quantum contextuality. We show that in
its quantum-theoretic formulation, the FEP is asymptotically equivalent to the
Principle of Unitarity. Based on these results, we suggest that biological
systems employ quantum coherence as a computational resource and - implicitly -
as a communication resource. We summarize a number of problems for future
research, particularly involving the resources required for classical
communication and for detecting and responding to quantum context switches.",0
178,"Abstract We present finite-temperature Monte Carlo studies of a 2D
random-anisotropy magnet on lattices containing one million spins. The
correlated spin-glass state predicted by analytical theories is reproduced in
simulations, as are the field-cooled and zero-field-cooled magnetization curves
observed in experiments. The orientations of lattice spins begin to freeze when
the temperature is lowered. The freezing transition is due to the energy
barriers generated by the random anisotropy rather than due to random
interactions in conventional spin-glasses. We describe freezing by introducing
the time-dependent spin-glass order parameter $q$ and the spin-melting time
$\tau_{M}$ defined via $q=\tau_{M}/t$ above freezing, where t is the time of
the experiment represented by the number of Monte Carlo steps.",0
179,"The reflection, refraction, and transmission of large-scale extreme
ultraviolet (EUV) waves (collectively, secondary waves) have been observed
during their interactions with coronal structures such as active regions (ARs)
and coronal holes (CHs). However, the effect of the total reflection of EUV
waves has not been reported in the literature. Here, we present the first
unambiguous observational evidence of the total reflection of a quasi-periodic
EUV wave train during its interaction with a polar CH. The event occurred in
NOAA AR 12473, located close to the southeast limb of the solar disk, and was
characterized by a jet-like CME. In this study, we focus in particular on the
driving mechanism s of the quasi-periodic wave train and the total reflection
effect at the CH boundary. We find that the periods of the incident and the
reflected wave trains are both about 100 seconds. The excitation of the
quasi-periodic wave train was possibly due to the intermittent energy release
in the associated flare since its period is similar to that of the
quasi-periodic pulsations in the associated flare. Our observational results
showed that the reflection of the wave train at the boundary of the CH was a
total reflection because the measured incidence and critical angles satisfy the
theory of total reflection, i.e., the incidence angle is less than the critical
angle.",0
180,"Molecular motors belonging to the kinesin and myosin super family hydrolyze
ATP by cycling through a sequence of chemical states. These cytoplasmic motors
are dimers made up of two linked identical monomeric globular proteins. Fueled
by the free energy generated by ATP hydrolysis, the motors walk on polar tracks
(microtubule or filamentous actin) processively, which means that only one head
detaches and executes a mechanical step while the other stays bound to the
track. Thus, the one motor head must regulate chemical state of the other,
referred to as ""gating"", a concept that is not fully understood. Inspired by
experiments, showing that only a fraction of the energy from ATP hydrolysis is
used to advance the kinesin motors against load, we demonstrate that additional
energy is used for coordinating the chemical cycles of the two heads in the
dimer - a feature that characterizes gating. To this end, we develop a general
framework based on information theory and stochastic thermodynamics, and
establish that gating could be quantified in terms of information flow between
the motor heads. Applications of the theory to kinesin-1 and Myosin V show that
information flow occurs, with positive cooperativity, at external resistive
loads that are less than a critical value, $F_c$. When force exceeds $F_c$,
effective information flow ceases. Interestingly, $F_c$, which is independent
of the input energy generated through ATP hydrolysis, coincides with force at
which the probability of backward steps starts to increase. Our findings
suggest that transport efficiency is optimal only at forces less than $F_c$,
which implies that these motors must operate at low loads under $\textit{in
vivo}$ conditions.",0
181,"We demonstrate that Robb-Geroch's definition of a relativistic interval
admits a simple and fairly natural generalization leading to a Finsler
extension of special relativity. Another justification for such an extension
goes back to the works of Lalan and Alway and, finally, was put on a solid
basis and systematically investigated by Bogoslovsky under the name
""Special-relativistic theory of locally anisotropic space-time"". The isometry
group of this space-time, $\mathrm{DISIM}_b(2)$, is a deformation of the Cohen
and Glashow's very special relativity symmetry group $\mathrm{ISIM(2)}$. Thus,
the deformation parameter b can be regarded as an analog of the cosmological
constant characterizing the deformation of the Poincare group into the de
Sitter (anti-de Sitter) group. The simplicity and naturalness of Finslerian
extension in the context of this article adds weight to the argument that the
possibility of a nonzero value of $b$ should be carefully considered.",0
182,"Multi-band emission from radio to ultra-high energy gamma-rays in the Crab
Nebula has been detected. To explain the observed results, non-thermal photon
production \textbf{in} the Crab Nebula is carefully studied in a spatially
dependent lepto-hadronic model. In our model, the dynamical evolution of the
PWN is simulated in a spherically symmetric system. Both electrons and protons
are accelerated at the termination shock. The relevant particle propagation
equations as well as the photon evolving equation are simultaneously solved.
For the Crab Nebula, our results reveal that the observed multi-band photon
spectra can be well reproduced with reasonable model parameters. In particular,
the photons with energy $\gtrsim 200$ TeV are mainly contributed by the
hadronic component via proton-proton interaction. The contribution of the
hadronic component depends on both proton spectral index $\alpha_{\rm p}$ and
number density $n_{\rm H}$ of medium within the PWN. Besides, high energy
neutrino fluxes are predicted with variable proton spectral indices. The
predicted fluxes are not only far below the sensitivities of current neutrino
observatories, but also beneath the atmospheric neutrino background with energy
less than $\sim 40$ TeV. Moreover, the calculated radial profiles of surface
brightness and spectral index are presented.",0
183,"Teleparallel Horndeski theory offers an avenue through which to circumvent
the speed constraint of gravitational waves in an efficient manner. However,
this provides an even larger plethora of models due to the increase in action
terms. In this work we explore these models in the context of cosmological
systems. Using Noether point symmetries, we classify the dynamical systems that
emerge from Teleparallel Horndeski cosmologies. This approach is very effective
at selecting specific models in the general class of second-order Teleparallel
scalar-tensor theories, as well as for deriving exact solutions within a
cosmological context. By iterating through the Lagrangians selected through the
Noether symmetries, we solve for a number of cosmological systems which
provides new cosmological systems to be studied.",0
184,"Ultrasonic irradiation of liquids, such as water-alcohol solutions, results
in cavitation or the formation of small bubbles. Cavitation bubbles are
generated in real solutions without the use of optical traps making our system
as close to real conditions as possible. Under the action of the ultrasound,
bubbles can grow, oscillate, and eventually, they collapse or decompose. We
apply the mathematical method of separation of motions to interpret the
acoustic effect on the bubbles. While in most situations, the spherical shape
of a bubble is the most energetically profitable as it minimizes the surface
energy, when the acoustic frequency is in resonance with the natural frequency
of the bubble, shapes with the dihedral symmetry emerge. Some of these
resonance shapes turn unstable, so the bubble decomposes. It turns out that
bubbles in the solutions of different concentrations (with different surface
energies and densities) attain different evolution paths. While it is difficult
to obtain a deterministic description of how the solution concentration affects
bubble dynamics, it is possible to separate images with different
concentrations by applying the Artificial Neural Network (ANN) algorithm. An
ANN was trained to detect the concentration of alcohol in a water solution
based on the bubble images. This indicates that Artificial Intelligence (AI)
methods can complement deterministic analysis in non-equilibrium, near-unstable
situations.",0
185,"This article presents numerical work on a special case of the cosmological
semiclassical Einstein equation (SCE). The SCE describes the interaction of
relativistic quantum matter by the expected value of the renormalized
stress-energy tensor of a quantum field with classical gravity. Here we
consider a free massless scalar field with general (not necessarily conformal)
coupling to curvature. In a cosmological scenario with flat spatial sections
for special choices of the initial conditions, we observe a separation of the
dynamics of the quantum degrees of freedom from the dynamics of the scale
factor, which extends a classical result by Starobinsky to general coupling.
For this new equation of fourth order for the dynamics of the scale factor, we
study numerical solutions. Typical solutions show a radiation-like Big Bang for
the early universe and de Sitter-like expansion for the late universe. We
discuss a specific solution to the cosmological horizon problem that can be
produced by tuning parameters in the given equation. Although the model
proposed here only contains massless matter, we give a preliminary comparison
of the obtained cosmology with the $\Lambda$CDM standard model of cosmology and
investigate parameter ranges in which the new models to a certain extent
assimilates standard cosmology.",0
186,"Quantum Neural Networks (QNNs) with random structures have poor trainability
due to the exponentially vanishing gradient as the circuit depth and the qubit
number increase. This result leads to a general belief that a deep QNN will not
be feasible. In this work, we provide the first viable solution to the
vanishing gradient problem for deep QNNs with theoretical guarantees.
Specifically, we prove that for circuits with controlled-layer architectures,
the expectation of the gradient norm can be lower bounded by a value that is
independent of the qubit number and the circuit depth. Our results follow from
a careful analysis of the gradient behaviour on parameter space consisting of
rotation angles, as employed in almost any QNNs, instead of relying on
impractical 2-design assumptions. We explicitly construct examples where only
our QNNs are trainable and converge, while others in comparison cannot.",0
187,"Recent years have witnessed considerable work on the development of lead-free
piezoelectric ceramic materials and their structure-property correlations. The
development of piezo response is a strong function of phase evolution in these
materials. In this work, we report the effect of Ru doping and consequent phase
evolution on maximization of piezoelectric response of polycrystalline
lead-free barium titanate, depicted as Ba(RuxTi1-x)O3 (BRT). The samples were
prepared in a narrow compositional range of using conventional solid-state
reaction method. Ru doping increases the leakage current of BaTiO3 samples
attributed to increased oxygen vacancy concentration due to substitution of
Ti4+ by Ru3+. Detailed structural analysis reveals samples exhibiting
coexistence of tetragonal (space group: P4mm) and orthorhombic (space group:
Amm2) structured phases near the room temperature reveal relatively enhanced
piezoelectric properties. The BRT sample with Ru content of 2 mol% yields a
maximum longitudinal piezoelectric coefficient, d33 of 269 pC/N, a high strain
value of 0.16% with a large remnant polarization of 19 micro C/cm2 and a
coercive field of 5.8 kV/cm. We propose that the 4d orbital of Ruthenium plays
a crucial role in improving the functional properties and in decreasing the
ferroelectric Curie temperature (Tc). Our work provides clues into tailoring
the phase evolution for designing lead-free piezoelectric materials with
enhanced piezoelectric property.",0
188,"We establish an abstract quenched linear response result for random dynamical
systems, which we then apply to the case of smooth expanding on average
cocycles on the unit circle. In sharp contrast to the existing results in the
literature, we deal with the class of random dynamics that does not necessarily
exhibit uniform decay of correlations. Our techniques rely on the
infinite-dimensional ergodic theory and in particular, on the study of the top
Oseledets space of a parametrized transfer operator cocycle.",0
189,"We study two-dimensional Ising spins, evolving through reinforcement learning
using their state, action and reward. State of a spin is defined on whether it
is in the majority or minority with its nearest neighbours. Further, spin
performs the action using {\epsilon}-greedy algorithm to either flip or remain
the same. The parameter {\epsilon} plays the role equivalent to the temperature
in the original Ising model. We find a phase transition from long-ranged
ordered to disordered state as we tune {\epsilon} from small to large values.
In analogy with the phase transition in Ising model, we calculate the critical
{\epsilon} and the three critical exponents {\beta}, {\gamma}, {\nu} of
magnetisation, susceptibility and correlation length respectively. A
hyperscaling relation d{\nu}= 2{\beta}+{\gamma} is obtained between the three
exponents. Hence, we introduced and studied a model-free Ising system using
reinforcement learning, and reproduced all the results of the standard original
Ising model. Our study provides a new approach to examine the many-particle
systems.",0
190,"(Sender-)Deniable encryption provides a very strong privacy guarantee: a
sender who is coerced by an attacker into ""opening"" their ciphertext
after-the-fact is able to generate ""fake"" local random choices that are
consistent with any plaintext of their choice. The only known fully-efficient
constructions of public-key deniable encryption rely on indistinguishability
obfuscation (iO) (which currently can only be based on sub-exponential hardness
assumptions). In this work, we study (sender-)deniable encryption in a setting
where the encryption procedure is a quantum algorithm, but the ciphertext is
classical. We propose two notions of deniable encryption in this setting. The
first notion, called quantum deniability, parallels the classical one. We give
a fully efficient construction satisfying this definition, assuming the quantum
hardness of the Learning with Errors (LWE) problem. The second notion,
unexplainability, starts from a new perspective on deniability, and leads to a
natural common view of deniability in the classical and quantum settings. We
give a construction which is secure in the random oracle model, assuming the
quantum hardness of LWE. Notably, our construction satisfies a strong form of
unexplainability which is impossible to achieve classically, thus highlighting
a new quantum phenomenon that may be of independent interest.",0
191,"Applying direct growth and deposition of optical surfaces holds great promise
for the advancement of future nanophotonic technologies. Here, we report on a
chemical vapor deposition (CVD) technique for depositing amorphous selenium
(a-Se) spheres by desorption of selenium from Bi2Se3 and re-adsorption on the
substrate. We utilize this process to grow scalable, large area Se spheres on
several substrates and characterize their Mie-resonant response in the
mid-infrared (MIR) spectral range. We demonstrate size-tunable Mie resonances
spanning the 2-16 um spectral range, for single isolated resonators and large
area ensembles, respectively. We further demonstrate strong absorption dips of
up to 90% in ensembles of particles in a broad MIR range. Finally, we show that
ultra-high-Q resonances arise in the case where Se Mie-resonators are coupled
to low-loss epsilon-near-zero (ENZ) substrates. These findings demonstrate the
enabling potential of amorphous Selenium as a versatile and tunable
nanophotonic material that may open up avenues for on-chip MIR spectroscopy,
chemical sensing, spectral imaging and large area metasurface fabrication.",0
192,"Interferometric methods for detecting the motion of a levitated nanoparticle
provide a route to the quantum ground state, but such methods are currently
limited by mode mismatch between the reference beam and the dipolar field
scattered by the particle. Here we demonstrate a self-interference method to
detect the particle's motion that solves this problem. A Paul trap confines a
charged dielectric nanoparticle in high vacuum, and a mirror retro-reflects the
scattered light. We measure the particle's motion with a sensitivity of
$1.7\times 10^{-12} \text{m}/\sqrt{\text{Hz}}$, corresponding to a detection
efficiency of 2.1%, with a numerical aperture of 0.18. As an application of
this method, we cool the particle, via feedback, to temperatures below those
achieved in the same setup using a standard position measurement.",0
193,"The large thermal Hall conductivity recently detected in Mott insulating
cuprates has been attributed to chiral neutral spin excitations. A quantum spin
liquid with Majorana excitations, Chern number +/-4 and large thermal Hall
conductivity is found to be an excited state of a frustrated Heisenberg model
on the square lattice. Using a Majorana mean-field theory and exact
diagonalizations, we explore two possible routes to achieve this chiral quantum
spin liquid, an orbital effect of an applied magnetic field and spin orbit
couplings as present in cuprates. In particular, we show how only the orbital
magnetic field allows this topological phase to be the ground state, while it
remains an excited state of the Majorana mean field under the
Dzyaloshinskii-Moriya terms. We interpret the large thermal Hall effect
observed in Mott cuprates from their close proximity to a transition to a
Majorana chiral quantum spin liquid which can be induced by an external
magnetic field.",0
194,"We report the discovery of a sub-Jovian-mass planet, OGLE-2014-BLG-0319Lb.
The characteristics of this planet will be added into a future extended
statistical analysis of the Microlensing Observations in Astrophysics (MOA)
collaboration. The planetary anomaly of the light curve is characterized by MOA
and OGLE survey observations and results in three degenerate models with
different planetary mass-ratios of $q=(10.3,6.6,4.5)\times10^{-4}$,
respectively. We find that the last two models require unreasonably small
lens-source relative proper motions of $\mu_{\rm rel}\sim1\;{\rm mas/yr}$.
Considering Galactic prior probabilities, we rule out these two models from the
final result. We conduct a Bayesian analysis to estimate physical properties of
the lens system using a Galactic model and find that the lens system is
composed of a $0.49^{+0.35}_{-0.27}\;M_{\rm Jup}$ sub-Jovian planet orbiting a
$0.47^{+0.33}_{-0.25}\; M_{\odot}$ M-dwarf near the Galactic bulge. This
analysis demonstrates that Galactic priors are useful to resolve this type of
model degeneracy. This is important for estimating the mass ratio function
statistically. However, this method would be unlikely successful in shorter
timescale events, which are mostly due to low-mass objects, like brown dwarfs
or free-floating planets. Therefore, careful treatment is needed for estimating
the mass ratio function of the companions around such low-mass hosts which only
the microlensing can probe.",0
195,"Quantum sensors can show unprecedented sensitivities, provided they are
controlled in a very specific, optimal way. Here, we consider a spin sensor of
time-varying fields in the presence of dephasing noise, and we show that the
problem of finding the optimal pulsed control field can be mapped to the
determination of the ground state of a spin chain. We find an approximate but
analytic solution of this problem, which provides a \emph{lower bound} for the
sensor sensitivity, and a pulsed control very close to optimal, which we
further use as initial guess for realizing a fast simulated annealing
algorithm. We experimentally demonstrate the sensitivity improvement for a
spin-qubit magnetometer based on a nitrogen-vacancy center in diamond.",0
196,"We report angle resolved photoemission experiments on a newly discovered
family of kagome metals $RV_{6}Sn_{6}$ ($R$=Gd, Ho). Intrinsic bulk states and
surface states of the vanadium kagome layer are differentiated from those of
other atomic sublattices by the real-space resolution of the measurements with
a small beam spot. Characteristic Dirac cone, saddle point and flat bands of
the kagome lattice are observed. Our results establish the two-dimensional (2D)
kagome surface states as a new platform to investigate the intrinsic kagome
physics.",0
197,"The black hole scalarization in a special Einstein-scalar-Gauss-Bonnet (EsGB)
gravity has been widely investigated in recent years. Especially, the
spontaneous scalarization of scalar-free black hole in de-Sitter (dS) spacetime
possesses interesting features due to the existence of cosmological horizon. In
this work, we focus on the massive scalar field perturbation on Schwarzschild
dS (SdS) black hole in a special EsGB theory. We study the (in)stability of SdS
in frequency domain and verify the results in time domain. Then we figure out
the unstable/stable regions in $(\Lambda,\alpha)$-plane as well as in
$(m,\alpha)$-plane for various perturbation modes, where $\Lambda$, $\alpha$
and $m$ denote the cosmological constant, the GB coupling strength and the mass
of scalar field, respectively. Our study could be a good preparation for one to
further understand the black hole scalarization in dS spacetime.",0
198,"According to lattice simulations and other theoretical approaches, the scalar
glueball is the lightest state in the Yang-Mills sector of QCD. Since within
this sector the scalar glueball is stable, the scattering between two glueballs
is a well-defined process. Moreover, a glueball-glueball bound state, called
glueballonium, might exist if the attraction turns out to be large enough. In
this work, we concentrate on the formation of the glueballonium in the context
of the dilaton potential. In particular, we investigate the parameter values
for which such a glueballonium emerges.",0
199,"The large-$N$ limit of $O(N)$-symmetric bosonic field theories, or
$U(N)$-symmetric fermionic field theories, is amenable to a saddle point
approximation. As a result, there is a family of closely related algorithms for
efficient lattice simulations in this limit, even in the presence of fermionic
or real-time sign problems. These can be used to study quenches, or other
observables for which",0
200,"The effect of bias on hypothesis formation is characterized for an automated
data-driven projection pursuit neural network to extract and select features
for binary classification of data streams. This intelligent exploratory process
partitions a complete vector state space into disjoint subspaces to create
working hypotheses quantified by similarities and differences observed between
two groups of labeled data streams. Data streams are typically time sequenced,
and may exhibit complex spatio-temporal patterns. For example, given atomic
trajectories from molecular dynamics simulation, the machine's task is to
quantify dynamical mechanisms that promote function by comparing protein
mutants, some known to function while others are nonfunctional. Utilizing
synthetic two-dimensional molecules that mimic the dynamics of functional and
nonfunctional proteins, biases are identified and controlled in both the
machine learning model and selected training data under different contexts. The
refinement of a working hypothesis converges to a statistically robust
multivariate perception of the data based on a context-dependent perspective.
Including diverse perspectives during data exploration enhances
interpretability of the multivariate characterization of similarities and
differences.",1
201,"In a non-sustainable, ""over-populated"" world, what might the use of
nanotechnology-based targeted, autonomous weapons mean for the future of
humanity? In order to gain some insights, we make a simplified game-theoretical
thought experiment. We consider a population where agents play the public goods
game, and where in parallel an epidemic unfolds. Agents that are infected
defectors are killed with a certain probability and replaced by susceptible
cooperators. We show that such ""nanowars"", even if aiming to promote good
behavior and planetary health, fail not only to promote cooperation, but they
also significantly increase the probability of repetitive epidemic waves. In
fact, newborn cooperators turn out to be easy targets for defectors in their
neighborhood. Therefore, counterintuitively, the discussed intervention may
even have the opposite effect as desired, promoting defection. We also find a
critical threshold for the death rate of infected defectors, beyond which
resurgent epidemic waves become a certainty. In conclusion, we urgently call
for international regulation of nanotechnology and autonomous weapons.",1
202,"We introduce an idealized model of an intelligent forager in which higher
intelligence corresponds to a larger spatial range over which the forager can
detect food. Such a forager diffuses randomly whenever the nearest food is more
distant than the forager's detection range, $R$, and moves ballistically
towards the nearest food inside its detection range. Concomitantly, the
forager's metabolic energy cost per step is an increasing function of its
intelligence. A dumb forager wanders randomly and may miss nearby food, thus
making it susceptible to starvation. Conversely, a too-smart forager incurs a
large metabolic cost per step during its search for food and is again
susceptible to starvation. We show that the forager's lifetime is maximized at
an optimal, intermediate level of intelligence.",1
203,"Artificial neural networks for simulated motor control and robotics often
adopt generic architectures like fully connected MLPs. While general, these
tabula rasa architectures rely on large amounts of experience to learn, are not
easily transferable to new bodies, and have internal dynamics that are
difficult to interpret. In nature, animals are born with highly structured
connectivity in their nervous systems shaped by evolution; this innate
circuitry acts synergistically with learning mechanisms to provide inductive
biases that enable most animals to function well soon after birth and improve
abilities efficiently. Convolutional networks inspired by visual circuitry have
encoded useful biases for vision. However, it is unknown the extent to which
ANN architectures inspired by neural circuitry can yield useful biases for
other domains. In this work, we ask what advantages biologically inspired
network architecture can provide in the context of motor control. Specifically,
we translate C. elegans circuits for locomotion into an ANN model controlling a
simulated Swimmer agent. On a locomotion task, our architecture achieves good
initial performance and asymptotic performance comparable with MLPs, while
dramatically improving data efficiency and requiring orders of magnitude fewer
parameters. Our architecture is more interpretable and transfers to new body
designs. An ablation analysis shows that principled excitation/inhibition is
crucial for learning, while weight initialization contributes to good initial
performance. Our work demonstrates several advantages of ANN architectures
inspired by systems neuroscience and suggests a path towards modeling more
complex behavior.",1
204,"We study a stochastic individual-based model of interacting plant and
pollinator species through a bipartite graph: each species is a node of the
graph, an edge representing interactions between a pair of species. The
dynamics of the system depends on the between- and within-species interactions:
pollination by insects increases plant reproduction rate but has a cost which
can increase plant death rate, depending on pollinators density. Pollinators
reproduction is increased by the resources harvested on plants. Each species is
characterized by a trait corresponding to its degree of generalism. This trait
determines the structure of the interactions graph and the quantity of
resources exchanged between species. Our model includes in particular nested or
modular networks. Deterministic approximations of the stochastic measure-valued
process by systems of ordinary differential equations or integro-differential
equations are established and studied, when the population is large or when the
graph is dense and can be replaced with a graphon. The long-time behaviors of
these limits are studied and central limit theorems are established to quantify
the difference between the discrete stochastic individual-based model and the
deterministic approximations. Finally, studying the continuous limits of the
interaction network and the resulting PDEs, we show that nested
plant-pollinator communities are expected to collapse towards a coexistence
between a single pair of species of plants and pollinators.",1
205,"Pattern detection and string matching are fundamental problems in computer
science and the accelerated expansion of bioinformatics and computational
biology have made them a core topic for both disciplines. The SARS-CoV-2
pandemic has made such problems more demanding with hundreds or thousands of
new genome variants discovered every week, because of constant mutations, and
there is a desperate need for fast and accurate analyses. The requirement for
computational tools for genomic analyses, such as sequence alignment, is very
important, although, in most cases the resources and computational power
required are enormous. The presented Multiple Genome Analytics Framework
combines data structures and algorithms, specifically built for text mining and
pattern detection, that can help to efficiently address several computational
biology and bioinformatics problems concurrently with minimal resources. A
single execution of advanced algorithms, with space and time complexity
O(nlogn), is enough to acquire knowledge on all repeated patterns that exist in
multiple genome sequences and this information can be used from other
meta-algorithms for further meta-analyses. The potential of the proposed
framework is demonstrated with the analysis of more than 300,000 SARS-CoV-2
genome sequences and the detection of all repeated patterns with length up to
60 nucleotides in these sequences. These results have been used to provide
answers to questions such as common patterns among all variants, sequence
alignment, palindromes and tandem repeats detection, different organism genome
comparisons, polymerase chain reaction primers detection, etc.",1
206,"Deep sequencing has become one of the most popular tools for transcriptome
profiling in biomedical studies. While an abundance of computational methods
exists for ""normalizing"" sequencing data to remove unwanted between-sample
variations due to experimental handling, there is no consensus on which
normalization is the most suitable for a given data set. To address this
problem, we developed ""DANA"" - an approach for assessing the performance of
normalization methods for microRNA sequencing data based on biology-motivated
and data-driven metrics. Our approach takes advantage of well-known biological
features of microRNAs for their expression pattern and chromosomal clustering
to simultaneously assess (1) how effectively normalization removes handling
artifacts, and (2) how aptly normalization preserves biological signals. With
DANA, we confirm that the performance of eight commonly used normalization
methods vary widely across different data sets and provide guidance for
selecting a suitable method for the data at hand. Hence, it should be adopted
as a routine preprocessing step (preceding normalization) for microRNA
sequencing data analysis. DANA is implemented in R and publicly available at
https://github.com/LXQin/DANA.",1
207,"Genome sequencing is the basis for many modern biological and medicinal
studies. With recent technological advances, metagenomics has become a problem
of interest. This problem entails the analysis and reconstruction of multiple
DNA sequences from different sources. Shotgun genome sequencing works by
breaking up long DNA sequences into shorter segments called reads. Given this
collection of reads, one would like to reconstruct the original collection of
DNA sequences. For experimental design in metagenomics, it is important to
understand how the minimal read length necessary for reliable reconstruction
depends on the number and characteristics of the genomes involved. Utilizing
simple probabilistic models for each DNA sequence, we analyze the
identifiability of collections of M genomes of length N in an asymptotic regime
in which N tends to infinity and M may grow with N. Our first main result
provides a threshold in terms of M and N so that if the read length exceeds the
threshold, then a simple greedy algorithm successfully reconstructs the full
collection of genomes with probability tending to one. Our second main result
establishes a lower threshold in terms of M and N such that if the read length
is shorter than the threshold, then reconstruction of the full collection of
genomes is impossible with probability tending to one.",1
208,"Rapid growth of genetic databases means huge savings from improvements in
their data compression, what requires better inexpensive statistical models.
This article proposes automatized optimizations e.g. of Markov-like models,
especially context binning and model clustering. While it is popular to cut low
bits of context, proposed context binning optimizes such reduction as tabled:
state=bin[context] determining probability distribution, this way extracting
nearly all useful information also from very large contexts, into a small
number of states. Model clustering uses k-means clustering in space of general
statistical models, allowing to optimize a few models (as cluster centroids) to
be chosen e.g. separately for each read. There are also briefly discussed some
adaptivity techniques to include data non-stationarity. This article is work in
progress, to be expanded in the future.",1
209,"Accurate knowledge of RNA hybridization is essential for understanding RNA
structure and function. Here we mechanically unzip and rezip a 2-kbp RNA
hairpin and derive the 10 nearest-neighbor base pair (NNBP) RNA free energies
in sodium and magnesium with 0.1 kcal/mol precision using optical tweezers.
Notably, force-distance curves (FDCs) exhibit strong irreversible effects with
hysteresis and several intermediates, precluding the extraction of the NNBP
energies with currently available methods. The combination of a suitable RNA
synthesis with a tailored pulling protocol allowed us to obtain the fully
reversible FDCs necessary to derive the NNBP energies. We demonstrate the
equivalence of sodium and magnesium free-energy salt corrections at the level
of individual NNBP. To characterize the irreversibility of the
unzipping-rezipping process, we introduce a barrier energy landscape of the
stem-loop structures forming along the complementary strands, which compete
against the formation of the native hairpin. This landscape correlates with the
hysteresis observed along the FDCs. RNA sequence analysis shows that base
stacking and base pairing stabilize the stem-loops that kinetically trap the
long-lived intermediates observed in the FDC. Stem-loops formation appears as a
general mechanism to explain a wide range of behaviors observed in RNA folding.",1
210,"The role of transportation vehicles, pig movement between farms, proximity to
infected premises, and feed deliveries has not been fully considered in the
dissemination dynamics of porcine epidemic diarrhea virus (PEDV). This has
limited efforts for disease control and elimination restricting the development
of risk-based resource allocation to the most relevant modes of PEDV
dissemination. Here, we modeled nine modes of between-farm transmission
pathways including farm-to-farm proximity (local transmission), contact network
of pig farm movements between sites, four different contact networks of
transportation vehicles (vehicles that transport pigs from farm-to-farm, pigs
to markets, feed distribution and crew), the volume of animal by-products
within feed diets (e.g. animal fat and meat and bone meal) to reproduce PEDV
transmission dynamics. The model was calibrated in space and time with weekly
PEDV outbreaks. We investigated the model performance to identify outbreak
locations and the contribution of each route in the dissemination of PEDV. The
model estimated that 42.7% of the infections in sow farms were related to
vehicles transporting feed, 34.5% of infected nurseries were associated with
vehicles transporting pigs to farms, and for both farm types, pig movements or
local transmission were the next most relevant routes. On the other hand,
finishers were most often (31.4%) infected via local transmission, followed by
the vehicles transporting feed and pigs to farm networks. Feed ingredients did
not significantly improve model calibration metrics. The proposed modeling
framework provides an evaluation of PEDV transmission dynamics, ranking the
most important routes of PEDV dissemination and granting the swine industry
valuable information to focus efforts and resources on the most important
transmission routes.",1
211,"Neuronal cable theory is usually derived from an electric analogue of the
membrane, which contrasts with the slow movement of ions in aqueous media. We
show here that it is possible to derive neuronal cable equations from a
different perspective, based on the laws of hydrodynamic motion of charged
particles (Navier-Stokes equations). This results in similar cable equations,
but with additional contributions arising from nonlinear interactions inherent
to fluid dynamics, and which may shape the integrative properties of the
neurons.",1
212,"Microstructural models of soft tissue deformation are important in
applications including artificial tissue design and surgical planning. The
basis of these models, and their advantage over their phenomenological
counterparts, is that they incorporate parameters that are directly linked to
the tissue's microscale structure and constitutive behaviour and can therefore
be used to predict the effects of structural changes to the tissue. Although
studies have attempted to determine such parameters using diverse,
state-of-the-art, experimental techniques, values ranging over several orders
of magnitude have been reported, leading to uncertainty in the true parameter
values and creating a need for models that can handle such uncertainty. We
derive a microstructural, hyperelastic model for transversely isotropic soft
tissues and use it to model the mechanical behaviour of tendons. To account for
parameter uncertainty, we employ a Bayesian approach and apply an adaptive
Markov chain Monte Carlo algorithm to determine posterior probability
distributions for the model parameters. The obtained posterior distributions
are consistent with parameter measurements previously reported and enable us to
quantify the uncertainty in their values for each tendon sample that was
modelled. This approach could serve as a prototype for quantifying parameter
uncertainty in other soft tissues.",1
213,"Motivation: Bacteriophages are viruses infecting bacteria. Being key players
in microbial communities, they can regulate the composition/function of
microbiome by infecting their bacterial hosts and mediating gene transfer.
Recently, metagenomic sequencing, which can sequence all genetic materials from
various microbiome, has become a popular means for new phage discovery.
However, accurate and comprehensive detection of phages from the metagenomic
data remains difficult. High diversity/abundance, and limited reference genomes
pose major challenges for recruiting phage fragments from metagenomic data.
Existing alignment-based or learning-based models have either low recall or
precision on metagenomic data.
  Results: In this work, we adopt the state-of-the-art language model,
Transformer, to conduct contextual embedding for phage contigs. By constructing
a protein-cluster vocabulary, we can feed both the protein composition and the
proteins' positions from each contig into the Transformer. The Transformer can
learn the protein organization and associations using the self-attention
mechanism and predicts the label for test contigs. We rigorously tested our
developed tool named PhaMer on multiple datasets with increasing difficulty,
including quality RefSeq genomes, short contigs, simulated metagenomic data,
mock metagenomic data, and the public IMG/VR dataset. All the experimental
results show that PhaMer outperforms the state-of-the-art tools. In the real
metagenomic data experiment, PhaMer improves the F1-score of phage detection by
27\%.",1
214,"One of the most effective strategies to mitigate the global spreading of a
pandemic (e.g., COVID-19) is to shut down international airports. From a
network theory perspective, this is since international airports and flights,
essentially playing the roles of bridge nodes and bridge links between
countries as individual communities, dominate the epidemic spreading
characteristics in the whole multi-community system. Among all epidemic
characteristics, the peak infection rate, $I_{\max}$, is a decisive factor in
evaluating an epidemic strategy given limited capacity of medical resources,
but is seldom considered in multi-community models. In this paper, we study a
general two-community system interconnected by a fraction $r$ of bridge nodes
and its dynamic properties, especially $I_{\max}$, under the evolution of the
Susceptible-Infected-Recovered (SIR) model. Comparing the characteristic time
scales of different parts of the system allows us to analytically derive the
asymptotic behavior of $I_{\max}$ with $r$, as $r\rightarrow 0$, which follows
different power-law relations in each regime of the phase diagram. We also
detect crossovers where $I_{\max}$ follows power-law relations from different
regimes on both sides, when $r$ is not small enough. Our results enable a
better prediction of the effectiveness of strategies acting on bridge nodes,
denoted by the power-law exponent $\epsilon_I$ as in $I_{\max}\propto
r^{1/\epsilon_I}$.",1
215,"Objectives: To enhance monitoring of high-burden foodborne pathogens, there
is opportunity to combine pangenome data with network analysis.
  Methods: Salmonella enterica subspecies Enterica serovar Enteritidis isolates
were referred to the New South Wales (NSW) Enteric Reference Laboratory between
August 2015 and December 2019 (1033 isolates in total), inclusive of a
confirmed outbreak. All isolates underwent whole genome sequencing. Distances
between genomes were quantified by in silico MLVA as well as core SNPs, which
informed construction of undirected networks. Prevalence-centrality spaces were
generated from the undirected networks. Components on the undirected SNP
network were considered alongside a phylogenetic tree representation.
  Results: Outbreak isolates were identifiable as distinct components on the
MLVA and SNP networks. The MLVA network based centrality/prevalence space did
not delineate the outbreak, whereas the outbreak was clearly delineated in the
SNP network based centrality/prevalence space. Components on the undirected SNP
network showed a high concordance to the SNP clusters based on phylogenetic
analysis.
  Conclusions: Bacterial whole genome data in network based analysis can
improve the resolution of population analysis. High concordance of network
components and SNP clusters is promising for rapid population analyses of
foodborne Salmonella spp. due to the low overhead of network analysis.",1
216,"Scientists world-wide are putting together massive efforts to understand how
the biodiversity that we see on Earth evolved from single-cell organisms at the
origin of life and this diversification process is represented through the Tree
of Life. Low sampling rates and high heterogeneity in the rate of evolution
across sites and lineages produce a phenomenon denoted ""long branch attraction""
(LBA) in which long non-sister lineages are estimated to be sisters regardless
of their true evolutionary relationship. LBA has been a pervasive problem in
phylogenetic inference affecting different types of methodologies from
distance-based to likelihood-based. Here, we present a novel neural network
model that outperforms standard phylogenetic methods and other neural network
implementations under LBA settings. Furthermore, unlike existing neural network
models, our model naturally accounts for the tree isomorphisms via permutation
invariant functions which ultimately result in lower memory and allows the
seamless extension to larger trees.",1
217,"We introduce an epistemic information measure between two data streams, that
we term $influence$. Closely related to transfer entropy, the measure must be
estimated by epistemic agents with finite memory resources via sampling
accessible data streams. We show that even under ideal conditions, epistemic
agents using slightly different sampling strategies might not achieve consensus
in their conclusions about which data stream is influencing which. As an
illustration, we examine a real world data stream where different sampling
strategies result in contradictory conclusions, explaining why some politically
charged topics might exist due to purely epistemic reasons irrespective of the
actual ontology of the world.",1
218,"We investigate the transport of a solute past isolated sinks in a bounded
domain when advection is dominant over diffusion, evaluating the effectiveness
of homogenization approximations when sinks are distributed uniformly randomly
in space. Corrections to such approximations can be non-local, non-smooth and
non-Gaussian, depending on the physical parameters (a P\'eclet number Pe,
assumed large, and a Damk\""ohler number Da) and the compactness of the sinks.
In one spatial dimension, solute distributions develop a staircase structure
for large Pe, with corrections being better described with credible intervals
than with traditional moments. In two and three dimensions, solute
distributions are near-singular at each sink (and regularized by sink size),
but their moments can be smooth as a result of ensemble averaging over variable
sink locations. We approximate corrections to a homogenization approximation
using a moment-expansion method, replacing the Green's function by its
free-space form, and test predictions against simulation. We show how, in two
or three dimensions, the leading-order impact of disorder can be captured in a
homogenization approximation for the ensemble mean concentration through a
modification to Da that grows with diminishing sink size.",1
219,"Understanding the underlying chemistry of a catalytic process is essential
for advancing of its medical and industrial applications.
  A well defined and compact representation of a catalytic process is becoming
exceedingly valuable in face of the growth in computer assisted development
strategies.
  A step-wise partition of a catalytic process is traditional in organic
chemistry.
  We argue, however, that such a description remains incomplete, especially in
face of automation and formal methods.
  In this contribution-based on enzymatic reaction mechanisms-we fully
formalize the step-wise notion of a catalytic process within the mathematical
framework of graph transformation, and propose a concise representation as an
overlay graph (OG).
  We formally define this concept in a dual form as an OG itself and an overlay
rule.
  The former representing a static summary of the process, and the latter being
an executable in the graph transformation formalism.
  We demonstrate that OGs readily expose the underlying chemistry and
facilitate the interpretation of catalytic processes.
  The emergence of electron flow patterns, for instance, provides an excellent
stage for classification efforts.
  We further show how the executable overlay rules can be employed to
automatically assign a fully specified mechanism to a reaction with only an
overall description, i.e. educt and product molecules.",1
220,"Self-supervised representation learning (SSL) on biomedical networks provides
new opportunities for drug discovery which is lack of available biological or
clinic phenotype. However, how to effectively combine multiple SSL models is
challenging and rarely explored. Therefore, we propose multi-task joint
strategies of self-supervised representation learning on biomedical networks
for drug discovery, named MSSL2drug. We design six basic SSL tasks that are
inspired by various modality features including structures, semantics, and
attributes in biomedical heterogeneous networks. In addition, fifteen
combinations of multiple tasks are evaluated by a graph attention-based
adversarial multi-task learning framework in two drug discovery scenarios. The
results suggest two important findings. (1) The combinations of multimodal
tasks achieve the best performance compared to other multi-task joint
strategies. (2) The joint training of local and global SSL tasks yields higher
performance than random task combinations. Therefore, we conjecture that the
multimodal and local-global combination strategies can be regarded as a
guideline for multi-task SSL to drug discovery.",1
221,"Biofilms are spatially organized microorganism colonies embedded in a
self-produced matrix, conferring to the microbial community resistance to
environmental stresses. Motile bacteria have been observed swimming in the
matrix of pathogenic exogeneous host biofilms. This observation opened new
promising routes for deleterious biofilms biocontrol: these bacterial swimmers
enhance biofilm vascularization for chemical treatment or could deliver
biocontrol agent by microbial hitchhiking or local synthesis.
%\cite{muok2021microbial,yu2020hitchhiking,samad2017swimming}. Hence,
characterizing swimmer trajectories in the biofilm matrix is of particular
interest to understand and optimize its biocontrol.In this study, a new
methodology is developed to analyze time-lapse confocal laser scanning images
to describe and compare the swimming trajectories of bacterial swimmers
populations and their adaptations to the biofilm structure. The method is based
on the inference of a kinetic model of swimmer population including mechanistic
interactions with the host biofilm. After validation on synthetic data, the
methodology is implemented on images of three different motile {Bacillus
species swimming in a Staphylococcus aureus biofilm. The fitted model allows to
stratify the swimmer populations by their swimming behavior and provides
insights into the mechanisms deployed by the micro-swimmers to adapt their
swimming traits to the biofilm matrix.",1
222,"The present study evaluated In vitro effect of gentamicin sulfate and
ceftiofur sodium on the viability of the Marek's disease virus. The titer of
cell associated turkey herpesvirus (HVT) vaccine was not appreciably reduced
when incubated with 50 mg/ml of gentamicin sulfate or ceftiofur sodium.
Statistic difference was not found between the number of plaqueforming units
(PFU) of reconstituted vaccine associated with both antibiotics 0, 15, 30 and
60 minutes after reconstitution of vaccine. The antibiotics did not
considerably alter the pH values. There was a significative decrease of the
titer of all vaccinal solutions when they were inoculated 30 and 60 minutes
after the reconstitution of the vaccine. Nevertheless, these titers are higher
than the required titers to protectect against the Marek disease.",1
223,"The objective of this study was to determine the effect of feeding an
antimicrobial association to males broiler chickens from 0 to 14 days of age on
the performance and clinical signs of bacterial processes. Two treatments were
tested: a control treatment without Respirend-M, and a treatment with the
antimicrobial (300 g/t Respirend-M, 0-14 d). 398 male Cobb 500 chicks were
randomly assigned to two treatments and four replications per treatment. Body
weight at 14 d, body weight gain, feed intake, feed conversion ratio, and
health status were evaluated. The results showed feeding 300 g/t Respirend-M
significantly (P<0.05) improved the final body weight, the body weight gain and
the feed intake. No statistically significant (P>0.05) effect was detected on
the feed conversion ratio.",1
224,"An experiment was conducted to evaluate the effect of feeding post-weaned
piglets different levels of a commercial amoxicillin plus norfloxacin formula
(Respirend) on the performance, clinical evidence of sickness, and the
cost:benefit ratio. 454 PIC x Camborough female and castrated male, weaned at
21 d of age, with an average initial weight of 6.19 kg, were used in this
experiment. Treatment 1 consisted in feeding a basal diet supplemented with
1000 g of a commercial tilmicosin product (Pulmotil) per metric tonne of feed
(control group, from 21 to 35 d of age) and Treatments 2), 3) and 4) consisted
in feeding the basal diet supplement with 500, 300 y 100 g of Respirend per
metric tonne of feed, respectively, from 21 to 42 d of age. The results did not
show significant differences (P<0.05) in mortality nor in parenteral medicine
administration cost in none of the periods. Nevertheless, 500 g of Respirend
per metric tonne of feed improved significantly (P<0.05) the general health
status in the period between 21 and 35 d of age. For the whole evaluation
period (21 to 42 d of age), this level improved significantly (P<0.05) the
final body weight, the average daily weight gain, the average daily feed
intake, the feed:gain ratio and optimized the cost:benefit ratio.",1
225,"Ligand-based virtual screening aims to reduce the cost and duration of drug
discovery campaigns. Shape similarity can be used to screen large databases,
with the goal of predicting potential new hits by comparing to molecules with
known favourable properties. This paper presents the theory underpinning
RGMolSA, a new alignment-free and mesh-free surface-based molecular shape
descriptor derived from the mathematical theory of Riemannian geometry. The
treatment of a molecule as a series of intersecting spheres allows the
description of its surface geometry using the Riemannian metric, obtained by
considering the spectrum of the Laplacian. This gives a simple vector
descriptor constructed of the weighted surface area and eight non-zero
eigenvalues, which capture the surface shape. We demonstrate the potential of
our method by considering a series of PDE5 inhibitors that are known to have
similar shape as an initial test case. RGMolSA displays promise when compared
to existing shape descriptors and in its capability to handle different
molecular conformers. The code and data used to produce the results are
available via GitHub: https://github.com/RPirie96/RGMolSA.",1
226,"Recent theory shows that extortioners taking advantage of the
zero-determinant (ZD) strategy can unilaterally claim an unfair share of the
payoffs in the Iterated Prisoner's Dilemma. It is thus suggested that against a
fixed extortioner, any adapting co-player should be subdued with full
cooperation as their best response. In contrast, recent experiments demonstrate
that human players often choose not to accede to extortion out of concern for
fairness, actually causing extortioners to suffer more loss than themselves. In
light of this, here we reveal fair-minded strategies that are unbending to
extortion such that any payoff-maximizing extortioner ultimately will concede
in their own interest by offering a fair split in head-to-head matches. We find
and characterize multiple general classes of such unbending strategies,
including generous zero-determinant strategies and Win-Stay, Lose-Shift as
particular examples. When against fixed unbending players, extortioners are
forced with consequentially increasing losses whenever intending to demand more
unfair share. Our analysis also pivots to the importance of payoff structure in
determining the superiority of zero-determinant strategies and in particular
their extortion ability. We show that an extortionate ZD player can be even
outperformed by, for example, Win-Stay Lose-Shift, if the total payoff of
unilateral cooperation is smaller than that of mutual defection. Unbending
strategies can be used to outlearn evolutionary extortioners and catalyze the
evolution of Tit-for-Tat-like strategies out of ZD players. Our work has
implications for promoting fairness and resisting extortion so as to uphold a
just and cooperative society.",1
227,"Model analogies and exchange of ideas between physics or chemistry with
biology or epidemiology have often involved inter-sectoral mapping of
techniques. Material mechanics has benefitted hugely from such interpolations
from mathematical physics where dislocation patterning of platstically deformed
metals [1,2,3] and mass transport in nanocomposite materials with high
diffusivity paths such as dislocation and grain boundaries, have been
traditionally analyzed using the paradigmatic Walgraef-Aifantis (W-A)
double-diffusivity (D-D) model [4,5,6,7,8,9]. A long standing challenge in
these studies has been the inherent nonlinear correlation between the
diffusivity paths, making it extremely difficult to analyze their
interdependence. Here, we present a novel method of approximating a closed form
solution of the ensemble averaged density profiles and correlation statistics
of coupled dynamical systems, drawing from a technique used in mathematical
biology to calculate a quantity called the {\it basic reproduction number}
$R_0$, which is the average number of secondary infections generated from every
infected. We show that the $R_0$ formulation can be used to calculate the
correlation between diffusivity paths, agreeing closely with the exact
numerical solution of the D-D model. The method can be generically implemented
to analyze other reaction-diffusion models.",1
228,"Baker's yeast (Saccharomyces cerevisiae) is a model organism for studying the
morphology that emerges at the scale of multi-cell colonies. To look at how
morphology develops, we collect a dataset of time-lapse photographs of the
growth of different strains of S. cerevisiae. We discuss the general
statistical challenges that arise when using time-lapse photographs to extract
time-dependent features. In particular, we show how texture-based feature
engineering and representative clustering can be successfully applied to
categorize the development of yeast colony morphology using our dataset. The
local binary pattern (LBP) from image processing is used to score the surface
texture of colonies. This texture score develops along a smooth trajectory
during growth. The path taken depends on how the morphology emerges. A
hierarchical clustering of the colonies is performed according to their texture
development trajectories. The clustering method is designed for practical
interpretability; it obtains the best representative colony image for any
hierarchical sub-cluster.",1
229,"Range expansion is a universal process in biological systems, and therefore
plays a part in biological evolution. Using a quantitative individual-based
method based on the stochastic process, we identify that enhancing the inherent
self-proliferation advantage of cooperators relative to defectors is a more
effective channel to promote the evolution of cooperation in range expansion
than weakening the benefit acquisition of defectors from cooperators. With this
self-proliferation advantage, cooperators can rapidly colonize virgin space and
establish spatial segregation more readily, which acts like a protective shield
to further promote the evolution of cooperation in return. We also show that
lower cell density and migration rate have a positive effect on the competition
of cooperators with defectors. Biological evolution is based on competition
between individuals and should therefore favor selfish behaviors. However, we
observe a counterintuitive phenomenon that the evolution of a population is
impeded by the fitness-enhancing chemotactic movement of individuals. This
highlights a conflict between the interests of the individual and the
population. The short-sighted selfish behavior of individuals may not be that
favored in the competition between populations. Such information provides
important implications for the handling of cooperation.",1
230,"During recent years, the study of long transients has been expanded in
ecological theory to account for shifts in long-term behavior of ecological
systems. These long transients may lead to regime shifts between alternative
states that resemble the dynamics of alternative stable states for a prolonged
period of time. One dynamic that potentially leads to long transients is the
group defense of a resource in a consumer-resource interaction. Furthermore,
time lags in the population caused by discrete reproductive pulses have the
potential to produce long transients, either independently or in conjunction to
the transients caused by the group defense. In this work, we analyze the
potential for long transients in a model for a consumer-resource system in
which the resource exhibits group defense and reproduces in discrete
reproductive pulses. We develop this discrete-time model by discretizing a
pulse differential equation. This system exhibits crawl-by transients near the
extinction and carrying capacity states of resource. In addition, we identify a
transcritical bifurcation in our system, under which a ghost limit cycle
appears. These transients resemble stable states for a prolonged transient time
period. We estimate the transient time of our system from these transients
using perturbation theory. This work advances an understanding of how systems
shift between alternate states and their duration of staying in a given regime
and what ecological dynamics may lead to long transients.",1
231,"Current scientific consensus holds that sound is transmitted, solely
mechanically, from the tympanum to the cochlea via ossicles. However this
theory does not explain the hearing extreme quality regarding high frequencies
in mammals. So, we propose a bioelectronic pathway (the covert path) that is
complementary to the overt path. We demonstrate experimentally that the
tympanum produces piezoelectric potentials isochronous to acoustic vibrations
thanks to its collagen fibers and that their amplitude increases along with the
frequency and level of the vibrations. This finding supports the existence of
an electrical pathway, specialized in transmitting high-frequency sounds that
works in unison with the mechanical pathway. A bio-organic triode, similar to a
field effect transistor, is the key mechanism of our hypothesized pathway. We
present evidence that any deficiency along this pathway produces hearing
impairment. By augmenting the classical theory of sound transmission, our
discovery offers new perspectives for research into both normal and
pathological audition and may contribute to an understanding of genetic and
physiological problems of hearing.",1
232,"This paper provides a concise description of the free energy principle,
starting from a formulation of random dynamical systems in terms of a Langevin
equation and ending with a Bayesian mechanics that can be read as a physics of
sentience. It rehearses the key steps using standard results from statistical
physics. These steps entail (i) establishing a particular partition of states
based upon conditional independencies that inherit from sparsely coupled
dynamics, (ii) unpacking the implications of this partition in terms of
Bayesian inference and (iii) describing the paths of particular states with a
variational principle of least action. Teleologically, the free energy
principle offers a normative account of self-organisation in terms of optimal
Bayesian design and decision-making, in the sense of maximising marginal
likelihood or Bayesian model evidence. In summary, starting from a description
of the world in terms of random dynamical systems, we end up with a description
of self-organisation as sentient behaviour that can be interpreted as
self-evidencing; namely, self-assembly, autopoiesis or active inference.",1
233,"We assess costs and efficiency of state-of-the-art high performance cloud
computing compared to a traditional on-premises compute cluster. Our use case
are atomistic simulations carried out with the GROMACS molecular dynamics (MD)
toolkit with a focus on alchemical protein-ligand binding free energy
calculations.
  We set up a compute cluster in the Amazon Web Services (AWS) cloud that
incorporates various different instances with Intel, AMD, and ARM CPUs, some
with GPU acceleration. Using representative biomolecular simulation systems we
benchmark how GROMACS performs on individual instances and across multiple
instances. Thereby we assess which instances deliver the highest performance
and which are the most cost-efficient ones for our use case.
  We find that, in terms of total costs including hardware, personnel, room,
energy and cooling, producing MD trajectories in the cloud can be as
cost-efficient as an on-premises cluster given that optimal cloud instances are
chosen. Further, we find that high-throughput ligand-screening for
protein-ligand binding affinity estimation can be accelerated dramatically by
using global cloud resources. For a ligand screening study consisting of 19,872
independent simulations, we used all hardware that was available in the cloud
at the time of the study. The computations scaled-up to reach peak performances
using more than 10,000 instances, 140,000 cores, and 3,000 GPUs simultaneously
around the globe. Our simulation ensemble finished in about two days in the
cloud, while weeks would be required to complete the task on a typical
on-premises cluster consisting of several hundred nodes. We demonstrate that
the costs of such and similar studies can be drastically reduced with a
checkpoint-restart protocol that allows to use cheap Spot pricing and by using
instance types with optimal cost-efficiency.",1
234,"Since December 2020, variants of COVID-19 (especially Delta and Omicron)
appeared with different characteristics that influenced death and
transmissibility emerged around the world. To address the novel dynamics of the
disease, we propose a dynamical model of two strains, namely native and mutant,
transmission dynamics with mutation and imperfect vaccination. It is also
assumed that the recuperated individuals from the native strain can be infected
with mutant strain through the direct contact with individual or contaminated
surfaces or aerosols. We compute the basic reproduction number for each strain
independently and take the maximum for $R_0$. We prove the nonexistence of
backward bifurcation using the center manifold theory, and global stability of
disease-free equilibrium when the basic reproduction number $R_0<1. An
intermediate mutation rate $\nu_1$ leads to oscillations. When $\nu_1$
increases over a threshold, the system regains its stability and exhibits an
interesting dynamics called endemic bubble. An analytical expression for
vaccine-induced herd immunity is derived. The model is parameterized using the
Indian data of the cumulative number of confirmed cases and deaths of COVID-19
from March 1 to September 27 in 2021, using MCMC method. The cumulative cases
and deaths can be reduced by increasing the vaccine efficacies to both native
and mutant strains. We observe that by considering the vaccine efficacy to
native strain as 90\%, the cumulative cases and deaths would be reduced by
3.27\% and 5.2\%, respectively; and by considering the vaccine efficacy to
mutant strain as 90\%, the cumulative cases and deaths would be reduced by
0.9\% and 2.5\%, respectively. Our study demonstrates that the COVID-19
pandemic may be worse due to the occurrence of oscillations for certain
mutation rates but better due to stability at a lower infection level with a
larger mutation rate.",1
235,"A genome read data set can be quickly and efficiently remapped from one
reference to another similar reference (e.g., between two reference versions or
two similar species) using a variety of tools, e.g., the commonly-used CrossMap
tool. With the explosion of available genomic data sets and references,
high-performance remapping tools will be even more important for keeping up
with the computational demands of genome assembly and analysis.
  We provide FastRemap, a fast and efficient tool for remapping reads between
genome assemblies. FastRemap provides up to a 7.19$\times$ speedup
(5.97$\times$, on average) and uses as low as 61.7% (80.7%, on average) of the
peak memory consumption compared to the state-of-the-art remapping tool,
CrossMap.
  FastRemap is written in C++. The source code and user manual are freely
available at: github.com/CMU-SAFARI/FastRemap",1
236,"Accurate information about protein content in the organism is instrumental
for a better understanding of human biology and disease mechanisms. While the
presence of certain types of proteins can be life-threatening, the abundance of
others is an essential condition for an individual's overall well-being.
Protein microarray is a technology that enables the quantification of thousands
of proteins in hundreds of human samples in a parallel manner. In a series of
studies involving protein microarrays, we have explored and implemented various
data science methods for all-around analysing of these data. This analysis has
enabled the identification and characterisation of proteins targeted by the
autoimmune reaction in patients with the APS1 condition. We have also assessed
the utility of applying machine learning methods alongside statistical tests in
a study based on protein expression data to evaluate potential biomarkers for
endometriosis. The keystone of this work is a web-tool PAWER. PAWER implements
relevant computational methods, and provides a semi-automatic way to run the
analysis of protein microarray data online in a drag-and-drop and
click-and-play style. The source code of the tool is publicly available. The
work that laid the foundation of this thesis has been instrumental for a number
of subsequent studies of human disease and also inspired a contribution to
refining standards for validation of machine learning methods in biology.",1
237,"Understanding the joint impact of vaccination and non-pharmaceutical
interventions on COVID-19 development is important for making public health
decisions that control the pandemic. Recently, we created a method in
forecasting the daily number of confirmed cases of infectious diseases by
combining a mechanistic ordinary differential equation (ODE) model for
infectious classes and a generalized boosting machine learning model (GBM) for
predicting how public health policies and mobility data affect the transmission
rate in the ODE model [WWR+]. In this paper, we extend the method to the
post-vaccination period, accordingly obtain a retrospective forecast of
COVID-19 daily confirmed cases in the US, and identify the relative influence
of the policies used as the predictor variables. In particular, our ODE model
contains both partially and fully vaccinated compartments and accounts for the
breakthrough cases, that is, vaccinated individuals can still get infected. Our
results indicate that the inclusion of data on non-pharmaceutical interventions
can significantly improve the accuracy of the predictions. With the use of
policy data, the model predicts the number of daily infected cases up to 35
days in the future, with an average mean absolute percentage error of 34%,
which is further improved to 21% if combined with human mobility data.
Moreover, similar to the pre-vaccination study, the most influential predictor
variable remains the policy of restrictions on gatherings. The modeling
approach used in this work can help policymakers design control measures as
variant strains threaten public health in the future.",1
238,"We propose a new deterministic methodology to predict RNA sequence and
protein folding. Is stem enough for structure prediction? The main idea is to
consider all possible stem formation in the given sequence. With the stem loop
energy and the strength of stem, we explore how to deterministically utilize
stem information for RNA sequence and protein folding structure prediction. We
use graph notation, where all possible stems are represented as vertices, and
co-existence as edges. This full Stem-graph presents all possible folding
structure, and we pick sub-graph(s) which give the best matching energy for
folding structure prediction. We introduce a Stem-Loop score to add structure
information and to speed up the computation. The proposed method can handle
secondary structure prediction as well as protein folding with pseudo knots.
Numerical experiments are done using a laptop and results take only a few
minutes or seconds. One of the strengths of this approach is in the simplicity
and flexibility of the algorithm, and it gives deterministic answer. We explore
protein sequences from Protein Data Bank, rRNA 5S sequences, and tRNA sequences
from the Gutell Lab. Various experiments and comparisons are included to
validate the propose method.",1
239,"Diffusion tensor imaging (DTI) is the most widely used tool for studying
brain white matter development and degeneration. However, standard DTI
estimation methods depend on a large number of high-quality measurements. This
would require long scan times and can be particularly difficult to achieve with
certain patient populations such as neonates. Here, we propose a method that
can accurately estimate the diffusion tensor from only six diffusion-weighted
measurements. Our method achieves this by learning to exploit the relationships
between the diffusion signals and tensors in neighboring voxels. Our model is
based on transformer networks, which represent the state of the art in modeling
the relationship between signals in a sequence. In particular, our model
consists of two such networks. The first network estimates the diffusion tensor
based on the diffusion signals in a neighborhood of voxels. The second network
provides more accurate tensor estimations by learning the relationships between
the diffusion signals as well as the tensors estimated by the first network in
neighboring voxels. Our experiments with three datasets show that our proposed
method achieves highly accurate estimations of the diffusion tensor and is
significantly superior to three competing methods. Estimations produced by our
method with six measurements are comparable with those of standard estimation
methods with 30-88 measurements. Hence, our method promises shorter scan times
and more reliable assessment of brain white matter, particularly in
non-cooperative patients such as neonates and infants.",1
240,"The evaluation of drug-induced Torsades de pointes (TdP) risks is crucial in
drug safety assessment. In this study, we discuss machine learning approaches
in the prediction of drug-induced TdP risks using preclinical data.
Specifically, the random forest model was trained on the dataset generated by
the rabbit ventricular wedge assay. The model prediction performance was
measured on 28 drugs from the Comprehensive In Vitro Proarrhythmia Assay
initiative. Leave-one-drug-out cross-validation provided an unbiased estimation
of model performance. Stratified bootstrap revealed the uncertainty in the
asymptotic model prediction. Our study validated the utility of machine
learning approaches in predicting drug-induced TdP risks from preclinical data.
Our methods can be extended to other preclinical protocols and serve as a
supplementary evaluation in drug safety assessment.",1
241,"Phenomenological and deterministic models are often used for the estimation
of transmission parameters in an epidemic and for the prediction of its growth
trajectory. Such analyses are usually based on single peak outbreak dynamics.
In light of the present COVID-19 pandemic, there is a pressing need to better
understand observed epidemic growth with multiple peak structures, preferably
using first-principles methods. Along the lines of our previous work [Physica A
574, 126014 (2021)], here we apply 2D random-walk Monte Carlo calculations to
better understand COVID-19 spread through contact interactions. Lockdown
scenarios and all other control interventions are imposed through mobility
restrictions and a regulation of the infection rate within the stochastically
interacting population. The susceptible, infected and recovered populations are
tracked over time, with daily infection rates obtained without recourse to the
solution of differential equations.
  The simulations were carried out for population densities corresponding to
four countries, India, Serbia, South Africa and USA. In all cases our results
capture the observed infection growth rates. More importantly, the simulation
model is shown to predict secondary and tertiary waves of infections with
reasonable accuracy. This predictive nature of multiple wave structures
provides a simple and effective tool that may be useful in planning mitigation
strategies during the present pandemic.",1
242,"Increasing interest in the acquisition of biotic and abiotic resources from
within the deep sea (e.g. fisheries, oil-gas extraction, and mining) urgently
imposes the development of novel monitoring technologies, beyond the
traditional vessel-assisted, time-consuming, high-cost sampling surveys. The
implementation of permanent networks of seabed and water-column cabled (fixed)
and docked mobile platforms is presently enforced, to cooperatively measure
biological features and environmental (physico-chemical) parameters. Video and
acoustic (i.e. optoacoustic) imaging are becoming central approaches for
studying benthic fauna (e.g. quantifying species presence, behaviour, and
trophic interactions) in a remote, continuous, and prolonged fashion. Imaging
is also being complemented by in situ environmental-DNA sequencing
technologies, allowing the traceability of a wide range of organisms (including
prokaryotes) beyond the reach of optoacoustic tools. Here, we describe the
different fixed and mobile platforms of those benthic and pelagic monitoring
networks, proposing at the same time an innovative roadmap for the automated
computing of hierarchical ecological information of deep-sea ecosystems (i.e.
from single species abundance and life traits, to community composition, and
overall biodiversity)",1
243,"We present a simple yet novel time series imputation technique with the goal
of constructing an irregular time series that is uniform across every sample in
a data set. Specifically, we fix a grid defined by the midpoints of
non-overlapping bins (dubbed ""slices"") of observation times and ensure that
each sample has values for all of the features at that given time. This allows
one to both impute fully missing observations to allow uniform time series
classification across the entire data and, in special cases, to impute
individually missing features. To do so, we slightly generalize the well-known
class imbalance algorithm SMOTE \cite{smote} to allow component wise nearest
neighbor interpolation that preserves correlations when there are no missing
features. We visualize the method in the simplified setting of 2-dimensional
uncoupled harmonic oscillators. Next, we use tSMOTE to train an Encoder/Decoder
long-short term memory (LSTM) model with Logistic Regression for predicting and
classifying distinct trajectories of different 2D oscillators. After
illustrating the the utility of tSMOTE in this context, we use the same
architecture to train a clinical model for COVID-19 disease severity on an
imputed data set. Our experiments show an improvement over standard mean and
median imputation techniques by allowing a wider class of patient trajectories
to be recognized by the model, as well as improvement over aggregated
classification models.",1
244,"DNA is strong polyelectrolyte macromolecule making metal ions (counterions)
condense to a cloud around the double helix. The counterions may be localized
outside the macromolecule and inside the minor and major grooves of the double
helix. In the present work, the distribution of condensed counterions between
inner and outer regions of DNA has been studied using the approaches of
counterion condensation theory. The results have shown that the number of
counterions trapped inside the macromolecule should be greater than 0.16 per
one phosphate group. The maximal number of counterions that may be localized
inside the DNA double helix is limited to about 0.4 per one phosphate group and
it is much lower than the total number of condensed counterions. To analyze the
structure of counterion cloud the molecular dynamics simulations of
\emph{B}-DNA with K$^{+}$ counterions have been performed. The obtained number
of the counterions trapped inside the grooves of the double helix is about
0.22$\pm$0.06 per one phosphate group that agree with the model estimations.
The developed model describes general features of the structure of counterion
cloud around DNA and is able to predict the number of counterions inside the
grooves of the double helix.",1
245,"Serological tests are important for understanding the physiopathology and
following the evolution of the Covid-19 pandemic. Assays based on flow
cytometry (FACS) of tissue culture cells expressing the spike (S) protein of
SARS-CoV-2 have repeatedly proven to perform slightly better than the
plate-based assays ELISA and CLIA (chemiluminescent immuno-assay), and markedly
better than lateral flow immuno-assays (LFIA). Here, we describe an optimized
and very simple FACS assay based on staining a mix of two Jurkat cell lines,
expressing either high levels of the S protein (Jurkat-S) or a fluorescent
protein (Jurkat-R expressing m-Cherry, or Jurkat-G, expressing GFP, which serve
as an internal negative control). We show that the Jurkat-S\&R-flow test has a
much broader dynamic range than a commercial ELISA test and performs at least
as well in terms of sensitivity and specificity. Also, it is more sensitive and
quantitative than the hemagglutination-based test HAT, which we described
recently. The Jurkat-flow test requires only a few microliters of blood; thus,
it can be used to quantify various Ig isotypes in capillary blood collected
from a finger prick. It can be used also to evaluate serological responses in
mice, hamsters, cats and dogs. FACS tests offer a very attractive solution for
laboratories with access to tissue culture and flow cytometry who want to
monitor serological responses in humans or in animals, and how these relate to
susceptibility to infection, or re-infection, by the virus, and to protection
against Covid-19.",1
246,"Pathway enrichment analysis has become a widely used knowledge-based approach
for the interpretation of biomedical data. Its popularity has led to an
explosion of both enrichment methods and pathway databases. While the elegance
of pathway enrichment lies in its simplicity, multiple factors can impact the
results of such an analysis which may not be accounted for. Researchers may
fail to give influential aspects their due, resorting instead to popular
methods and gene set collections, or default settings. Despite ongoing efforts
to establish set guidelines, meaningful results are still hampered by a lack of
consensus or gold standards around how enrichment analysis should be conducted.
Nonetheless, such concerns have prompted a series of benchmark studies
specifically focused on evaluating the influence of various factors on pathway
enrichment results. In this review, we organize and summarize the findings of
these benchmarks to provide a comprehensive overview on the influence of these
factors. Our work covers a broad spectrum of factors, spanning from
methodological assumptions to those related to prior biological knowledge, such
as pathway definitions and database choice. In doing so, we aim to shed light
on how these aspects can lead to insignificant, uninteresting, or even
contradictory results. Finally, we conclude the review by proposing future
benchmarks as well as solutions to overcome some of the challenges which
originate from the outlined factors.",1
247,"The isocitrate dehydrogenase (IDH) gene mutation status is an important
biomarker for glioma patients. The gold standard of IDH mutation detection
requires tumour tissue obtained via invasive approaches and is usually
expensive. Recent advancement in radiogenomics provides a non-invasive approach
for predicting IDH mutation based on MRI. Meanwhile, tumor geometrics encompass
crucial information for tumour phenotyping. Here we propose a collaborative
learning framework that learns both tumor images and tumor geometrics using
convolutional neural networks (CNN) and graph neural networks (GNN),
respectively. Our results show that the proposed model outperforms the baseline
model of 3D-DenseNet121. Further, the collaborative learning model achieves
better performance than either the CNN or the GNN alone. The model
interpretation shows that the CNN and GNN could identify common and unique
regions of interest for IDH mutation prediction. In conclusion, collaborating
image and geometric learners provides a novel approach for predicting genotype
and characterising glioma.",1
248,"Although we can measure muscle activity and analyze their activation
patterns, we understand little about how individual muscles affect the joint
torque generated. It is known that they are controlled by circuits in the
spinal cord, a system much less well understood than the cortex. Knowing the
contribution of the muscles towards a joint torque would improve our
understanding of human limb control. We present a novel framework to examine
the control of biomechanics using physics simulations informed by
electromyography (EMG) data. These signals drive a virtual musculoskeletal
model in the Neurorobotics Platform (NRP), which we then use to evaluate
resulting joint torques. We use our framework to analyze raw EMG data collected
during an isometric knee extension study to identify synergies that drive a
musculoskeletal lower limb model. The resulting knee torques are used as a
reference for genetic algorithms (GA) to generate new simulated activation
patterns. On the platform the GA finds solutions that generate torques matching
those observed. Possible solutions include synergies that are similar to those
extracted from the human study. In addition, the GA finds activation patterns
that are different from the the biological ones while still producing the same
knee torque. The NRP forms a highly modular integrated simulation platform
allowing these in silico experiments. We argue that our framework allows for
research of the neurobiomechanical control of muscles during tasks, which would
otherwise not be possible.",1
249,"The widespread and in many countries unprecedented use of non-pharmaceutical
interventions (NPIs) during the COVID-19 pandemic has highlighted the need for
mathematical models which can estimate the impact of these measures while
accounting for the highly heterogeneous risk profile of COVID-19. Models
accounting either for age/risk structure or the household structure necessary
to explicitly model many NPIs are commonly used in infectious disease
modelling, but models incorporating both levels of structure present
substantial computational and mathematical challenges due to their high
dimensionality. Here we present a modelling framework for the spread of an
epidemic including explicit representation of age/risk structure and household
structure. Our model is formulated in terms of tractable systems of ordinary
differential equations for which we provide an open-source Python
implementation. Such tractability leads to significant benefits for model
calibration, exhaustive evaluation of possible parameter values, and
interpretability of results. We demonstrate the flexibility of our model
through four policy case studies, where we quantify the likely benefits of the
following measures which were either considered or implemented in the UK during
the current COVID-19 pandemic: control of within- and between-household mixing
through NPIs; formation of support bubbles during lockdown periods;
out-of-household isolation (OOHI); and temporary relaxation of NPIs during
holiday periods. Our ordinary differential equation formulation and associated
analysis demonstrate that multiple dimensions of risk stratification and social
structure can be incorporated into infectious disease models without
sacrificing mathematical tractability. This model and its software
implementation expand the range of tools available to infectious disease policy
analysts.",1
250,"Enquiries concerning the underlying mechanisms and the emergent properties of
a biological brain have a long history of theoretical postulates and
experimental findings. Today, the scientific community tends to converge to a
single interpretation of the brain's cognitive underpinnings -- that it is a
Bayesian inference machine. This contemporary view has naturally been a strong
driving force in recent developments around computational and cognitive
neurosciences. Of particular interest is the brain's ability to process the
passage of time -- one of the fundamental dimensions of our experience. How can
we explain empirical data on human time perception using the Bayesian brain
hypothesis? Can we replicate human estimation biases using Bayesian models?
What insights can the agent-based machine learning models provide for the study
of this subject? In this chapter, we review some of the recent advancements in
the field of time perception and discuss the role of Bayesian processing in the
construction of temporal models.",1
251,"Polymerase chain reaction (PCR) testing is the gold standard for diagnosing
COVID-19. PCR amplifies the virus DNA 40 times to produce measurements of viral
loads that span seven orders of magnitude. Unfortunately, the outputs of these
tests are imprecise and therefore quantitative group testing methods, which
rely on precise measurements, are not applicable. Motivated by the
ever-increasing demand to identify individuals infected with SARS-CoV-19, we
propose a new model that leverages tropical arithmetic to characterize the PCR
testing process. Our proposed framework, termed tropical group testing,
overcomes existing limitations of quantitative group testing by allowing for
imprecise test measurements. In many cases, some of which are highlighted in
this work, tropical group testing is provably more powerful than traditional
binary group testing in that it require fewer tests than classical approaches,
while additionally providing a mechanism to identify the viral load of each
infected individual. It is also empirically stronger than related works that
have attempted to combine PCR, quantitative group testing, and compressed
sensing.",1
252,"Coherence resonance (CR), stochastic synchronization (SS), and
spike-timing-dependent plasticity (STDP) are ubiquitous dynamical processes in
biological neural networks. Whether enhancing CR can be associated with
improving SS and vice versa is a fundamental question of interest. The effects
of STDP and different network connectivity on this enhancement interplay are
still elusive. In this paper, we consider a small-world network of excitable
Hodgkin-Huxley neurons driven by channel noise and excitatory STDP with a
Hebbian time window. Numerical simulations indicate that there exist intervals
of parameter values of the network topology and the STDP learning rule in which
an enhanced SS (CR) would improve CR (SS). In particular, it is found that at
certain intermediate values of the average degree of the network, higher values
of the potentiation adjusting rate, and lower values of the depression temporal
window, an enhanced SS (CR) would improve CR (SS). Our results could shed some
light on the efficient coding mechanisms based on the spatiotemporal coherence
of the spiking activity in neural networks.",1
253,"Protein Contact Network (PCN) is a powerful tool for analysing the structure
and function of proteins. In particular, PCN has been used for disclosing the
molecular features of allosteric regulation through PCN clustering. Such
analysis is relevant in many applications, such as the recent study of
SARS-CoV-2 Spike Protein. Despite its relevance, methods for the analysis of
PCN are spread into a set of different libraries and tools. Therefore, the
introduction of a tool that incorporates all the function may help researchers.
We present PCN-Miner a software tool implemented in the Python programming
language able to import protein in the Protein Data Bank format and generate
the corresponding protein contact network. Then it offers a set of algorithms
for the analysis of PCS that cover a large set of applications: from clustering
to embedding and subsequent analysis. Software is available at
\url{https://github.com/hguzzi/ProteinContactNetworks}",1
254,"In synaptic molecular communication (MC), the activation of postsynaptic
receptors by neurotransmitters (NTs) is governed by a stochastic
reaction-diffusion process. This randomness of synaptic MC contributes to the
randomness of the electrochemical downstream signal in the postsynaptic cell,
called postsynaptic membrane potential (PSP). Since the randomness of the PSP
is relevant for neural computation and learning, characterizing the statistics
of the PSP is critical. However, the statistical characterization of the
synaptic reaction-diffusion process is difficult because the reversible
bi-molecular reaction of NTs with receptors renders the system nonlinear.
Consequently, there is currently no model available which characterizes the
impact of the statistics of postsynaptic receptor activation on the PSP. In
this work, we propose a novel statistical model for the synaptic
reaction-diffusion process in terms of the chemical master equation (CME). We
further propose a novel numerical method which allows to compute the CME
efficiently and we use this method to characterize the statistics of the PSP.
Finally, we present results from stochastic particle-based computer simulations
which validate the proposed models. We show that the biophysical parameters
governing synaptic transmission shape the autocovariance of the receptor
activation and, ultimately, the statistics of the PSP. Our results suggest that
the processing of the synaptic signal by the postsynaptic cell effectively
mitigates synaptic noise while the statistical characteristics of the synaptic
signal are preserved. The results presented in this paper contribute to a
better understanding of the impact of the randomness of synaptic signal
transmission on neuronal information processing.",1
255,"MicroRNAs play an indispensable role in numerous biological processes ranging
from organismic development to tumor progression.In oncology,these microRNAs
constitute a fundamental regulation role in the pathology of cancer that
provides the basis for probing into the influences on clinical features through
transcriptome data. Previous work focused on machine learning (ML) for
searching biomarkers in different cancer databases, but the functions of these
biomarkers are fully not clear. Taking lung cancer as a prototype case of
study. Through integrating clinical information into the transcripts expression
data, we systematically analyzed the effect of microRNA on diagnostic and
prognostic factors at deteriorative lung adenocarcinoma (LUAD). After dimension
reduction, unsupervised hierarchical clustering was used to find the diagnostic
factors which represent the unique expression patterns of microRNA at various
patient's stages. In addition, we developed a classification framework, Light
Gradient Boosting Machine (LightGBM) and SHAPley Additive explanation (SHAP)
algorithm, to screen out the prognostic factors. Enrichment analyses show that
the diagnostic and prognostic factors are not only enriched in cancer-related
athways, but also involved in many vital cellular signaling transduction and
immune responses. These key microRNAs also impact the survival risk of LUAD
patients at all (or a specific) stage(s) and some of them target some important
Transcription Factors (TF).The key finding is that five microRNAs
(hsa-mir-196b, hsa-mir-31, hsa-mir-891a, hsa-mir-34c, and hsa-mir-653) can then
serve as not only potential diagnostic factors but also prognostic tools in the
monitoring of lung cancer.",1
256,"In the field theories in physics, any particular region of the presumed
space-time continuum and all interactions between elementary objects therein
can be objectively measured and/or accounted for mathematically. Since this
does not apply to any of the field theories, or any other neural theory, of
consciousness, their explanatory power is limited. As discussed in detail
herein, the matter is complicated further by the facts than any scientifically
operational definition of consciousness is inevitably partial, and that the
phenomenon has no spatial dimensionality. Under the light of insights from
research on meditation and expanded consciousness, chronic pain syndrome,
healthy ageing, and eudaimonic well-being, we may conceive consciousness as a
source of potential energy that has no clearly defined spatial dimensionality,
but can produce significant changes in others and in the world, observable in
terms of changes in time. It is argued that consciousness may have evolved to
enable the human species to generate such changes in order to cope with
unprecedented and/or unpredictable adversity. Such coping could, ultimately,
include the conscious planning of our own extinction when survival on the
planet is no longer an acceptable option.",1
257,"In this work, we present mixed dimensional models for simulating blood flow
and transport processes in breast tissue and the vascular tree supplying it.
These processes are considered, to start from the aortic inlet to the
capillaries and tissue of the breast. Large variations in biophysical
properties and flow conditions exist in this system necessitating the use of
different flow models for different geometries and flow regimes. Large
variations in biophysical properties and flow conditions exist in this system
necessitating the use of different flow models for different geometries and
flow regimes. In total, we consider four different model types. First, a system
of 1D nonlinear hyperbolic PDEs is considered to simulate blood flow in larger
arteries with highly elastic vessel walls. Second, we assign 1D linearized
hyperbolic PDEs to model the smaller arteries with stiffer vessel walls. The
third model type consists of ODE systems (0D models). It is used to model the
arterioles and peripheral circulation. Finally, homogenized 3D porous media
models are considered to simulate flow and transport in capillaries and tissue
within the breast volume. Sink terms are used to account for the influence of
the venous and lymphatic systems. Combining the four model types, we obtain two
different 1D-0D-3D coupled models for simulating blood flow and transport
processes: The first 1D-0D-3D model covers the whole path from the aorta to the
breast, while the second model is a sub-model obtained by restriction to breast
vasculature and tissue making possible a significant reduction in computational
cost. Several numerical experiments are conducted that demonstrate realistic
flow simulations compared to existing data on blood flow in human breast and
vascular system.",1
258,"Repeated epileptic seizures impair around 65 million people worldwide and a
successful prediction of seizures could significantly help patients suffering
from refractory epilepsy. For two dogs with yearlong intracranial
electroencephalography (iEEG) recordings, we studied the influence of time
series nonstationarity on the performance of seizure prediction using in-house
developed machine learning algorithms. We observed a long-term evolution on the
scale of weeks or months in iEEG time series that may be represented as
switching between certain meta-states. To better predict impending seizures,
retraining of prediction algorithms is therefore necessary and the retraining
schedule should be adjusted to the change in meta-states. There is evidence
that the nature of seizure-free interictal clips also changes with the
transition between meta-states, accwhich has been shown relevant for seizure
prediction.",1
259,"We have ordered the entire set of X-ray crystallographic structures of the
Protein Data Bank in hierarchies of progressive interactions involving the same
or very similar protein chains, obtaining 40 205 hierarchies of protein
complexes with increasing number of partners. We have then studied these
hierarchies as a proxy of the pathways of assembly in protein complex
formation. The database was used to show statistically that new interfaces,
upon protein complex oligomerization, tend to be observed on residues that were
characterized as soft disordered (that is, flexible, amorphous or intrinsically
disordered) in simpler complexes, that is, structures with a smaller number of
partners and preceding the newly oligomerized complex in the hierarchy.
Furthermore, we observe that the location of soft disorder changes place on a
protein as its partners increase during complex formation. Our results unveil
the existence of a general protein assembly mechanism involving soft disorder
that modulates and orders the way protein complexes are assembled. Being soft
disorder regions highly correlated with interaction interfaces in protein
complexes, this work highlights the difficulty of structure prediction of large
protein compounds from sequence, and highlights the importance of coupling soft
disorder predictors with the next generation of complex structure predictors.",1
260,"The human capability to reason about one domain by using knowledge of other
domains has been researched for more than 50 years, but models that are
formally sound and predict cognitive process are sparse. We propose a formally
sound method that models associative reasoning by adapting logical reasoning
mechanisms. In particular it is shown that the combination with large
commensense knowledge within a single reasoning system demands for an efficient
and powerful association technique. This approach is also used for modelling
mind-wandering and the Remote Associates Test (RAT) for testing creativity. In
a general discussion we show implications of the model for a broad variety of
cognitive phenomena including consciousness.",1
261,"The idea that memory behavior relies on a gradually-changing internal state
has a long history in mathematical psychology. This chapter traces this line of
thought from statistical learning theory in the 1950s, through distributed
memory models in the latter part of the 20th century and early part of the 21st
century through to modern models based on a scale-invariant temporal history.
We discuss the neural phenomena consistent with this form of representation and
sketch the kinds of cognitive models that can be constructed using it and
connections with formal models of various memory tasks.",1
262,"COVID-19 pandemic, is still unknown and is an important open question. There
are speculations that bats are a possible origin. Likewise, there are many
closely related (corona-) viruses, such as SARS, which was found to be
transmitted through civets. The study of the different hosts which can be
potential carriers and transmitters of deadly viruses to humans is crucial to
understanding, mitigating and preventing current and future pandemics. In
coronaviruses, the surface (S) protein, or spike protein, is an important part
of determining host specificity since it is the point of contact between the
virus and the host cell membrane. In this paper, we classify the hosts of over
five thousand coronaviruses from their spike protein sequences, segregating
them into clusters of distinct hosts among avians, bats, camels, swines, humans
and weasels, to name a few. We propose a feature embedding based on the
well-known position-weight matrix (PWM), which we call PWM2Vec, and use to
generate feature vectors from the spike protein sequences of these
coronaviruses. While our embedding is inspired by the success of PWMs in
biological applications such as determining protein function, or identifying
transcription factor binding sites, we are the first (to the best of our
knowledge) to use PWMs in the context of host classification from viral
sequences to generate a fixed-length feature vector representation. The results
on the real world data show that in using PWM2Vec, we are able to perform
comparably well as compared to baseline models. We also measure the importance
of different amino acids using information gain to show the amino acids which
are important for predicting the host of a given coronavirus.",1
263,"Certain defense mechanisms of phages against the immune system of their
bacterial host rely on cooperation of phages. Motivated by this example we
analyse invasion probabilities of cooperative parasites in host populations
that are moderately structured. More precisely we assume that hosts are
arranged on the vertices of a configuration model and that offspring of
parasites move to nearest neighbours sites to infect new hosts. We consider
parasites that generate many offspring at reproduction, but do this (usually)
only when infecting a host simultaneously. In this regime we identify and
analyse the spatial scale of the population structure at which invasion of
parasites turns from being an unlikely to an highly probable event.",1
264,"The Moran process is a classic stochastic process that models invasion
dynamics on graphs. A single ""mutant"" (e.g., a new opinion, strain, social
trait etc.) invades a population of residents spread over the nodes of a graph.
The mutant fitness advantage $\delta\geq 0$ determines how aggressively mutants
propagate to their neighbors. The quantity of interest is the fixation
probability, i.e., the probability that the initial mutant eventually takes
over the whole population. However, in realistic settings, the invading mutant
has an advantage only in certain locations. E.g., a bacterial mutation allowing
for lactose metabolism only confers an advantage on places where dairy products
are present. In this paper we introduce the positional Moran process, a natural
generalization in which the mutant fitness advantage is only realized on
specific nodes called active nodes. The associated optimization problem is
fixation maximization: given a budget $k$, choose a set of $k$ active nodes
that maximize the fixation probability of the invading mutant. We show that the
problem is NP-hard, while the optimization function is not submodular, thus
indicating strong computational hardness. Then we focus on two natural limits.
In the limit of $\delta\to\infty$ (strong selection), although the problem
remains NP-hard, the optimization function becomes submodular and thus admits a
constant-factor approximation using a simple greedy algorithm. In the limit of
$\delta\to 0$ (weak selection), we show that in $O(m^\omega)$ time we can
obtain a tight approximation, where $m$ is the number of edges and $\omega$ is
the matrix-multiplication exponent. Finally, we present an experimental
evaluation of the new algorithms together with some proposed heuristics.",1
265,"Longitudinal gross tumour volume (GTV) clinical data from head and neck
cancer patients show that tumours of a similar size and stage pre-treatment may
respond very differently to the same radiotherapy fractionation protocol.
Mathematical models of radiation response are often proposed as a means to
predict treatment outcome and prescribe more personalised fractionation
protocols. Predictive mathematical models aimed towards clinical applications
should be sufficiently detailed to capture the range of dynamics observed in
vivo while being sufficiently simple such that the model parameters are
identifiable with respect to the data typically available for model
calibration.
  In this paper we show that models describing the spatiotemporal heterogeneity
of the intratumoural composition may better capture observed in vivo dynamics
than those describing the evolution of the tumour volume alone. We compare the
range of qualitative responses with respect to GTV data throughout radiotherapy
treatment to those observed in a cohort of head and neck cancer patients.
Synthetic data studies are used to address questions of identifiability and
uncertainty quantification. Our results argue that, to develop accurate,
predictive mathematical models, there is a simultaneous need for more detailed
models (possibly incorporating some notion of tumour heterogeneity) along with
more sophisticated data metrics with which to inform such models.",1
266,"Wavefront propagation and the appearance of spatially distant clusters of
activity generically arise in many contexts including neuron firing, social
contagions, epidemics, and critical infrastructure failures. These phenomena
are in direct competition and represent a frustration between the network's
geometry and topology. Here, we extend their study to systems with higher-order
interactions by analyzing cascades over noisy geometric complexes, which
contain short- and long-range `simplices' that encode dyadic, triadic, and
high-order dependencies. We present a simplicial threshold model (STM) for
cascades in which a vertex $v_i$ becomes active only when the activity across
its simplicial neighbors -- including adjacent 1-simplices, 2-simplices, etc.
-- surpasses a threshold $T_i$. We show that higher-order interactions and
thresholding can coordinate to robustly guide cascades along a geometric
substrate comprised of lower-adjacent simplices despite the presence of
long-range simplices. We explore this phenomenon for a simplicial-complex-based
model of a neuronal network, i.e., a neuronal complex, revealing that
higher-order interactions also promote the expressiveness and efficiency of
diverse spatio-temporal patterns. We support these findings with bifurcation
theory to predict wavefront and clustering dynamics as well as introduce
simplicial cascade maps that embed simplicial complexes with a latent geometry
in which pairwise distances reflect the time required for STM cascades to
travel between vertices. Our findings and proposed mathematical tools reveal
the dynamical/structural interplay of higher-order nonlinearity and the
multidimensional geometry of simplicial complexes to be a fruitful direction
for uncovering the multiscale mechanisms that orchestrate higher-order
processing within complex systems.",1
267,"Cancers are complex adaptive diseases regulated by the nonlinear feedback
systems between genetic instabilities, environmental signals, cellular protein
flows, and gene regulatory networks. Understanding the cybernetics of cancer
requires the integration of information dynamics across multidimensional
spatiotemporal scales, including genetic, transcriptional, metabolic,
proteomic, epigenetic, and multi-cellular networks. However, the time-series
analysis of these complex networks remains vastly absent in cancer research.
With longitudinal screening and time-series analysis of cellular dynamics,
universally observed causal patterns pertaining to dynamical systems, may
self-organize in the signaling or gene expression state-space of cancer
triggering processes. A class of these patterns, strange attractors, may be
mathematical biomarkers of cancer progression. The emergence of intracellular
chaos and chaotic cell population dynamics remains a new paradigm in systems
oncology. As such, chaotic and complex dynamics are discussed as mathematical
hallmarks of cancer cell fate dynamics herein. Given the assumption that
time-resolved single-cell datasets are made available, a survey of
interdisciplinary tools and algorithms from complexity theory, are hereby
reviewed to investigate critical phenomena and chaotic dynamics in cancer
ecosystems. To conclude, the perspective cultivates an intuition for
computational systems oncology in terms of nonlinear dynamics, information
theory, inverse problems and complexity. We highlight the limitations we see in
the area of statistical machine learning but the opportunity at combining it
with the symbolic computational power offered by the mathematical tools
explored.",1
268,"Biomolecular condensates, physically underpinned to a significant extent by
liquid-liquid phase separation (LLPS), are now widely recognized by numerous
experimental studies to be of fundamental biological, biomedical, and
biophysical importance. In the face of experimental discoveries, analytical
formulations emerged as a powerful yet tractable tool in recent theoretical
investigations of the role of LLPS in the assembly and dissociation of these
condensates. The pertinent LLPS often involves, though not exclusively,
intrinsically disordered proteins engaging in multivalent interactions that are
governed by their amino acid sequences. For researchers interested in applying
these theoretical methods, here we provide a practical guide to a set of
computational techniques devised for extracting sequence-dependent LLPS
properties from analytical formulations. The numerical procedures covered
include those for the determinination of spinodal and binodal phase boundaries
from a general free energy function with examples based on the random phase
approximation in polymer theory, construction of tie lines for
multiple-component LLPS, and field-theoretic simulation of multiple-chain
heteropolymeric systems using complex Langevin dynamics. Since a more accurate
physical picture often requires comparing analytical theory against
explicit-chain model predictions, a commonly utilized methodology for
coarse-grained molecular dynamics simulations of sequence-specific LLPS is also
briefly outlined.",1
269,"Double-strand DNA breaks (DSBs) are a form of DNA damage that can cause
abnormal chromosomal rearrangements. Recent technologies based on
high-throughput experiments have obvious high costs and technical
challenges.Therefore, we design a graph neural network based method to predict
DSBs (GraphDSB), using DNA sequence features and chromosome structure
information. In order to improve the expression ability of the model, we
introduce Jumping Knowledge architecture and several effective structural
encoding methods. The contribution of structural information to the prediction
of DSBs is verified by the experiments on datasets from normal human epidermal
keratinocytes (NHEK) and chronic myeloid leukemia cell line (K562), and the
ablation studies further demonstrate the effectiveness of the designed
components in the proposed GraphDSB framework. Finally, we use GNNExplainer to
analyze the contribution of node features and topology to DSBs prediction, and
proved the high contribution of 5-mer DNA sequence features and two chromatin
interaction modes.",1
270,"Insect flight is a strongly nonlinear and actuated dynamical system. As such,
strategies for understanding its control have typically relied on either
model-based methods or linearizations thereof. Here we develop a framework that
combines model predictive control on an established flight dynamics model and
deep neural networks (DNN) to create an efficient method for solving the
inverse problem of flight control. We turn to natural systems for inspiration
since they inherently demonstrate network pruning with the consequence of
yielding more efficient networks for a specific set of tasks. This bio-inspired
approach allows us to leverage network pruning to optimally sparsify a DNN
architecture in order to perform flight tasks with as few neural connections as
possible, however, there are limits to sparsification. Specifically, as the
number of connections falls below a critical threshold, flight performance
drops considerably. We develop sparsification paradigms and explore their
limits for control tasks. Monte Carlo simulations also quantify the statistical
distribution of network weights during pruning given initial random weights of
the DNNs. We demonstrate that on average, the network can be pruned to retain
approximately 7% of the original network weights, with statistical
distributions quantified at each layer of the network. Overall, this work shows
that sparsely connected DNNs are capable of predicting the forces required to
follow flight trajectories. Additionally, sparsification has sharp performance
limits.",1
271,"Environmental temperature (ET) often changes the nutrient intake/output for
layers. Changing feed formulations based on ET may need to be utilized to
obtain optimum performance, shell quality and bone status. This study was
conducted to investigate the effects of temperature, Ca intake, non-phytate P
(NPP) intake and in vitro limestone solubility (LS) on egg-shell quality and
bone status in commercial White Leghorn hens. Egg mass and shell weight per
unit surface area (SWUSA) decreased with increasing ET (p lower than 0.05),
especially when ET was 29.7 C (cycling mean ET)or a constant ET was 32.2 C.
Feeding layers a low soluble larger particle size limestone instead of a highly
soluble limestone produced beneficial effects for SWUSA at the thermoneutral ET
(21.1 C) but the beneficial effect was less or disappeared when ET was higher
than 26.6 C in EXP 1 and 2. Feeding layers 245 and 353 mg NPP/h/d supported
satisfactory bone status at 21.1 C, however layers housed at higher than 30 C
needed an additional intake of 50 mg NPP/h/d to support bone integrity. Results
of EXP 1 and 2 indicates that 48 week old layers housed in thermoneutral or
warmer ET require a minimum of 4.2 g Ca/h/d for maintaining optimum shell
quality and bone integrity. Feeding low LS (34.1% in vitro solubility) improved
egg shell quality only for hens housed in thermoneutral ET (21.1 C) and did not
improve egg shell quality at higher ET (constant or cycling). Daily NPP intake
of 245 and 353 mg/h/d supported optimum egg production and bone status at 21.1
C, respectively. A higher NPP and Ca intake may be required for bone status
compared to egg production, especially in older hens.",1
272,"Background and Objective: Several studies confirm that the age of hens has a
tremendous impact on external and internal egg quality characteristics. Egg
production could be at serious risk if egg quality characteristics and age of
hens are not seriously considered. This study was conducted to analyze the
phenotypic correlations between some internal and external egg quality
characteristics in old laying hens. Materials and Methods: A total of 288 eggs
of 85-week-old Hy-Line Brown laying hens were collected during 3 weeks and
their internal and external egg characteristics were evaluated. Results:
Phenotypic correlations between egg quality characteristics in old laying hens
indicate a negative impact on shell and albumen quality but not affected yolk
quality characteristics. Conclusion: This study helps to understand that
raising laying hens above 80 weeks would have a negative impact on egg quality
characteristics.",1
273,"Background and Objective: In the current study, we sought to determine the
value of a meta-analysis to improve decision-making processes related to
nutrition in the poultry industry. To this end, nine commercial size
experiments were conducted to test the effect of a phytogenic feed additive and
three approaches were applied to the data. Materials and Methods: In all
experiments, 1-day-old male Cobb 500 chicks were used and fed corn-soybean meal
diets. Two dietary treatments were tested: T1, control diet and T2, control
diet + feed additive at a 0.05% inclusion rate. The experimental units were
broiler houses (7 experiments), floor pens (1 experiment) and cages (1
experiment). The response variables were final body weight, feed intake, feed
conversion ratio, mortality and production efficiency. Analyses of variance of
data from each and all the experiments were performed using SAS under
completely randomized non-blocked or blocked designs, respectively. The
meta-analyses were performed in R programming language. Results: No
statistically significant effects were found in the evaluated variables in any
of the independent experiments (p>0.12), nor following the application of a
block design (p>0.08). The meta-analyses showed no statistically significant
global effects in terms of final body weight (p>0.19), feed intake (p>0.23),
mortality (p>0.09), or European Production Efficiency Factor (p>0.08); however,
a positive global effect was found with respect to feed conversion ratio
(p<0.046). Conclusion: This meta-analysis demonstrated that the phytogenic feed
additive improved the efficiency of birds to convert feed to body weight (35 g
less feed per 1 kg of body weight obtained). Thus, the use of meta-analyses in
commercial-scale poultry trials can increase statistical power and as a result,
help to detect statistical differences if they exist.",1
274,"In this paper we analyze the relaxation to steady-state of intracellular
diffusion in a pair of cells with gap-junction coupling. Gap junctions are
prevalent in most animal organs and tissues, providing a direct diffusion
pathway for both electrical and chemical communication between cells. Most
analytical models of gap junctions focus on the steady-state diffusive flux and
the associated effective diffusivity. Here we investigate the relaxation to
steady state in terms of the so-called local accumulation time. The latter is
commonly used to estimate the time to form a protein concentration gradient
during morphogenesis. The basic idea is to treat the fractional deviation from
the steady-state concentration as a cumulative distribution for the local
accumulation time. One of the useful features of the local accumulation time is
that it takes into account the fact that different spatial regions can relax at
different rates. We consider both static and dynamic gap junction models. The
former treats the gap junction as a resistive channel with effective
permeability $\mu$, whereas the latter represents the gap junction as a
stochastic gate that randomly switches between an open and closed state. The
local accumulation time is calculated by solving the diffusion equation in
Laplace space and then taking the small-$s$ limit. We show that the
accumulation time is a monotonically increasing function of spatial position,
with a jump discontinuity at the gap junction. This discontinuity vanishes in
the limit $\mu \rightarrow \infty$ for a static junction and $\beta \rightarrow
0$ for a stochastically-gated junction, where $\beta$ is the rate at which the
gate closes.
  Finally, our results are generalized to the case of a linear array of cells
with nearest neighbor gap junction coupling.",1
275,"Transfer Entropy, a generalisation of Granger Causality, promises to measure
""information transfer"" from a source to a target signal by ignoring
self-predictability of a target signal when quantifying the source-target
relationship. A simple example for signals with such self-predictability are
narrowband signals. These are both thought to be intrinsically generated by the
brain as well as commonly dealt with in analyses of brain signals, where
band-pass filters are used to separate responses from noise. However, the use
of Transfer Entropy is usually discouraged in such cases. We simulate
simplistic examples where we confirm the failure of classic implementations of
Transfer Entropy when applied to narrow-band signals, as made evident by a
flawed recovery of effect sizes and interaction delays. We propose an
alternative approach based on a whitening of the input signals before computing
a bivariate measure of directional time-lagged dependency. This approach solves
the problems found in the simple simulated systems. Finally, we explore the
behaviour of our measure when applied to delta and theta response components in
Magnetoencephalography (MEG) responses to continuous speech. The small effects
that our measure attributes to a directed interaction from the stimulus to the
neuronal responses are stronger in the theta than in the delta band. This
suggests that the delta band reflects a more predictive coupling, while the
theta band is stronger involved in bottom-up, reactive processing. Taken
together, we hope to increase the interest in directed perspectives on
frequency-specific dependencies.",1
276,"A general topic of current interest is the analysis of diffusion problems in
singularly perturbed domains with small interior targets or traps (the narrow
capture problem). One major application is to intracellular diffusion, where
the targets typically represent some form of reactive biochemical substrate.
Most studies of the narrow capture problem treat the target boundaries as
totally absorbing. In this paper, we analyze the three-dimensional narrow
capture problem in the more realistic case of partially reactive target
boundaries. We begin by considering classical Robin boundary conditions.
Matching inner and outer solutions of the single-particle probability density,
we derive an asymptotic expansion of the Laplace transformed flux into each
reactive surface in powers of $\epsilon$, where $\epsilon \rho$ is a given
target size. In turn, the fluxes determine the splitting probabilities for
target absorption. We then extend our analysis to more general types of
reactive targets by combining matched asymptotic analysis with an
encounter-based formulation of diffusion-mediated surface reactions. That is,
we derive an asymptotic expansion of the joint probability density for particle
position and the boundary local time. The effects of surface reactions are then
incorporated via an appropriate stopping condition for the boundary local time.
Finally, we illustrate the theory by exploring how the leading-order
contributions to the splitting probabilities depend on the choice of surface
reactions. In particular, we show that there is an effective renormalization of
the target radius of the form $\rho\rightarrow \rho-\widetilde{\Psi}(1/\rho)$,
where $\widetilde{\Psi}$ is the Laplace transform of the stopping local time
distribution.",1
277,"Many processes in cell biology involve diffusion in a domain $\Omega$ that
contains a target $\calU$ whose boundary $\partial \calU$ is a chemically
reactive surface. Such a target could represent a single reactive molecule, an
intracellular compartment or a whole cell. Recently, a probabilistic framework
for studying diffusion-mediated surface reactions has been developed that
considers the joint probability density or propagator for the particle position
and the so-called boundary local time. The latter characterizes the amount of
time that a Brownian particle spends in the neighborhood of a point on a
totally reflecting boundary. The effects of surface reactions are then
incorporated via an appropriate stopping condition for the boundary local time.
In this paper we generalize the theory of diffusion-mediated surface reactions
to cases where the whole interior target domain $\calU$ acts as a partial
absorber rather than the target boundary $\partial \calU$. Now the particle can
freely enter and exit $\calU$, and is only able to react (be absorbed) within
$\calU$. The appropriate Brownian functional is then the occupation time
(accumulated time that the particle spends within $\calU$) rather than the
boundary local time. We show that both cases can be considered within a unified
framework by using a Feynman-Kac formula to derive a boundary value problem
(BVP) for the propagator of the corresponding Brownian functional, and
introducing an associated stopping condition. We illustrate the theory by
calculating the mean first passage time (MFPT) for a spherical target $\calU$
located at the center of a spherical domain $\Omega$. This is achieved by
solving the propagator BVP directly, rather than using spectral methods. We
find that if the first moment of the stopping time density is infinite, then
the MFPT is also infinite, that is, the spherical target is not sufficiently
absorbing.",1
278,"What can we hope to learn about brains from the free energy principle? In
adopting the ""primordial soup"" physical model, Bruineberg et al. perpetuate the
unsupported notion that the free energy principle has a meaningful
physical--and neuronal--interpretation. We examine how minimization of free
energy arises in physical contexts, and what this can and cannot tell us about
brains.",1
279,"Reaction networks (RNs) comprise a set $X$ of species and a set $\mathscr{R}$
of reactions $Y\to Y'$, each converting a multiset of educts $Y\subseteq X$
into a multiset $Y'\subseteq X$ of products. RNs are equivalent to directed
hypergraphs. However, not all RNs necessarily admit a chemical interpretation.
Instead, they might contradict fundamental principles of physics such as the
conservation of energy and mass or the reversibility of chemical reactions. The
consequences of these necessary conditions for the stoichiometric matrix
$\mathbf{S} \in \mathbb{R}^{X\times\mathscr{R}}$ have been discussed
extensively in the literature. Here, we provide sufficient conditions for
$\mathbf{S}$ that guarantee the interpretation of RNs in terms of balanced sum
formulas and structural formulas, respectively.
  Chemically plausible RNs allow neither a perpetuum mobile, i.e., a ""futile
cycle"" of reactions with non-vanishing energy production, nor the creation or
annihilation of mass. Such RNs are said to be thermodynamically sound and
conservative. For finite RNs, both conditions can be expressed equivalently as
properties of $\mathbf{S}$. The first condition is vacuous for reversible
networks, but it excludes irreversible futile cycles and - in a stricter sense
- futile cycles that even contain an irreversible reaction. The second
condition is equivalent to the existence of a strictly positive reaction
invariant. Furthermore, it is sufficient for the existence of a realization in
terms of sum formulas, obeying conservation of ""atoms"". In particular, these
realizations can be chosen such that any two species have distinct sum
formulas, unless $\mathbf{S}$ implies that they are ""obligatory isomers"". In
terms of structural formulas, every compound is a labeled multigraph, in
essence a Lewis formula, and reactions comprise only a rearrangement of bonds
such that the total bond order is preserved.",1
280,"Microbial biomass carbon (MBC), a crucial soil labile carbon fraction, is the
most active component of the soil organic carbon (SOC) that regulates
bio-geochemical processes in terrestrial ecosystems. Some studies in the
literature ignore the effect of microbial population growth on carbon
decomposition rates. In reality, we might expect that the decomposition rate
should be related to the population of microbes in the soil and have a positive
relationship with the size of the microbial biomass pool. In this study, we
explore the effect of microbial population growth on the accuracy of modelling
soil carbon sequestration by developing and comparing two soil carbon models
that consider a carrying capacity and limit to the growth of the microbial
pool. We apply our models to three datasets, two small and one large datasets,
and we select the best model in terms of having the best predictive performance
through two model selection methods. Through this analysis we reveal that
commonly used complex soil carbon models can over-fit in the presence of both
small and large time-series datasets, and our simpler model can produce more
accurate predictions. We conclude that considering the microbial population
growth in a soil carbon model improves the accuracy of a model in the presence
of a large dataset.",1
281,"In this work, we let the sweet spot be the region where a sound wave
generated by an array of loudspeakers is psycho-acoustically close to a desired
auditory scene, and we develop a method that aims to generate a sound wave that
directly maximizes this sweet spot. Our method incorporates psycho-acoustic
principles from the onset and is flexible: while it imposes little to no
constraints on the regions of interest, the arrangement of loudspeakers or
their radiation pattern, it allows for a wide array of psycho-acoustic models
that include state-of-the-art monaural psycho-acoustic models. Our method
leverages tools from analysis and optimization that allow for its mathematical
analysis and efficient implementation. Our numerical results show that our
method yields larger sweet spots compared to some state-of-the-art methods when
performing sound field reconstruction for sinusoidal signals using van de Par's
psycho-acoustic model.",1
282,"Two-sample p-values test for statistical significance. Yet p-values cannot
determine if a result has a negligible (near-zero) effect size, nor compare
evidence for negligibility among independent studies. We propose the most
difference in means ({\delta}M) statistic to assess the practical
insignificance of results by measuring the evidence for a negligible effect
size. Both {\delta}M and the relative form of {\delta}M allow hypothesis
testing for negligibility and outperform other candidate statistics in
identifying results with stronger evidence of negligible effect. We compile
results from broadly related experiments and use the relative {\delta}M to
compare practical insignificance across different measurement methods and
experiment models. Reporting the relative {\delta}M builds consensus for
negligible effect size by making near-zero results more quantitative and
publishable.",1
283,"Phagocytosis is the process of engulfment and internalization of
comparatively large particles by the cell, that plays a central role in the
functioning of our immune system. We study the process of phagocytosis by
considering a simplified coarse grained model of a three-dimensional vesicle,
having uniform adhesion interaction with a rigid particle, in the presence of
curved membrane proteins and active cytoskeletal forces. Complete engulfment is
achieved when the bending energy cost of the vesicle is balanced by the gain in
the adhesion energy. The presence of curved (convex) proteins reduces the
bending energy cost by self-organizing with higher density at the highly curved
leading edge of the engulfing membrane, which forms the circular rim of the
phagocytic cup that wraps around the particle. This allows the engulfment to
occur at much smaller adhesion strength. When the curved proteins exert
outwards protrusive forces, representing actin polymerization, at the leading
edge, we find that engulfment is achieved more quickly and at lower protein
density. We consider spherical as well as non-spherical particles, and find
that non-spherical particles are more difficult to engulf in comparison to the
spherical particles of the same surface area. For non-spherical particles, the
engulfment time crucially depends upon the initial orientation of the particles
with respect to the vesicle. Our model offers a mechanism for the spontaneous
self-organization of the actin cytoskeleton at the phagocytic cup, in good
agreement with recent high-resolution experimental observations.",1
284,"Prokaryotic viruses, which infect bacteria and archaea, are key players in
microbial communities. Predicting the hosts of prokaryotic viruses helps
decipher the dynamic relationship between microbes. Although there are
experimental methods for host identification, they are either labor-intensive
or require the cultivation of the host cells, creating a need for computational
host prediction. Despite some promising results, computational host prediction
remains a challenge because of the limited known interactions and the sheer
amount of sequenced phages by high-throughput sequencing technologies. The
state-of-the-art methods can only achieve 43% accuracy at the species level.
This work presents CHERRY, a tool formulating host prediction as link
prediction in a knowledge graph. As a virus-prokaryotic interaction prediction
tool, CHERRY can be applied to predict hosts for newly discovered viruses and
also the viruses infecting antibiotic-resistant bacteria. We demonstrated the
utility of CHERRY for both applications and compared its performance with the
state-of-the-art methods in different scenarios. To our best knowledge, CHERRY
has the highest accuracy in identifying virus-prokaryote interactions. It
outperforms all the existing methods at the species level with an accuracy
increase of 37%. In addition, CHERRY's performance is more stable on short
contigs than other tools.",1
285,"Conceptual and mathematical models of neurons have lagged behind empirical
understanding for decades. Here we extend previous work in modeling biological
systems with fully scale-independent quantum information-theoretic tools to
develop a uniform, scalable representation of synapses, dendritic and axonal
processes, neurons, and local networks of neurons. In this representation,
hierarchies of quantum reference frames act as hierarchical active-inference
systems. The resulting model enables specific predictions of correlations
between synaptic activity, dendritic remodeling, and trophic reward. We
summarize how the model may be generalized to nonneural cells and tissues in
developmental and regenerative contexts.",1
286,"There is evidence that for most brain networks all pathways between cortical
regions either pass through the thalamus or a transthalamic parallel route
exists for any direct corticocortical connection. The networks created by these
connections through the thalamus, which effects neural signals in an inhibitory
manner, has not been significantly studied in the literature and as such it is
of interest to investigate the role of the thalamus in selective inhibition of
these networks. Using a linear-threshold model for individual brain
subnetworks, in this work we study both hierarchical and star-connected
thalamocortical networks. Using tools from singular perturbation and switched
system theory we show that selective inhibition can be achieved in such
networks through a combination of feedback and feedforward control. We finish
with numerical examples illustrating that the input from the thalamus appears
to allow for a lower magnitude control law than for strictly cortical networks.",1
287,"We report on the first proof-of-concept system demonstrating how one can
control a qubit with mental activity. We developed a method to encode neural
correlates of mental activity as instructions for a quantum computer. Brain
signals are detected utilizing electrodes placed on the scalp of a person, who
learns how to produce the required mental activity to issue instructions to
rotate and measure a qubit. Currently, our proof-of-concept runs on a software
simulation of a quantum computer. At the time of writing, available quantum
computing hardware and brain activity sensing technology are not sufficiently
developed for real-time control of quantum states with the brain. But we are
one step closer to interfacing the brain with real quantum machines, as
improvements in hardware technology at both fronts become available in time to
come. The paper ends with a discussion on some of the challenging problems that
need to be addressed before we can interface the brain with quantum hardware.",1
288,"The brain's functional connectivity fluctuates over time instead of remaining
steady in a stationary mode even during the resting state. This fluctuation
establishes the dynamical functional connectivity that transitions in a
non-random order between multiple modes. Yet it remains unexplored how the
transition facilitates the entire brain network as a dynamical system and what
utility this mechanism for dynamic reconfiguration can bring over the widely
used graph theoretical measurements. To address these questions, we propose to
conduct an energetic analysis of functional brain networks using resting-state
fMRI and behavioral measurements from the Human Connectome Project. Through
comparing the state transition energy under distinct adjacent matrices, we
justify that dynamic functional connectivity leads to 60% less energy cost to
support the resting state dynamics than static connectivity when driving the
transition through default mode network. Moreover, we demonstrate that
combining graph theoretical measurements and our energy-based control
measurements as the feature vector can provide complementary prediction power
for the behavioral scores (Combination vs. Control: t = 9.41, p = 1.64e-13;
Combination vs. Graph: t = 4.92, p = 3.81e-6). Our approach integrates
statistical inference and dynamical system inspection towards understanding
brain networks.",1
289,"Deep autoencoders are often extended with a supervised or adversarial loss to
learn latent representations with desirable properties, such as greater
predictivity of labels and outcomes or fairness with respects to a sensitive
variable. Despite the ubiquity of supervised and adversarial deep latent factor
models, these methods should demonstrate improvement over simpler linear
approaches to be preferred in practice. This necessitates a reproducible linear
analog that still adheres to an augmenting supervised or adversarial objective.
We address this methodological gap by presenting methods that augment the
principal component analysis (PCA) objective with either a supervised or an
adversarial objective and provide analytic and reproducible solutions. We
implement these methods in an open-source Python package, AugmentedPCA, that
can produce excellent real-world baselines. We demonstrate the utility of these
factor models on an open-source, RNA-seq cancer gene expression dataset,
showing that augmenting with a supervised objective results in improved
downstream classification performance, produces principal components with
greater class fidelity, and facilitates identification of genes aligned with
the principal axes of data variance with implications to development of
specific types of cancer.",1
290,"In particle-based stochastic reaction-diffusion models, reaction rate and
placement kernels are used to decide the probability per time a reaction can
occur between reactant particles, and to decide where product particles should
be placed. When choosing kernels to use in reversible reactions, a key
constraint is to ensure that detailed balance of pointwise reaction-fluxes
holds at equilibrium. In this work we formulate a general partial-integral
differential equation model that encompasses several of the commonly used
contact reactivity (e.g. Collins-Kimball-Smoluchowski) and volume reactivity
(e.g. Doi) particle models. From these equations we derive a detailed balance
condition for the reversible $\textrm{A} + \textrm{B} \leftrightarrows
\textrm{C}$ reaction. In bounded domains with no-flux boundary conditions, when
choosing unbinding kernels consistent with several commonly used binding
kernels, we show that preserving pointwise detailed balance requires spatially
varying unbinding rate functions near the domain boundary. Brownian Dynamics
simulation algorithms can realize such varying rates through ignoring domain
boundaries during unbinding, but rejecting unbinding events that result in
product particles being placed outside the domain.",1
291,"Colorectal cancer (CRC) is the third most frequent malignant disease in the
world. In some countries with established screening programs, its incidence and
mortality have decreased, and survival has improved. AIMS: To obtain reliable
data about the epidemiology of CRC in Chile, we analyzed the trends in the last
ten years and the influence of observable factors on survival, including
explicit guarantees in CRC treatment access (GES program).
  Publicly available data published by the Health Ministry and National
Institute of Statistics were used. Data were obtained from registries of
mortality and hospital discharges, making follow-up of the individuals
possible. Crude and age-standardized incidence and mortality rates were
calculated, and individual survival was studied by constructing Kaplan-Meier
curves. Finally, a Cox statistical model was established to estimate the impact
of the observable factors.",1
292,"The capability of current force fields to reproduce RNA structural dynamics
is limited. Several methods have been developed to take advantage of
experimental data in order to enforce agreement with experiments. We herein
extend an existing framework, which allows arbitrarily chosen force-field
correction terms to be fitted by quantification of the discrepancy between
observables back-calculated from simulation and corresponding experiments. We
apply a robust regularization protocol to avoid overfitting, and additionally
introduce and compare a number of different regularization strategies, namely
L1-, L2-, Kish Size-, Relative Kish Size- and Relative Entropy-penalties. The
training set includes a GACC tetramer as well as more challenging systems,
namely gcGAGAgc and gcUUCGgc RNA tetraloops. Specific intramolecular hydrogen
bonds in the AMBER RNA force field are corrected with automatically determined
parameters that we call gHBfix$_{opt}$. A validation involving a separate
simulation of a system present in the training set (gcUUCGgc) and new systems
not seen during training (CAAU and UUUU tetramers) displays improvements
regarding native population of the tetraloop as well as good agreement with
NMR-experiments for tetramers when using the new parameters. Then we simulate
folded RNAs (a kink-turn and L1 stalk rRNA) including hydrogen bond types not
sufficiently present in the training set. This allows a final modification of
the parameter set which is named gHBfix21 and is suggested to be applicable to
a wider range of RNA systems.",1
293,"We have developed a graphic user interface (GUI), ExBrainable, dedicated to
convolutional neural networks (CNN) model training and visualization in
electroencephalography (EEG) decoding. Available functions include model
training, evaluation, and parameter visualization in terms of temporal and
spatial representations. We demonstrate these functions using a well-studied
public dataset of motor-imagery EEG and compare the results with existing
knowledge of neuroscience. The primary objective of ExBrainable is to provide a
fast, simplified, and user-friendly solution of EEG decoding for investigators
across disciplines to leverage cutting-edge methods in brain/neuroscience
research.",1
294,"Active inference is an account of cognition and behavior in complex systems
which brings together action, perception, and learning under the theoretical
mantle of Bayesian inference. Active inference has seen growing applications in
academic research, especially in fields that seek to model human or animal
behavior. While in recent years, some of the code arising from the active
inference literature has been written in open source languages like Python and
Julia, to-date, the most popular software for simulating active inference
agents is the DEM toolbox of SPM, a MATLAB library originally developed for the
statistical analysis and modelling of neuroimaging data. Increasing interest in
active inference, manifested both in terms of sheer number as well as
diversifying applications across scientific disciplines, has thus created a
need for generic, widely-available, and user-friendly code for simulating
active inference in open-source scientific computing languages like Python. The
Python package we present here, pymdp (see
https://github.com/infer-actively/pymdp), represents a significant step in this
direction: namely, we provide the first open-source package for simulating
active inference with partially-observable Markov Decision Processes or POMDPs.
We review the package's structure and explain its advantages like modular
design and customizability, while providing in-text code blocks along the way
to demonstrate how it can be used to build and run active inference processes
with ease. We developed pymdp to increase the accessibility and exposure of the
active inference framework to researchers, engineers, and developers with
diverse disciplinary backgrounds. In the spirit of open-source software, we
also hope that it spurs new innovation, development, and collaboration in the
growing active inference community.",1
295,"Visual attention estimation is an active field of research at the crossroads
of different disciplines: computer vision, artificial intelligence and
medicine. One of the most common approaches to estimate a saliency map
representing attention is based on the observed images. In this paper, we show
that visual attention can be retrieved from EEG acquisition. The results are
comparable to traditional predictions from observed images, which is of great
interest. For this purpose, a set of signals has been recorded and different
models have been developed to study the relationship between visual attention
and brain activity. The results are encouraging and comparable with other
approaches estimating attention with other modalities. The codes and dataset
considered in this paper have been made available at
\url{https://figshare.com/s/3e353bd1c621962888ad} to promote research in the
field.",1
296,"Analysis of longitudinal Electronic Health Record (EHR) data is an important
goal for precision medicine. Difficulty in applying Machine Learning (ML)
methods, either predictive or unsupervised, stems in part from the
heterogeneity and irregular sampling of EHR data. We present an unsupervised
probabilistic model that captures nonlinear relationships between variables
over continuous-time. This method works with arbitrary sampling patterns and
captures the joint probability distribution between variable measurements and
the time intervals between them. Inference algorithms are derived that can be
used to evaluate the likelihood of future using under a trained model. As an
example, we consider data from the United States Veterans Health Administration
(VHA) in the areas of diabetes and depression. Likelihood ratio maps are
produced showing the likelihood of risk for moderate-severe vs minimal
depression as measured by the Patient Health Questionnaire-9 (PHQ-9).",1
297,"Modern histopathological image analysis relies on the segmentation of cell
structures to derive quantitative metrics required in biomedical research and
clinical diagnostics. State-of-the-art deep learning approaches predominantly
apply convolutional layers in segmentation and are typically highly customized
for a specific experimental configuration; often unable to generalize to
unknown data. As the model capacity of classical convolutional layers is
limited by a finite set of learned kernels, our approach uses a graph
representation of the image and focuses on the node transitions in multiple
magnifications. We propose a novel architecture for semantic segmentation of
cell nuclei robust to differences in experimental configuration such as
staining and variation of cell types. The architecture is comprised of a novel
neuroplastic graph attention network based on residual graph attention layers
and concurrent optimization of the graph structure representing multiple
magnification levels of the histopathological image. The modification of graph
structure, which generates the node features by projection, is as important to
the architecture as the graph neural network itself. It determines the possible
message flow and critical properties to optimize attention, graph structure,
and node updates in a balanced magnification loss. In experimental evaluation,
our framework outperforms ensembles of state-of-the-art neural networks, with a
fraction of the neurons typically required, and sets new standards for the
segmentation of new nuclei datasets.",1
298,"Like a rocket being propelled into space, evolution has engineered flies to
launch into adulthood via multiple stages. Flies develop and deploy two
distinct bodies, linked by the transformative process of metamorphosis. The fly
larva is a soft hydraulic tube that can crawl to find food and avoid predators.
The adult fly has a stiff exoskeleton with articulated limbs capable of
long-distance navigation and rich social interactions. Because the larval and
adult forms are so distinct in structure, they require distinct strategies for
sensing and moving the body. The metamorphic divide thus presents an
opportunity for comparative analysis of neural circuits. Here, we review recent
progress toward understanding the neural mechanisms of proprioception and motor
control in larval and adult Drosophila. We highlight commonalities that point
toward general principles of sensorimotor control and differences that may
reflect unique constraints imposed by biomechanics. Finally, we discuss
emerging opportunities for comparative analysis of neural circuit architecture
in the fly and other animal species.",1
299,"The performance of medical devices that record electrophysiological activity
to diagnose epilepsy and cardiac arrythmia depend on consistently low impedance
interfaces between conductors and the skin. Clinically standard devices, wet
electrodes, use hydrogels and skin abrasion to improve the interface and
recorded signal quality. These electrodes and their required preparation are
challenging to self-administer which reduces the frequency of disease
monitoring and impedes in-home care. Wearable dry electrodes are more
practical; however, they show higher impedance relative to wet electrodes and
are costly to customize. In this work, a fabrication method for producing
anatomically fit, dry electrodes that can be optimized for individuals and/or
specific recording locations on the body is presented. Electroless gold plating
is used in combination with 3D printing to enable anatomically fit,
high-performance, 3D dry electrodes that do not require any skin preparation
and are comfortable to wear. The performance of the example 3D dry electrodes
is compared to clinically standard devices. The resulting electrodes exhibited
an average electrode-skin impedance of 71.2 k{\Omega} at 50Hz and DC offset of
-20 mV, which is within the range achieved by clinical wet electrodes.",1
300,"A wide variety of cultural practices take the form of ""tacit"" knowledge,
where the rules and principles are neither obvious to an observer nor known
explicitly by the practitioners. This poses a problem for cultural evolution:
if beginners cannot simply imitate experts, and experts cannot simply say or
demonstrate what they are doing, how can tacit knowledge pass from generation
to generation? We present a domain-general model of ""tacit teaching"", that
shows how high-fidelity transmission of tacit knowledge is possible. It applies
in cases where the underlying features of the practice are subject to
interacting and competing constraints, as is expected both in embodied and in
social practices. Our model makes predictions for key features of the teaching
process. It predicts a tell-tale distribution of teaching outcomes: some
students will be nearly perfect performers while others receiving the same
instruction will be disastrously bad. This differs from most mainstream
cultural evolution models centered on high-fidelity transmission with minimal
copying errors, which lead to a much narrower distribution where students are
mostly equally mediocre. The model also predicts generic features of the
cultural evolution of tacit knowledge. The evolution of tacit knowledge is
expected to be bursty, with long periods of stability interspersed with brief
periods of dramatic change, and where tacit knowledge, once lost, becomes
essentially impossible to recover.",1
301,"We developed a simple mathematical model to describe criminality and the
justice system composed of the police investigation and court trial. The model
assessed two features of organized crime -- the cost-benefit analysis done by
the crime-susceptible to commit a crime and the whistleblowing of the law
offenders. The model was formulated considering the mass action law commonly
used in the disease propagation modelings, which can shed light on the model's
analysis. The crime-susceptible individuals analyze two opposing forces --
committing crime influenced by the law offenders not caught by police neither
imprisonment by the court trial (benefit of enjoying the corruption incoming),
and the refraction to commit crime influenced by those caught by police or
condemned by a court (cost of incarceration). Moreover, we assessed the dilemma
for those captured by police investigation to participate in the rewarding
whistleblowing program. The model was applied to analyze the ""car wash
operation"" against corruption in Brazil. The model analysis showed that the
cost-benefit analysis of crime-susceptible individuals whether the act of
bribery is worth or not determined the basic crime reproduction number
(threshold); however, the rewarding whistleblowing policies improved the combat
to corruption arising a sub-threshold. Some adopted mechanisms to control the
Covid-19 pandemic shed light on understanding the ""car wash peration"" and
threatens to the fight against corruption. Appropriate coverage of corruption
by media, enhancement of laws against white-collar crimes, well-functioning
police investigation and court trial, and the rewarding whistleblowing policies
inhibited and decreased the corruption.",1
302,"Functional optical imaging in neuroscience is rapidly growing with the
development of new optical systems and fluorescence indicators. To realize the
potential of these massive spatiotemporal datasets for relating neuronal
activity to behavior and stimuli and uncovering local circuits in the brain,
accurate automated processing is increasingly essential. In this review, we
cover recent computational developments in the full data processing pipeline of
functional optical microscopy for neuroscience data and discuss ongoing and
emerging challenges.",1
303,"Nowcasting and forecasting of epidemic spreading, fundamental support for
policy makers' decisions, rely on incidence series of reported cases to derive
the fundamental epidemiological parameters. Two relevant drawbacks for
predictions are the unknown fraction of undocumented cases and levels of
nonpharmacological interventions that span highly heterogeneously across
different places. We describe a simple approach using a compartmental model
including asymptomatic and pre-asymptomatic contagions that allows to estimate
both the level of undocumented infections and the value of $R_t$ from reported
case series in terms of epidemiological parameters. The method was applied to
epidemic series for of COVID-19 across different municipalities in Brazil
allowing to quantify the heterogeneity level of under-reporting across
different places. The reproductive number derived within the current framework
is little sensitive to both diagnosis and infection rates during the
asymptomatic states, while being very sensitive to variations in case count
series. The methods described here are general and we expect that they can be
extended to other epidemiological approaches and surveillance data.",1
304,"As a result of the identification of 'identity' and 'indistinguishability'
and strong experimental evidence for the presence of the associated
Bose-Einstein statistics in human cognition and language, we argued in previous
work for an extension of the research domain of quantum cognition. In addition
to quantum complex vector spaces and quantum probability models, we showed that
quantization itself, with words as quanta, is relevant and potentially
important to human cognition. In the present work, we build on this result, and
introduce a powerful radiation quantization scheme for human cognition. We show
that the lack of independence of the Bose-Einstein statistics compared to the
Maxwell-Boltzmann statistics can be explained by the presence of a 'meaning
dynamics', which causes words to be attracted to the same words. And so words
clump together in the same states, a phenomenon well known for photons in the
early years of quantum mechanics, leading to fierce disagreements between
Planck and Einstein. Using a simple example, we introduce all the elements to
get a better and detailed view of this 'meaning dynamics', such as micro and
macro states, and Maxwell-Boltzmann, Bose-Einstein and Fermi-Dirac numbers and
weights, and compare this example and its graphs, with the radiation
quantization scheme of a Winnie the Pooh story, also with its graphs. By
connecting a concept directly to human experience, we show that entanglement is
a necessity for preserving the 'meaning dynamics' we identified, and it becomes
clear in what way Fermi-Dirac addresses human memory. There, in spaces with
internal parameters identical words can nevertheless be assigned different
states.",1
305,"It has recently been discovered that the viscoelastic properties of cells are
inherent markers reflecting the complex biological states, functions and
malfunctions of the cells. Although the extraction of model parameters from the
viscoelasticity data of many cell types has been done successfully using
integer order mechanical and power-law viscoelastic models, there are some cell
types and conditions where the goodness of fits falls behind. Thus, fractional
order viscoelastic models have been proposed as more general and better suited
for such modeling. In this work, we test such proposed generality using
published data already fitted by integer order models. We find that cell
viscoelasticity data can be fitted using fractional order viscoelastic models
in more situations than integer order. For macrophages, which are among the
white blood cells that function in the immune system, the fractional order
Kelvin-Voigt model best captures pharmacological interventions and maturation
of the cells. The steady state viscosity of macrophages decreases following
depolymerization of F-actin using the drug cytochalasin D, and also decreases
following myosin II breakdown using Blebbistatin. When macrophages are treated
with a bacterium-derived chemoattractant, the steady state viscosity decreases.
Interestingly, both the steady state viscosity and elastic modulus are
progressively altered as the cells become mature and approach senescence. Taken
together, these results show that fractional viscoelastic modeling, more
robustly than integer order modeling, enables the further quantification of
cell function and malfunction, with potential diagnostic and therapeutic
applications especially in cases of cancer and immune system dysfunctions.",1
306,"In natural ecosystems, species often compete with multiple species to survive
and acquire resources. These species can be characterized by their different
growth profile and life history strategies, namely r and K selected species
(Williams et.al., The journal of wildlife management, 2013). In this paper, we
examine the role of different life-history strategies on the stability of the
competitive community along with network complexity. We obtain that the
r-selected species enhance community stability and resilience; on the other
hand, K-selected species show the opposite trend. Network connectance shows a
consistent trend of increasing stability; on the other side, the number of
species shows a context-dependent effect on stability. Stability decreases with
species number when the proportion of r-selected species is low or moderate and
shows precisely the opposite trend when the ratio of r-selected species is
high. We also show that our result is robust irrespective of network structure.",1
307,"Some genes can promote or repress their own expressions, which is called
autoregulation. In general, it is extremely difficult to determine the
existence of autoregulation with direct biochemical approaches. Nevertheless,
autoregulations reveal themselves in gene expression profiles. We prove some
mathematical propositions. Based on these propositions, we develop some simple
but robust mathematical methods that infer the existence of autoregulation from
gene expression data. We also apply our methods to experimental data to
determine some genes that might have autoregulation.",1
308,"A model is presented relating the evolution of genomic GC content over time
to AT$\rightarrow$GC and GC$\rightarrow$AT mutation rates. By employing It\^o
calculus it is shown that if mutation rates in asexually reproducing organisms
are subject to random perturbations that can vary over time several
implications follow. For instance, an extra Brownian motion term appears
influencing nucleotide variability; the greater the variability of the random
perturbations on the mutation rates the stronger the impact of the Brownian
motion term. Reducing the influence of the random perturbations, to limit
fitness decreasing and deleterious mutations, will likely imply divesting
resources to genomic repair systems. The stable mutation rates seen in many
organisms could thus be an evolved strategy to reduce the influence of the
Brownian motion term. Furthermore, if change to genomic GC content, i.e. the GC
content of variable sites or single nucleotide polymorphisms (SNPs), is just as
likely to increase as to decrease, something that resembles knockout of repair
enzymes and removal of selective pressures seen in evolutionary laboratory
experiments, the species genome will likely decay unless infinite resources are
available. These implications are solely a consequence of allowing random
perturbations affect AT- and GC mutation rates and not obtainable using
standard non-stochastic methodology. Finally, a connection between the model
for genomic GC content evolution and the classical Luria-Delbr\""uck mutation
model is presented in a stochastic setting.",1
309,"We propose a computational framework to study the growth and spread of
bacterial biofilms on interfaces, as well as the action of antibiotics on them.
Bacterial membranes are represented by boundaries immersed in a fluid matrix
and subject to interaction forces. Growth, division and death of bacterial
cells follow dynamic energy budget rules, in response to variations in
environmental concentrations of nutrients, toxicants and substances released by
the cells. In this way, we create, destroy and enlarge boundaries, either
spherical or rod-like. Appropriate forces represent details of the interaction
between cells, and the interaction with the environment. Numerical simulations
illustrate the evolution of top views and diametral slices of small biofilm
seeds, as well as the action of antibiotics. We show that cocktails of
antibiotics targeting active and dormant cells can entirely eradicate a
biofilm.",1
310,"Attention-based deep networks have been successfully applied on textual data
in the field of NLP. However, their application on protein sequences poses
additional challenges due to the weak semantics of the protein words, unlike
the plain text words. These unexplored challenges faced by the standard
attention technique include (i) vanishing attention score problem and (ii) high
variations in the attention distribution. In this regard, we introduce a novel
{\lambda}-scaled attention technique for fast and efficient modeling of the
protein sequences that addresses both the above problems. This is used to
develop the {\lambda}-scaled attention network and is evaluated for the task of
protein function prediction implemented at the protein sub-sequence level.
Experiments on the datasets for biological process (BP) and molecular function
(MF) showed significant improvements in the F1 score values for the proposed
{\lambda}-scaled attention technique over its counterpart approach based on the
standard attention technique (+2.01% for BP and +4.67% for MF) and
state-of-the-art ProtVecGen-Plus approach (+2.61% for BP and +4.20% for MF).
Further, fast convergence (converging in half the number of epochs) and
efficient learning (in terms of very low difference between the training and
validation losses) were also observed during the training process.",1
311,"Effective strategies of vaccine prioritization are essential to mitigate the
impacts of severe infectious diseases. We investigate the role of infection
fatality ratio (IFR) and social contact matrices on vaccination prioritization
using a compartmental epidemic model fueled by real-world data of different
diseases (COVID-19 and influenza) and countries (Brazil, Germany, and Uganda).
Our study confirms that massive and early vaccination is extremely effective to
reduce the disease fatality if the contagion is mitigated, but the
effectiveness is increasingly reduced as vaccination beginning delays in an
uncontrolled epidemiological scenario. The optimal and least effective
prioritization strategies depend non-linearly on epidemiological variables.
Regions of the epidemiological parameter space, in which prioritizing the most
vulnerable is more effective than the most contagious individuals, are
substantially broader for COVID-19's IFR in comparison with influenza.
Demographics and social contact matrices deform the phase diagrams but do not
alter their qualitative shapes.",1
312,"Recent breakthroughs in high resolution imaging of biomolecules in solution
with cryo-electron microscopy (cryo-EM) have unlocked new doors for the
reconstruction of molecular volumes, thereby promising further advances in
biology, chemistry, and pharmacological research amongst others. Despite
significant headway, the immense challenges in cryo-EM data analysis remain
legion and intricately inter-disciplinary in nature, requiring insights from
physicists, structural biologists, computer scientists, statisticians, and
applied mathematicians. Meanwhile, recent next-generation volume reconstruction
algorithms that combine generative modeling with end-to-end unsupervised deep
learning techniques have shown promising results on simulated data, but still
face considerable hurdles when applied to experimental cryo-EM images. In light
of the proliferation of such methods and given the interdisciplinary nature of
the task, we propose here a critical review of recent advances in the field of
deep generative modeling for high resolution cryo-EM volume reconstruction. The
present review aims to (i) compare and contrast these new methods, while (ii)
presenting them from a perspective and using terminology familiar to scientists
in each of the five aforementioned fields with no specific background in
cryo-EM. The review begins with an introduction to the mathematical and
computational challenges of deep generative models for cryo-EM volume
reconstruction, along with an overview of the baseline methodology shared
across this class of algorithms. Having established the common thread weaving
through these different models, we provide a practical comparison of these
state-of-the-art algorithms, highlighting their relative strengths and
weaknesses, along with the assumptions that they rely on. This allows us to
identify bottlenecks in current methods and avenues for future research.",1
313,"This paper discusses a simple and explicit toy-model example of the
categorical Hopfield equations introduced in previous work of Manin and the
author. These describe dynamical assignments of resources to networks, where
resources are objects in unital symmetric monoidal categories and assignments
are realized by summing functors. The special case discussed here is based on
computational resources (computational models of neurons) as objects in a
category of DNNs, with a simple choice of the endofunctors defining the
Hopfield equations that reproduce the usual updating of the weights in DNNs by
gradient descent.",1
314,"We routinely communicate distinct social and emotional sentiments through
nuanced touch. For example, we might gently hold another's arm to offer a sense
of calm, yet intensively hold another's arm to express excitement or anxiety.
As this example indicates, distinct sentiments may be shaped by the subtlety in
one's touch delivery. This work investigates how slight distinctions in
skin-to-skin contact influence both the recognition of cued emotional messages
(e.g., anger, sympathy) and the rating of emotional content (i.e., arousal,
valence). By self-selecting preferred gestures (e.g., holding, stroking),
touchers convey distinct messages by touching the receiver's forearm.
Skin-to-skin contact attributes (e.g., velocity, depth, area) are optically
tracked in high resolution. Contact is then examined within gesture, between
messages. The results indicate touchers subtly, but significantly, vary contact
attributes of a gesture to communicate distinct messages, which are
recognizable by receivers. This tuning also correlates with receivers' arousal
and valence. For instance, arousal increases with velocity for stroking, and
depth for holding. Moreover, as shown here with human-to-human touch, valence
is tied with velocity, which is the same trend as reported with brushes. The
findings indicate that subtle nuance in skin-to-skin contact is important in
conveying social messages and inducing emotions.",1
315,"In this paper we model the spreading of the SARS-CoV-2 in Mexico by
introducing a new stochastic approximation constructed from first principles,
structured on the basis of a Latent-Infectious- (Recovered or Deceased)
(LI(RD)) compartmental approximation, where the number of new infected
individuals caused by a single infectious individual per unit time (a day), is
a random variable of a Poisson distribution and whose parameter is modulated
through a weight-like time-dependent function. The weight function serves to
introduce a time dependence to the average number of new infections and as we
will show, this information can be extracted from empirical data, giving to the
model self-consistency and provides a tool to study information about periodic
patterns encoded in the epidemiological dynamics",1
316,"The frequency of heat waves are increasing due to climate change, which leads
to an increase in the occurrence of heat stress in dairy cattle. Previous
studies have shown that dairy cattle identified as high immune responders have
a reduced incidence of disease and improved vaccine response compared to
average and low responders. Additionally, it has been observed that when cells
from immune phenotyped cattle are exposed to in-vitro heat challenge, high
immune responders exhibit increased heat tolerance compared to average and low
responders. Therefore, the objective of this study was to evaluate
physiological parameters and the function of blood mononuclear cells in immune
phenotyped dairy cattle exposed to in-vivo heat challenge. A total of 24 immune
phenotyped lactating dairy cattle (8 high, 8 average and 8 low) were housed in
the tie-stall area of the barn and exposed to an in-vivo heat challenge for 4
hours on 2 subsequent days. Blood samples were taken both pre- and
post-challenge and respiration rates and rectal temperatures were recorded.
Temperature and humidity measurements were taken in correspondence with all
respiration rate and rectal temperature measurements to calculate the
temperature humidity index. Blood mononuclear cells were isolated from blood
collected pre and post challenge and the concentration of heat shock protein 70
and cell proliferation were assessed. Results showed that average and low
responders had significantly greater respiration rates compared to high
responders at a temperature humidity index of 77 and above. High responders had
a higher heat shock protein 70 concentration and greater cell proliferation
after in-vivo heat challenge compared to average and low responders. These
results paralleled those found during in-vitro heat challenge confirming that
high responders may be more resilient to heat stress compared average and low
responders.",1
317,"Genetic data are now routinely used to study the history of population size,
subdivision, and gene flow. A variety of formal statistical methods is
available for testing hypotheses and fitting models to data. Yet it is often
unclear which hypotheses are worth testing, which models worth fitting. There
is a need for less formal methods that can be used in exploratory analysis of
genetic data. One approach to this problem uses *nucleotide site patterns*,
which provide a simple summary of the pattern in genetic data. This article
shows how to use them in exploratory data analysis.",1
318,"We show that when cells communicate by contact-mediated interactions,
heterogeneity in cell shapes and sizes leads to qualitatively distinct
collective behavior in the tissue. For inter-cellular coupling that implements
lateral inhibition, such disorder-driven transitions can substantially alter
the asymptotic pattern of differentiated cells by modulating their fate choice
through changes in the neighborhood geometry. In addition, when contact-induced
signals influence inherent cellular oscillations, disorder leads to the
emergence of functionally relevant partially-ordered dynamical states.",1
319,"How do species coexist? A framework known as Modern Coexistence Theory
measures mechanisms of coexistence by comparing a species perturbed to low
density (the invader) to other species that remain at their typical densities
(the residents); this invader-resident comparison measures a rare-species
advantage that results from specialization. However, there are several
reasonable ways (i.e., methods) to compare invaders and residents, each
differing in practicality and biological interpretation. Here, using
theoretical arguments and case studies, we compare four such methods for
calculating coexistence mechanisms: 1) Scaling factors, the traditional
approach where resident growth rates are scaled by a measure of relative
sensitivity to competition, obtained by solving a system of linear equations;
2) The simple comparison, which gives equal weight to all resident species; 3)
Speed conversion factors, a novel method in which resident growth rates are
scaled by a ratio of generation times, and; 4) The invader-invader comparison,
another novel method in which a focal species is compared to itself at high vs.
low density. We conclude that the conventional scaling factors can be useful in
some theoretical research, but are not recommended for empirical applications,
i.e., determining the mechanisms of coexistence in real communities. Instead,
we recommend the simple comparison and speed conversion factor methods. The
speed conversion factors are most useful when comparing species with dissimilar
generation times. However, ecologists often study coexistence in guilds of
species with similar life-histories, and therefore, similar generation times.
In such scenarios, the easier-to-use simple comparison method is reasonable.",1
320,"The storage effect is a well-known explanation for the coexistence of
competing species in temporally varying environments. Like many complex
ecological theories, the storage effect is often used as an explanation for
observed coexistence on the basis of heuristic understanding, rather than
careful application of a detailed model. But, a careful examination of some
widely employed heuristic representations of the storage effect shows that they
can be misleading. One interpretation of the storage effect states that species
coexist by specializing on a small slice of all environmental states, and
therefore must have a robust life-stage (e.g., long-lived adults, a seedbank)
in order to ""wait it out"" for a good year. Another more general interpretation
states that ""buffering"" helps species ""wait it out"", where ""buffering"" means
populations are protected from large losses when the environment is poor and
competition is high. Here, we show that both of these conventional
interpretations are imperfect. Multiple models show that stage-structure, long
lifespans, and overlapping generations are not required for the storage effect.
Further, a species that experiences buffering necessarily grows poorly when the
environment is favorable and competition is high. We review the empirical
literature and conclude that species are likely to find themselves in this
good-environment/high-competition scenario, and therefore, that buffering tends
to hurt individual species. The buffering interpretation of the storage effect
can be thought of as conflating a community-level criterion for coexistence
with a population-level criterion for persistence; it is like claiming that
species can persist by limiting their own growth rates, since the
Lotka-Volterra model tells us that intraspecific competition must be greater
than interspecific competition.",1
321,"During the COVID pandemic, periods of exponential growth of the disease have
been mitigated by containment measures that in different occasions have
resulted in a power-law growth of the number of cases. The first observation of
such behaviour has been obtained from 2020 late spring data coming from China
by Ziff and Ziff in Ref. [1]. After this important observation the power-law
scaling (albeit with different exponents) has also been observed in other
countries during periods of containment of the spread. Early interpretations of
these results suggest that this phenomenon might be due to spatial effects of
the spread. Here we show that temporal modulations of infectivity of
individuals due to containment measures can also cause power-law growth of the
number of cases over time. To this end we propose a stochastic well-mixed
Susceptible-Infected-Removed (SIR) model of epidemic spreading in presence of
containment measures resulting in time-dependent infectivity and we explore the
statistical properties of the resulting branching process at criticality. We
show that at criticality it is possible to observe power-law growth of the
number of cases with exponents ranging between one and two. Our asymptotic
analytical results are confirmed by extensive Monte Carlo simulations. Although
these results do not exclude that spatial effects might be important in
modulating the power-law growth of the number of cases at criticality, this
work shows that even well-mixed populations may already feature non-trivial
power-law exponents at criticality.",1
322,"We study methylation dynamics of the chemoreceptors as an {\sl E.coli} cell
moves around in a spatially varying chemo-attractant environment. We consider
attractant concentration with strong and weak spatial gradient. During the
uphill and downhill motion of the cell along the gradient, we measure the
temporal variation of average methylation level of the receptor clusters. Our
numerical simulations show that the methylation dynamics depends sensitively on
the size of the receptor clusters and also on the strength of the gradient. At
short times after the beginning of a run, the methylation dynamics is mainly
controlled by short runs which are generally associated with high receptor
activity. This results in demethylation at short times. But for intermediate or
large times, long runs play an important role and depending on receptor
cooperativity or gradient strength, the qualitative variation of methylation
can be completely different in this time regime. For weak gradient, both for
uphill and downhill runs, after the initial demethylation, we find methylation
level increases steadily with time for all cluster sizes. Similar qualitative
behavior is observed for strong gradient during uphill runs as well. However,
the methylation dynamics for downhill runs in strong gradient show highly
non-trivial dependence on the receptor cluster size. We explain this behavior
as a result of interplay between the sensing and adaptation modules of the
signaling network.",1
323,"In mathematical modeling, several different functional forms can often be
used to fit a data set equally well, especially if the data is sparse. In such
cases, these mathematically different but similar looking functional forms are
typically considered interchangeable. Recent work, however, shows that similar
functional responses may nonetheless result in significantly different
bifurcation points for the Rosenzweig-MacArthur predator-prey system. Since the
bifurcation behaviours include destabilising oscillations, predicting the
occurrence of such behaviours is clearly important. Ecologically, different
bifurcation behaviours mean that different predictions may be obtained from the
models. These predictions can range from stable coexistence to the extinction
of both species, so obtaining more accurate predictions is also clearly
important for conservationists. Mathematically, this difference in bifurcation
structure given similar functional responses is called structural sensitivity.
We extend the existing work to find that the Leslie-Gower-May predator-prey
system is also structurally sensitive to the functional response. Using the
Rosenzweig-MacArthur and Leslie-Gower-May models, we then aim to determine if
there is some way to obtain a functional description of data so that different
functional responses yield the same bifurcation structure, i.e., we aim to
describe data such that our model is not structurally sensitive. We analyze the
functional responses using two different methods to determine which part of
each function contributes most to the observed bifurcation behaviour. We find
that prey densities around the coexistence steady state are most important in
defining the functional response. Lastly, we propose a procedure for ecologists
and mathematical modelers to increase the accuracy of model predictions in
predator-prey systems.",1
324,"Focal cortical dysplasias (FCDs) are the most common cause of treatment
resistant epilepsy affecting the pediatric population. Most individuals with
FCD have seizure onset during the first five years of life and the majority
will have seizures by the age of sixteen. Many cases of FCD are postulated to
be the result of abnormal brain development in utero by germline or somatic
gene mutations regulating neuronal growth and migration during corticogenesis.
Other cases of FCD are thought to be related to infections during brain
development, or even other causes still unable to be fully determined. Typical
anti-seizure medications are oftentimes ineffective in FCD as well as surgery
is unable to be successfully performed due to the involvement of eloquent areas
of the brain or insufficient resection of the epileptogenic focus, posing a
challenge for physicians. The genetic nature of FCD provides an avenue for drug
development with several genetic and molecular targets undergoing study over
the last two decades.",1
325,"Self-supervised protein language models have proved their effectiveness in
learning the proteins representations. With the increasing computational power,
current protein language models pre-trained with millions of diverse sequences
can advance the parameter scale from million-level to billion-level and achieve
remarkable improvement. However, those prevailing approaches rarely consider
incorporating knowledge graphs (KGs), which can provide rich structured
knowledge facts for better protein representations. We argue that informative
biology knowledge in KGs can enhance protein representation with external
knowledge. In this work, we propose OntoProtein, the first general framework
that makes use of structure in GO (Gene Ontology) into protein pre-training
models. We construct a novel large-scale knowledge graph that consists of GO
and its related proteins, and gene annotation texts or protein sequences
describe all nodes in the graph. We propose novel contrastive learning with
knowledge-aware negative sampling to jointly optimize the knowledge graph and
protein embedding during pre-training. Experimental results show that
OntoProtein can surpass state-of-the-art methods with pre-trained protein
language models in TAPE benchmark and yield better performance compared with
baselines in protein-protein interaction and protein function prediction. Code
and datasets are available in https://github.com/zjunlp/OntoProtein.",1
326,"We introduce a novel, probabilistic binary latent variable model to detect
noisy or approximate repeats of patterns in sparse binary data. The model is
based on the ""Noisy-OR model"" (Heckerman, 1990), used previously for disease
and topic modelling. The model's capability is demonstrated by extracting
structure in recordings from retinal neurons, but it can be widely applied to
discover and model latent structure in noisy binary data. In the context of
spiking neural data, the task is to ""explain"" spikes of individual neurons in
terms of groups of neurons, ""Cell Assemblies"" (CAs), that often fire together,
due to mutual interactions or other causes. The model infers sparse activity in
a set of binary latent variables, each describing the activity of a cell
assembly. When the latent variable of a cell assembly is active, it reduces the
probabilities of neurons belonging to this assembly to be inactive. The
conditional probability kernels of the latent components are learned from the
data in an expectation maximization scheme, involving inference of latent
states and parameter adjustments to the model. We thoroughly validate the model
on synthesized spike trains constructed to statistically resemble recorded
retinal responses to white noise stimulus and natural movie stimulus in data.
We also apply our model to spiking responses recorded in retinal ganglion cells
(RGCs) during stimulation with a movie and discuss the found structure.",1
327,"Previous simulation studies on human connectomes suggested, that critical
dynamics emerge subcrititcally in the so called Griffiths Phases. %This is the
consequence of the strong heterogeneity of the graphs. Now we investigate this
on the largest available brain network, the $21.662$ node fruit-fly connectome,
using the Kuramoto synchronization model. As this graph is less heterogeneous,
lacking modular structure and exhibit high topological dimension, we expect a
difference from the previous results. Indeed, the synchronization transition is
mean-field like, and the width of the transition region is larger than in
random graphs, but much smaller than as for the KKI-18 human connectome. This
demonstrates the effect of modular structure and dimension on the dynamics,
providing a basis for better understanding the complex critical dynamics of
humans.",1
328,"Reproduction of pre-clinical results has a high failure rate. The fundamental
methodology including replication (""protocol"") for hypothesis
testing/validation to a state allowing inference, varies within medical and
plant sciences with little justification. Here, five protocols are
distinguished which deal differently with systematic/random errors and vary
considerably in result veracity. Aim: to compare prevalence of protocols
(defined in text). Medical/plant science articles from 2017/2019 were surveyed:
713 random articles assessed for eligibility for counts: first (with p-values):
1) non-replicated; 2) global; 3) triple-result protocols; second: 4)
replication-error protocol; 5) meta-analyses. Inclusion criteria:
human/plant/fungal studies with categorical groups. Exclusion criteria: phased
clinical trials, pilot studies, cases, reviews, technology, rare subjects,
-omic studies. Abbreviated PICOS question: which protocol was evident for a
main result with categorically distinct group difference(s) ? Electronic
sources: Journal Citation Reports 2017/2019, Google. Triplication prevalence
differed dramatically between sciences (both years p<10-16; cluster-adjusted
chi-squared tests): From 320 studies (80/science/year): in 2017, 53 (66%, 95%
confidence interval (C.I.) 56%:77%) and in 2019, 48 (60%, C.I. 49%:71%) plant
studies had triple-result or triplicated global protocols, compared with, in
both years, 4 (5%, C.I. 0.19%:9.8%) medical studies. Plant sciences had a
higher prevalence of protocols more likely to counter generalised systematic
errors (the most likely cause of false positives) and random error than
non-replicated protocols, without suffering from serious flaws found with
random-Institutes protocols. It is suggested that a triple-result
(organised-reproduction) protocol, with Institute consortia, is likely to solve
most problems connected with the replicability crisis.",1
329,"The Sackin and Colless indices are two widely-used metrics for measuring the
balance of trees and for testing evolutionary models in phylogenetics. This
short paper provides an asymptotic analysis of the expected Sackin and Colless
indices on tree shapes (which are full binary unlabelled trees) under the
uniform model where tree shapes are sampled with equal probability. It also
presents a short elementary proof of the closed formula for the expected Sackin
index of phylogenetic trees (which are full binary tree with leaves being
labelled) under the uniform model. The new derivation does not even use the
Catalan numbers.",1
330,"Minimum flow decomposition (MFD) (the problem of finding a minimum set of
paths that perfectly decomposes a flow) is a classical problem in Computer
Science, and variants of it are powerful models in multiassembly problems in
Bioinformatics (e.g. RNA assembly). However, because this problem and its
variants are NP-hard, practical multiassembly tools either use heuristics or
solve simpler, polynomial-time solvable versions of the problem, which may
yield solutions that are not mini-mal or do not perfectly decompose the flow.
Many RNA assemblers also use integer linear programming(ILP) formulations of
such practical variants, having the major limitation they need to encode all
the potentially exponentially many solution paths. Moreover, the only exact
solver for MFD does not scale to large instances and cannot be efficiently
generalized to practical MFD variants. In this work, we provide the first
practical ILP formulation for MFD (and thus the first fast and exact solver for
MFD), based on encoding all of the exponentially many solution paths using only
a quadratic number of variables. On both simulated and real flow graphs, our
approach solves any instance in under 13 seconds. We also show that our ILP
formulation can be easily and efficiently adapted for many practical variants,
such as incorporating longer or paired-end reads or minimizing flow errors. We
hope that our results can remove the current tradeoff between the complexity of
a multi assembly model and its tractability and can lie at the core of future
practical RNA assembly tools.",1
331,"Phase separation has emerged as an essential concept for the spatial
organization inside biological cells. However, despite the clear relevance to
virtually all physiological functions, we understand surprisingly little about
what phases form in a system of many interacting components, like in cells.
Here, we introduce a new numerical method based on physical relaxation dynamics
to study the coexisting phases in such systems. We use our approach to optimize
interactions between components, similar to how evolution might have optimized
the interactions of proteins. These evolved interactions robustly lead to a
defined number of phases, despite substantial uncertainties in the initial
composition, while random or designed interactions perform much worse.
Moreover, the optimized interactions are robust to perturbations and they allow
fast adaption to new target phase counts. We thus show that genetically encoded
interactions of proteins provide versatile control of phase behavior. The
phases forming in our system are also a concrete example of a robust emergent
property that does not rely on fine-tuning the parameters of individual
constituents.",1
332,"Natural decomposition of organic matter is important in food systems, and
compost is worldwidely used as an organic fermented fertilizer, but, as a
feature of ecosystem, its effects on the animals are poorly understood. Here we
show that oral administration of compost itself and/or its included
thermophilic Bacillaceae, i.e. Caldibacillus hisashii and Weizmannia coagulans,
can regulate the prophylactic activities of various industrial animals. In
these regulatory processes, biological defense responses and/or efficient
utilization of fecal metabolites were in consistent with the alterations in
animal species-specific fecal inhabitant microbiota with reduced pathogenic
and/or non-pathogenic damages dependent upon environmental conditions.
Furthermore, association analyses and structural equation modeling predicted
gut inhabitant beneficial bacteria and metabolites that interacted with the
compost-derived thermophiles, as common key components beyond the animal
species. These results highlight the potential of universal probiotics based on
the microbial structure of compost for sustainable protective control in
agriculture, fishery and livestock industries.",1
333,"Despite the introduction of vaccines, Coronavirus disease (COVID-19) remains
a worldwide dilemma, continuously developing new variants such as Delta and the
recent Omicron. The current standard for testing is through polymerase chain
reaction (PCR). However, PCRs can be expensive, slow, and/or inaccessible to
many people. X-rays on the other hand have been readily used since the early
20th century and are relatively cheaper, quicker to obtain, and typically
covered by health insurance. With a careful selection of model,
hyperparameters, and augmentations, we show that it is possible to develop
models with 83% accuracy in binary classification and 64% in multi-class for
detecting COVID-19 infections from chest x-rays.",1
334,"The insideness problem is an aspect of image segmentation that consists of
determining which pixels are inside and outside a region. Deep Neural Networks
(DNNs) excel in segmentation benchmarks, but it is unclear if they have the
ability to solve the insideness problem as it requires evaluating long-range
spatial dependencies. In this paper, the insideness problem is analysed in
isolation, without texture or semantic cues, such that other aspects of
segmentation do not interfere in the analysis. We demonstrate that DNNs for
segmentation with few units have sufficient complexity to solve insideness for
any curve. Yet, such DNNs have severe problems with learning general solutions.
Only recurrent networks trained with small images learn solutions that
generalize well to almost any curve. Recurrent networks can decompose the
evaluation of long-range dependencies into a sequence of local operations, and
learning with small images alleviates the common difficulties of training
recurrent networks with a large number of unrolling steps.",1
335,"Microscale oxygenation plays a prominent role in tumour progression.
Spatiotemporal variability of oxygen distribution in the tumour
microenvironment contributes to cellular heterogeneity and to the emergence of
normoxic and hypoxic populations. Local levels of oxygen strongly affect the
response of tumours to the administration of different therapeutic modalities
and, more generally, to the phenomenon of resistance to treatments. Several
interventions have been proposed to improve tumour oxygenation, being the
elevation of the local temperature (hyperthermia) an important one. While other
factors such as the metabolic activity have to be considered, the proficiency
of the tumour vascular system is a key factor both for the tissue oxygenation
and for its temperature maps. Consequently, the interplay of these factors has
attracted considerable attention from the mathematical modelling perspective.
Here we put forward a transport-based system of partial differential equations
aimed at describing the dynamics of healthy and tumour cell subpopulations at
the microscale in a region placed between two blood vessels. By using this
model with diverse flow conditions, we analyse the oxygen and temperature
profiles that arise in different scenarios of vascular status, both during free
progression and under thermal therapy. We find that low oxygen levels are
associated to elevations of temperature in locations preferentially populated
by hypoxic cells, and hyperthermia-induced cell death, being strongly dependent
on blood flow, would only appear under highly disrupted conditions of the local
vasculature. This results in a noticeable effect of heat on hypoxic cells.
Additionally, when pronounced cell death occurs, it is followed by a
significant increase in the oxygen levels.",1
336,"In mental health research, it has proven difficult to find measures of brain
function that provide reliable indicators of mental health and well-being,
including susceptibility to mental health disorders. Recently, a family of
data-driven analyses have provided such reliable measures when applied to
large, population-level datasets. In the current pre-registered replication
study, we show that the canonical correlation analysis (CCA) methods previously
developed using resting-state MRI functional connectivity and subject measures
of cognition and behavior from healthy adults are also effective in measuring
well-being (a ""positive-negative axis"") in an independent developmental
dataset. Our replication was successful in two out of three of our
pre-registered criteria, such that a primary CCA mode's weights displayed a
significant positive relationship and explained a significant amount of
variance in both functional connectivity and subject measures. The only
criteria that was not successful was that compared to other modes the magnitude
of variance explained by the primary CCA mode was smaller than predicted, a
result which could indicate a developmental trajectory of a primary mode. This
replication establishes a signature neurotypical relationship between
connectivity and phenotype, opening new avenues of research in neuroscience
with clear clinical applications.",1
337,"In recent years, the monarch butterfly's iconic migration patterns have come
under threat from a number of factors, from climate change to pesticide use. To
track trends in their populations, scientists as well as citizen scientists
must identify individuals accurately. This is uniquely key for the study of
monarch butterflies because there exist other species of butterfly, such as
viceroy butterflies, that are ""look-alikes"" (coined by the Convention on
International Trade in Endangered Species of Wild Fauna and Flora), having
similar phenotypes. To tackle this problem and to aid in more efficient
identification, we present MonarchNet, the first comprehensive dataset
consisting of butterfly imagery for monarchs and five look-alike species. We
train a baseline deep-learning classification model to serve as a tool for
differentiating monarch butterflies and its various look-alikes. We seek to
contribute to the study of biodiversity and butterfly ecology by providing a
novel method for computational classification of these particular butterfly
species. The ultimate aim is to help scientists track monarch butterfly
population and migration trends in the most precise and efficient manner
possible.",1
338,"Decomposing a network flow into weighted paths has numerous applications.
Some applications require any decomposition that is optimal w.r.t. some
property such as number of paths, robustness, or length. Many bioinformatic
applications require a specific decomposition where the paths correspond to
some underlying data that generated the flow. For real inputs, no optimization
criteria guarantees to uniquely identify the correct decomposition. Therefore,
we propose to report safe paths, i.e., subpaths of at least one path in every
flow decomposition.
  Ma, Zheng, and Kingsford [WABI 2020] addressed the existence of multiple
optimal solutions in a probabilistic framework, i.e., non-identifiability.
Later [RECOMB 2021], they gave a quadratic-time algorithm based on a global
criterion for solving a problem called AND-Quant, which generalizes the problem
of reporting whether a given path is safe.
  We give the first local characterization of safe paths for flow
decompositions in directed acyclic graphs (DAGs), leading to a practical
algorithm for finding the complete set of safe paths. We evaluated our
algorithms against the trivial safe algorithms (unitigs, extended unitigs) and
the popularly used heuristic (greedy-width) for flow decomposition on RNA
transcripts datasets. Despite maintaining perfect precision our algorithm
reports significantly higher coverage ($\approx 50\%$ more) than trivial safe
algorithms. The greedy-width algorithm though reporting a better coverage, has
significantly lower precision on complex graphs. Overall, our algorithm
outperforms (by $\approx 20\%$) greedy-width on a unified metric (F-Score) when
the dataset has significant number of complex graphs. Moreover, it has superior
time ($3-5\times$) and space efficiency ($1.2-2.2\times$), resulting in a
better and more practical approach for bioinformatics applications of flow
decomposition.",1
339,"A hallmark of living systems is the ability to employ a common set of
versatile building blocks that can self-organize into a multitude of different
structures, in a way that can be controlled with minimal cost. This capability
can only be afforded in non-equilibrium conditions, as evident from the
energy-consuming nature of the plethora of such dynamical processes. In the
last three decades, synthetic self-assembly has experienced a significant boost
with the development of tools to design specific interactions at different
scales, from nucleic acids and peptides to proteins and colloids. To achieve
automated dynamical control of such self-assembled structures and transitions
between them, we need to identify the relevant fundamental aspects of
non-equilibrium dynamics that can enable such processes. Here, we identify
programmable non-reciprocal interactions as a potential paradigm using which
such functionalities can be achieved. In particular, we propose a model that
enables a system to learn and retrieve predetermined desired structures and
transition between them, thus behaving as a shape-shifter. The learning rule is
composed of reciprocal interactions that lead to the equilibrium assembly of
the structures, and non-reciprocal interactions that give rise to
non-equilibrium dynamical transitions between the structures.",1
340,"Accelerometers are widely used to measure physical activity behaviour,
including in children. The traditional method for processing acceleration data
uses cut points to define physical activity intensity, relying on calibration
studies that relate the magnitude of acceleration to energy expenditure.
However, these relationships do not generalise across diverse populations and
hence they must be parametrised for each subpopulation (e.g., age groups) which
is costly and makes studies across diverse populations and over time difficult.
A data driven approach that allows physical activity intensity states to emerge
from the data, without relying on parameters derived from external populations,
and offers a new perspective on this problem and potentially improved results.
We applied an unsupervised machine learning approach, namely a hidden semi
Markov model, to segment and cluster the accelerometer data recorded from 279
children (9 to 38 months old) with a diverse range of physical and
social-cognitive abilities (measured using the Paediatric Evaluation of
Disability Inventory). We benchmarked this analysis with the cut points
approach calculated using the best available thresholds for the population.
Time spent active as measured by this unsupervised approach correlated more
strongly with measures of the childs mobility, social-cognitive capacity,
independence, daily activity, and age than that measured using the cut points
approach. Unsupervised machine learning offers the potential to provide a more
sensitive, appropriate, and cost-effective approach to quantifying physical
activity behaviour in diverse populations, compared to the current cut points
approach. This, in turn, supports research that is more inclusive of diverse or
rapidly changing populations.",1
341,"CANVAS caused by RFC1 biallelic expansions is a major cause of inherited
sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can
be associated with atypical features. We clinically and genetically
characterized 50 patients, selected based on the presence of sensory
neuronopathy confirmed by EMG. We screened RFC1 expansion by PCR, repeat-primed
PCR, and Southern blotting of long-range PCR products, a newly developed
method. Neuropathological characterization was performed on the brain and
spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n
expansion in RFC1. In addition to the core CANVAS phenotype (sensory
neuronopathy, cerebellar syndrome, and vestibular impairment), we observed
chronic cough (97%), oculomotor signs (85%), motor neuron involvement (55%),
dysautonomia (50%), and parkinsonism (10%). Motor neuron involvement was found
for 24 of 38 patients (63.1%). First motor neuron signs, such as brisk
reflexes, extensor plantar responses, and/or spasticity, were present in 29% of
patients, second motor neuron signs, such as fasciculations, wasting, weakness,
or a neurogenic pattern on EMG in 18%, and both in 16%. Mixed motor and sensory
neuronopathy was observed in 19% of patients. Among six non-RFC1 patients, one
carried a heterozygous AAGGG expansion and a pathogenic variant in GRM1.
Neuropathological examination of one RFC1 patient with an enriched phenotype,
including parkinsonism, dysautonomia, and cognitive decline, showed posterior
column and lumbar posterior root atrophy. Degeneration of the vestibulospinal
and spinocerebellar tracts was mild. We observed marked astrocytic gliosis and
axonal swelling of the synapse between first and second motor neurons in the
anterior horn at the lumbar level. The cerebellum showed mild depletion of
Purkinje cells, with empty baskets, torpedoes, and astrogliosis characterized
by a disorganization of the Bergmann's radial glia. We found neuronal loss in
the vagal nucleus. The pars compacta of the substantia nigra was depleted, with
widespread Lewy bodies in the locus coeruleus, substantia nigra, hippocampus,
entorhinal cortex, and amygdala. We propose new guidelines for the screening of
RFC1 expansion, considering different expansion motifs. Here, we developed a
new method to more easily detect pathogenic RFC1 expansions. We report frequent
motor neuron involvement and different neuronopathy subtypes. Parkinsonism was
more prevalent in this cohort than in the general population, 10% versus the
expected 1% (p < .001). We describe, for the first time, the spinal cord
pathology in CANVAS, showing the alteration of posterior columns and roots,
astrocytic gliosis and axonal swelling, suggesting motor neuron synaptic
dysfunction.",1
342,"Parcellations are fundamental tools in neuroanatomy, allowing researchers to
place functional imaging and molecular data within a structural context in the
brain. Visualizing these parcellations is critical to guide biological
understanding of clinical and experimental datasets in humans and model
organisms. However, software used to visualize parcellations is different from
the one used to analyze these datasets, greatly limiting the visualization of
experimental data within parcellations. We present coldcuts, an open source R
package that allows to automatically generate, store and visualize any
volume-based parcellation easily and with minimal manual curation. coldcuts
allows to integrate external datasets and offers rich 2D and 3D visualizations.
coldcuts is freely available at http://github.com/langleylab/coldcuts and
several curated coldcuts objects are made available for human, mouse,
chimpanzee and Drosophila parcellations at
https://github.com/langleylab/coldcuts_segmentations.",1
343,"Neonatal respiratory distress is a common condition that if left untreated,
can lead to short- and long-term complications. This paper investigates the
usage of digital stethoscope recorded chest sounds taken within 1min
post-delivery, to enable early detection and prediction of neonatal respiratory
distress. Fifty-one term newborns were included in this study, 9 of whom
developed respiratory distress. For each newborn, 1min anterior and posterior
recordings were taken. These recordings were pre-processed to remove noisy
segments and obtain high-quality heart and lung sounds. The random
undersampling boosting (RUSBoost) classifier was then trained on a variety of
features, such as power and vital sign features extracted from the heart and
lung sounds. The RUSBoost algorithm produced specificity, sensitivity, and
accuracy results of 85.0%, 66.7% and 81.8%, respectively.",1
344,"Deep learning has contributed to major advances in the prediction of protein
structure from sequence, a fundamental problem in structural bioinformatics.
With predictions now approaching the accuracy of crystallographic resolution in
some cases, and with accelerators like GPUs and TPUs making inference using
large models rapid, fast genome-level structure prediction becomes an obvious
aim. Leadership-class computing resources can be used to perform genome-scale
protein structure prediction using state-of-the-art deep learning models,
providing a wealth of new data for systems biology applications. Here we
describe our efforts to efficiently deploy the AlphaFold2 program, for
full-proteome structure prediction, at scale on the Oak Ridge Leadership
Computing Facility's resources, including the Summit supercomputer. We
performed inference to produce the predicted structures for 35,634 protein
sequences, corresponding to three prokaryotic proteomes and one plant proteome,
using under 4,000 total Summit node hours, equivalent to using the majority of
the supercomputer for one hour. We also designed an optimized structure
refinement that reduced the time for the relaxation stage of the AlphaFold
pipeline by over 10X for longer sequences. We demonstrate the types of analyses
that can be performed on proteome-scale collections of sequences, including a
search for novel quaternary structures and implications for functional
annotation.",1
345,"A theoretical study of cell evolution is presented here. By using a toolbox
containing an intracellular catalytic reaction network model and a
mutation-selection process, four distinct phases of self-organization were
unveiled. First, the nutrients prevail as the central substrate of the chemical
reactions. Second, the cell becomes a small-world. Third, a highly connected
core component emerges, concurrently with the nutrient carriers becoming the
central product of reactions. Finally, the cell reaches a steady configuration
where the concentrations of the core chemical species are described by Zipf's
law.",1
346,"Effective biological utilization of wood biomass is necessary worldwide.
Since several insect larvae can use wood biomass as a nutrient source, studies
on their digestive mechanism are expected to speculate a novel rule in wood
biomass processing. Here, the relationships of inhabitant bacteria involved in
carbon and nitrogen metabolism in the intestine of beetle larvae, an insect
model, are investigated. Bacterial analysis of larval feces showed enrichment
of members of which could include candidates for plant growth promotion,
nitrogen cycle modulation, and/or environmental protection. The abundances of
these bacteria were not necessarily positively correlated with the abundance in
the habitat, suggesting that they might be selectively enriched in the
intestines of larvae. Further association analysis predicted that carbon and
nitrogen metabolism in the intestine was affected by the presence of the other
common bacteria, the populations of which were not remarkably altered in the
habitat and feces. Based on hypotheses targeting these selected bacterial
groups, structural estimation modeling analyses statistically suggested that
their metabolism of carbon and nitrogen and their stable isotopes, {\delta}13C
and {\delta}15N, may be associated with fecal enriched bacteria and other
common bacteria. In addition, other causal inference analyses, such as causal
mediation analysis, linear non-Gaussian acyclic model (LiNGAM), and
BayesLiNGAM, did not necessarily affirm the existence of prominent bacteria
involved in metabolism, implying its importance as the bacterial groups for
metabolism rather than a remarkable bacterium. Thus, these observations
highlight a multifaceted view of symbiotic bacterial groups utilizing carbon
and nitrogen from wood biomass in insect larvae as a cultivator of potentially
environmentally beneficial bacteria.",1
347,"Performing full-resolution atomistic simulations of nucleic acid folding has
remained a challenge for biomolecular modeling. Understanding how nucleic acids
fold and how they transition between different folded structures as they unfold
and refold has important implications for biology. This paper reports a
theoretical model and computer simulation of the ab initio folding of DNA
inverted repeat sequences. The formulation is based on an all-atom
conformational model of the sugar-phosphate backbone via chain closure, and it
incorporates three major molecular-level driving forces - base stacking,
counterion-induced backbone self-interactions and base pairing - via separate
analytical theories designed to capture and reproduce the effects of the
solvent without requiring explicit water and ions in the simulation. To
accelerate computational throughput, a mixed numerical/analytical algorithm for
the calculation of the backbone conformational volume is incorporated into the
Monte Carlo simulation, and special stochastic sampling techniques were
employed to achieve the computational efficiency needed to fold nucleic acids
from scratch. This paper describes implementation details, benchmark results
and the advantages and technical challenges with this approach.",1
348,"Physical mechanisms of phase separation in living systems can play key
physiological roles and have recently been the focus of intensive studies. The
strongly heterogeneous and disordered nature of such phenomena in the
biological domain poses difficult modeling challenges that require going beyond
mean field approaches based on postulating a free energy landscape. The
alternative pathway we take in this work is to tackle the full statistical
mechanics problem of calculating the partition function in these systems,
starting from microscopic interactions, by means of cavity methods. We
illustrate the procedure first on the simple binary case, and we then apply it
successfully to ternary systems, in which the naive mean field approximations
are proved inadequate. We then demonstrate the agreement with lattice model
simulations, to finally contrast our theory also with experiments of coacervate
formation by associative de-mixing of nucleotides and poly-lysine in aqueous
solution. In this way, different types of evidence are provided to support
cavity methods as ideal tools for quantitative modeling of biomolecular
condensation, giving an optimal balance between the accurate consideration of
spatial aspects of the microscopic dynamics and the fast computational results
rooted in their analytical tractability.",1
349,"In focal epilepsy, various seizure features, such as spread and duration, can
change from one seizure to the next within the same patient. Importantly,
within-patient seizure evolutions do not change randomly over time, but instead
appear to fluctuate over circadian and slower timescales. However, the specific
timescales of this variability, as well as the specific seizure characteristics
that change over time, are unclear.
  Here we analysed this within-patient seizure variability in 10 patients with
chronic intracranial EEG recordings (185-767 days of recording time, 57-452
analysed seizures/patient). We characterised the seizure evolutions as
sequences of a finite number of functional network states. We then compared
seizure state occurrence and seizure state duration to (1) time since
implantation and (2) patient-specific circadian and multidien cycles in
interictal spike rate, which were extracted using empirical mode decomposition.
  In most patients, the occurrence or duration of at least one state was
associated with the time since implantation (8 and 9 patients for state
occurrence and state duration, respectively). Additionally, some patients also
had one or more states that were associated with phases of circadian and/or
multidien spike rate cycles (4 and 7 patients for state occurrence and state
duration, respectively). A given state's occurrence and duration were not
usually associated with the same timescale.
  Our results suggest that time-varying factors modulate within-patient seizure
evolutions over multiple timescales, with separate processes modulating a
seizure state's occurrence and duration. These findings provide new insight
into the patterns and mechanisms of intra-patient seizure variability, with
potential implications for forecasting and treating seizures.",1
350,"The storage effect is a general explanation for coexistence in a variable
environment. The generality of the storage effect is both a strength - it can
be quantified in many systems - and a challenge - there is not a clear
relationship between the abstract conditions for storage effect and species'
life-history traits (e.g., dormancy, stage-structure, non-overlapping
generations), thus precluding a simple ecological interpretation of the storage
effect. Our goal here is to provide a clearer understanding of the conditions
for the storage effect as a step towards a better general explanation for
coexistence in a variable environment. Our approach focuses on dividing one of
the key conditions for the storage effect, covariance between environment and
competition, into two pieces, namely that there must be a causal relationship
between environment and competition, and that the effects of the environment do
not change too quickly. This finer-grained definition can explain a number of
previous results, including 1) that the storage effect promotes annual plant
coexistence when the germination rate fluctuates, but not when the seed yield
fluctuates, 2) that the storage effect is more likely to be induced by resource
competition than apparent competition, and 3) that the spatial storage effect
is more probable than the temporal storage effect. Additionally, our expanded
definition suggests two novel mechanisms by which the temporal storage effect
can arise: transgenerational plasticity, and causal chains of environmental
variables. These mechanisms produce coexistence via the storage effect without
any need for stage structure or a temporally autocorrelated environment.",1
351,"Antibodies are canonically Y-shaped multimeric proteins capable of highly
specific molecular recognition. The CDRH3 region located at the tip of variable
chains of an antibody dominates antigen-binding specificity. Therefore, it is a
priority to design optimal antigen-specific CDRH3 regions to develop
therapeutic antibodies to combat harmful pathogens. However, the combinatorial
nature of CDRH3 sequence space makes it impossible to search for an optimal
binding sequence exhaustively and efficiently, especially not experimentally.
Here, we present AntBO: a Combinatorial Bayesian Optimisation framework
enabling efficient in silico design of the CDRH3 region. Ideally, antibodies
should bind to their target antigen and be free from any harmful outcomes.
Therefore, we introduce the CDRH3 trust region that restricts the search to
sequences with feasible developability scores. To benchmark AntBO, we use the
Absolut! software suite as a black-box oracle because it can score the target
specificity and affinity of designed antibodies in silico in an unconstrained
fashion. The results across 188 antigens demonstrate the benefit of AntBO in
designing CDRH3 regions with diverse biophysical properties. In under 200
protein designs, AntBO can suggest antibody sequences that outperform the best
binding sequence drawn from 6.9 million experimentally obtained CDRH3s and a
commonly used genetic algorithm baseline. Additionally, AntBO finds very-high
affinity CDRH3 sequences in only 38 protein designs whilst requiring no domain
knowledge. We conclude AntBO brings automated antibody design methods closer to
what is practically viable for in vitro experimentation.",1
352,"Patterns of interregional brain connectivity characterize the function of the
human nervous system in both health and disease. Such connectivity can be
inferred non-invasively by analyzing the blood-oxygen-level-dependent signal
from functional magnetic resonance imaging. However, approaches to quantify
this connectivity must solve an under-constrained problem with several
potential solutions due to noise and spatial resolution limitations during
imaging. Previous research has insufficiently evaluated the reproducibility of
the traditional approaches and these approaches have been insufficiently
regularized to increase solution stability. We propose three new measures of
correlative and causal connectivity which incorporate explicit regularization
through a structural connectivity prior from diffusion MRI. These new measures,
which exploit a machine learning formulation for efficient computation, are
evaluated against traditional measures for reproducibility and predictive
power. In particular, correlation, partial correlation, spectral density, and a
novel measure of functional connectivity based on machine learning feature
importance (MLFC) are compared. Additionally, measures of effective
connectivity are compared to two proposed connectivity measures: a machine
learning approach and a Granger-causal approach incorporating a low-dimensional
projection with a diffusion MRI-derived tractography prior. In addition to
reproducibility, we quantify the capacity each measures has to infer three
study participant traits: a physiologic trait, a cognitive trait, and a
combined physiologic and cognitive trait. The proposed measures prove to be
both reproducible and predictive, demonstrating the strong potential for the
proposed connectivity measures to be broadly used in future neuroimaging
studies.",1
353,"The innocuousness property of a controller is that property that makes the
closed-loop system stable regardless the values of the controller parameters.
In other words, the closed-loop system exhibits some structural stability
property with respect to the parameters of the controller. The innocuousness
property was first emphasized in [Briat, Gupta, and Khammash, Cell Systems,
2016] where it was shown that for stochastic unimolecular networks, the
Antithetic Integral Controller (AIC) is innocuous under very mild conditions on
the controlled network; namely the ergodicity and the output-controllability of
the open-loop network, and the admissibility of the set-point value. We show
here that the class of p-type AIC controllers arising in the use of Antithetic
Integral Rein Controllers (AIRC) also exhibit such a property. It is shown in
the unimolecular reaction network case that the closed-loop network is
structurally stable with respect to the controller parameters provided that the
open-loop dynamics is stable and that the set-point is admissible. Those
results are then extended to the case of so-called output unstable linear
systems and to stable nonlinear networks. Some examples are given for
illustration.",1
354,"Detailed dynamical systems models used in life sciences may include dozens or
even hundreds of state variables. Models of large dimension are not only harder
from the numerical perspective (e.g., for parameter estimation or simulation),
but it is also becoming challenging to derive mechanistic insights from such
models. Exact model reduction is a way to address this issue by finding a
self-consistent lower-dimensional projection of the corresponding dynamical
system. A recent algorithm CLUE allows one to construct an exact linear
reduction of the smallest possible dimension such that the fixed variables of
interest are preserved. However, CLUE is restricted to systems with polynomial
dynamics. Since rational dynamics occurs frequently in the life sciences (e.g.,
Michaelis-Menten or Hill kinetics), it is desirable to extend CLUE to the
models with rational dynamics.
  In this paper, we present an extension of CLUE to the case of rational
dynamics and demonstrate its applicability on examples from literature. Our
implementation is available in version 1.5 of CLUE at
https://github.com/pogudingleb/CLUE.",1
355,"Optical neuroimaging has become a well-established clinical and research tool
to monitor cortical activations in the human brain. It is notable that outcomes
of fNIRs studies depend heavily on the data processing pipeline and
classification model employed. Recently, Deep Learning (DL) methodologies have
demonstrated fast and accurate performances in data processing and
classification tasks across many biomedical fields. Herein, we review the
emerging DL applications in fNIRS studies. We first introduce some of the
commonly used DL techniques. Then the review summarizes current DL work in some
of the most active areas of this field, including brain-computer interface,
neuroimpairment diagnosis, and neuroscience discovery. Of the 51 papers
considered in this review, deep learning techniques have been shown to
outperform traditional machine learning techniques in classification accuracy
and can be used to reduce the amount of preprocessing typically done with fNIRS
data.",1
356,"A protein performs biological functions by folding to a particular 3D
structure. To accurately model the protein structures, both the overall
geometric topology and local fine-grained relations between amino acids (e.g.
side-chain torsion angles and inter-amino-acid orientations) should be
carefully considered. In this work, we propose the Directed Weight Neural
Network for better capturing geometric relations among different amino acids.
Extending a single weight from a scalar to a 3D directed vector, our new
framework supports a rich set of geometric operations on both classical and
SO(3)--representation features, on top of which we construct a perceptron unit
for processing amino-acid information. In addition, we introduce an equivariant
message passing paradigm on proteins for plugging the directed weight
perceptrons into existing Graph Neural Networks, showing superior versatility
in maintaining SO(3)-equivariance at the global scale. Experiments show that
our network has remarkably better expressiveness in representing geometric
relations in comparison to classical neural networks and the (globally)
equivariant networks. It also achieves state-of-the-art performance on various
computational biology applications related to protein 3D structures.",1
357,"A family of results, referred to as inheritance results, tell us which
enlargements of a chemical reaction network (CRN) preserve its capacity for
nontrivial behaviours such as multistationarity and oscillation. In this paper,
the following inheritance result is proved: under mild assumptions, splitting
chemical reactions and inserting complexes involving some new chemical species
preserves the capacity of a mass action CRN for multiple nondegenerate
equilibria and/or periodic orbits. The claim has been proved previously for
equilibria alone, using different techniques. Several inheritance results for
multistationarity and oscillation in mass action CRNs, including the main
result of this paper, are gathered into a single theorem. Examples are
presented showing how these results can be used together to make claims about
reaction networks based on knowledge of their subnetworks.",1
358,"Gene transcription is a stochastic process that involves thousands of
reactions. The first set of these reactions, which happen near a gene promoter,
are considered to be the most important in the context of stochastic noise. The
most common models of transcription are primarily concerned with the effect of
activators/repressors on the overall transcription rate and approximate the
basal transcription processes as a one step event. According to such effective
models, the Fano factor of mRNA copy distributions is always greater than
(super-Poissonian) or equal to 1 (Poissonian), and the only way to go below
this limit (sub-Poissonian) is via a negative feedback. It is partly due to
this limit that the first stage of transcription is held responsible for most
of the stochastic noise in mRNA copy numbers. However, by considering all major
reactions that build and drive the basal transcription machinery, from the
first protein that binds a promoter to the entrance of the transcription
complex (TC) into productive elongation, it is shown that the first two stages
of transcription, namely the pre-initiation complex (PIC) formation and the
promoter proximal pausing (PPP), is a highly punctual process. In other words,
the time between the first and the last step of this process is narrowly
distributed, which gives rise to sub-Poissonian distributions for the number of
TCs that have entered productive elongation. In fact, having simulated the PIC
formation and the PPP via the Gillespie algorithm using 2000 distinct parameter
sets and 4 different reaction network topologies, it is shown that only 4.4%
give rise to a Fano factor that is > 1 with the upper bound of 1.7, while for
31% of cases the Fano factor is below 0.5, with 0.19 as the lower bound. These
results cast doubt on the notion that most of the stochastic noise observed in
mRNA distributions always originates at the promoter.",1
359,"It is well-known that the Hessian matters to optimization, generalization,
and even robustness of deep learning. Recent works empirically discovered that
the Hessian spectrum in deep learning has a two-component structure that
consists of a small number of large eigenvalues and a large number of
nearly-zero eigenvalues. However, the theoretical mechanism behind the Hessian
spectrum is still absent or under-explored. We are the first to theoretically
and empirically demonstrate that the Hessian spectrums of well-trained deep
neural networks exhibit simple power-law distributions. Our work further
reveals how the power-law spectrum essentially matters to deep learning: (1) it
leads to low-dimensional and robust learning space, and (2) it implicitly
penalizes the variational free energy, which results in low-complexity
solutions. We further used the power-law spectral framework as a powerful tool
to demonstrate multiple novel behaviors of deep learning. Interestingly, the
power-law spectrum is also known to be important in protein, which indicates a
novel bridge between deep learning and protein science.",1
360,"With the recent advance of deep learning, neural networks have been
extensively used for the task of molecular generation. Many deep generators
extract atomic relations from molecular graphs and ignore hierarchical
information at both atom and molecule levels. In order to extract such
hierarchical information, we propose a novel hyperbolic generative model. Our
model contains three parts: first, a fully hyperbolic junction-tree
encoder-decoder that embeds the hierarchical information of the molecules in
the latent hyperbolic space; second, a latent generative adversarial network
for generating the latent embeddings; third, a molecular generator that
inherits the decoders from the first part and the latent generator from the
second part. We evaluate our model on the ZINC dataset using the MOSES
benchmarking platform and achieve competitive results, especially in metrics
about structural similarity.",1
361,"Motivation: The branching processes model yields unevenly stochastically
distributed data that consists of sparse and dense regions. The work tries to
solve the problem of a precise evaluation of a parameter for this type of
model. The application of the branching processes model to cancer cell
evolution has many difficulties like high dimensionality and the rare
appearance of a result of interest. Moreover, we would like to solve the
ambitious task of obtaining the coefficients of the model reflecting the
relationship of driver genes mutations and cancer hallmarks on the basis of
personal data of variant allele frequencies. Results: The Approximate Bayesian
computation method based on the Isolation kernel is designed. The method
includes a transformation row data to a Hilbert space (mapping) and measures
the similarity between simulation points and maxima weighted Isolation kernel
mapping related to the observation point. Also, we designed a heuristic
algorithm to find parameter estimation without gradient calculation and
dimension-independent. The advantage of the proposed machine learning method is
shown for multidimensional test data as well as for an example of cancer cell
evolution.",1
362,"Gait is an essential manifestation of depression. Laboratory gait
characteristics have been found to be closely associated with depression.
However, the gait characteristics of daily walking in real-world scenarios and
their relationships with depression are yet to be fully explored. This study
aimed to explore associations between depression symptom severity and
daily-life gait characteristics derived from acceleration signals in real-world
settings. In this study, we used two ambulatory datasets: a public dataset with
71 elder adults' 3-day acceleration signals collected by a wearable device, and
a subset of an EU longitudinal depression study with 215 participants and their
phone-collected acceleration signals (average 463 hours per participant). We
detected participants' gait cycles and force from acceleration signals and
extracted 20 statistics-based daily-life gait features to describe the
distribution and variance of gait cadence and force over a long-term period
corresponding to the self-reported depression score. The gait cadence of faster
steps (75th percentile) over a long-term period has a significant negative
association with the depression symptom severity of this period in both
datasets. Daily-life gait features could significantly improve the goodness of
fit of evaluating depression severity relative to laboratory gait patterns and
demographics, which was assessed by likelihood-ratio tests in both datasets.
This study indicated that the significant links between daily-life walking
characteristics and depression symptom severity could be captured by both
wearable devices and mobile phones. The gait cadence of faster steps in
daily-life walking has the potential to be a biomarker for evaluating
depression severity, which may contribute to clinical tools to remotely monitor
mental health in real-world settings.",1
363,"Chromatic dispersion is a common problem to degrade the system resolution in
optical coherence tomography (OCT). This study is to develop a deep learning
network for automated dispersion compensation (ADC-Net) in OCT. The ADC-Net is
based on a redesigned UNet architecture which employs an encoder-decoder
pipeline. The input section encompasses partially compensated OCT B-scans with
individual retinal layers optimized. Corresponding output is a fully
compensated OCT B-scans with all retinal layers optimized. Two numeric
parameters, i.e., peak signal to noise ratio (PSNR) and structural similarity
index metric computed at multiple scales (MS-SSIM), were used for objective
assessment of the ADC-Net performance. Comparative analysis of training models,
including single, three, five, seven and nine input channels were implemented.
The five-input channels implementation was observed as the optimal mode for
ADC-Net training to achieve robust dispersion compensation in OCT",1
364,"Protein design is a technique to engineer proteins by modifying their
sequence to obtain novel functionalities. In this method, amino acids in the
sequence are permutated to find the low energy states satisfying the
configuration. However, exploring all possible combinations of amino acids is
generally impossible to achieve on conventional computers due to the
exponential growth of possibilities with the number of designable sites. Thus,
sampling methods are currently used as a conventional approach to address the
protein design problems. Recently, quantum computation methods have shown the
potential to solve similar types of problems. In the present work, we use the
general idea of Grover's algorithm, a pure quantum computation method, to
design circuits at the gate-based level and address the protein design problem.
In our quantum algorithms, we use custom pair-wise energy tables consisting of
eight different amino acids. Also, the distance reciprocals between designable
sites are included in calculating energies in the circuits. Due to the noisy
state of current quantum computers, we mainly use quantum computer simulators
for this study. However, a very simple version of our circuits is implemented
on real quantum devices to examine their capabilities to run these algorithms.
Our results show that using $\mathcal{O}(\sqrt N)$ iterations, the circuits
find the correct results among all $N$ possibilities, providing the expected
quadratic speed up of Grover's algorithm over classical methods.",1
365,"Studies have shown that some people with underlying conditions such as
cancer, heart failure, diabetes, and hypertension are more likely to get
COVID-19 and have worse outcomes. In this paper, a fractional-order derivative
is proposed to study the transmission dynamics of COVID-19 taken into
consideration, a fraction of the population having an underlying condition. The
fractional derivative is defined in the Atangana Beleanu Caputo (ABC) sense.
For the proposed model, we find the basic reproductive number, the equilibrium
points and determine the stability of these equilibrium points. The existence
and the uniqueness of the solution are established along with Hyers Ulam
Stability. The numerical scheme for the operator was carried out to obtain a
numerical simulation to support the analytical results using the demographic
data of Ghana. Optimal control was incorporated into the model. The numerical
results reveal vaccinating the susceptible individuals with or without an
underlying condition greatly reduces the number of individuals exposed to the
COVID-19, thereby reducing COVID-19",1
366,"When considering large sets of molecules, it is helpful to place them in the
context of a ""chemical space"" - a multidimensional space defined by a set of
descriptors that can be used to visualize and analyze compound grouping as well
as identify regions that might be void of valid structures. The chemical space
of all possible molecules in a given biological or environmental sample can be
vast and largely unexplored, mainly due to current limitations in processing of
'big data' by brute force methods (e.g., enumeration of all possible compounds
in a space). Recent advances in artificial intelligence (AI) have led to
multiple new cheminformatics tools that incorporate AI techniques to
characterize and learn the structure and properties of molecules in order to
generate plausible compounds, thereby contributing to more accessible and
explorable regions of chemical space without the need for brute force methods.
We have used one such tool, a deep-learning software called DarkChem, which
learns a representation of the molecular structure of compounds by compressing
them into a latent space. With DarkChem's design, distance in this latent space
is often associated with compound similarity, making sparse regions interesting
targets for compound generation due to the possibility of generating novel
compounds. In this study, we used 1 million small molecules (less than 1000 Da)
to create a representative chemical space (defined by calculated molecular
properties) of all small molecules. We identified regions with few or no
compounds and investigated their location in DarkChem's latent space. From
these spaces, we generated 694,645 valid molecules, all of which represent
molecules not found in any chemical database to date. These molecules filled
50.8% of the probed empty spaces in molecular property space. Generated
molecules are provided in the supporting information.",1
367,"With the increasing availability and size of multi-omics datasets,
investigating the casual relationships between molecular phenotypes has become
an important aspect of exploring underlying biology and genetics. This paper
aims to introduce and review the available methods for building large-scale
causal molecular networks that have been developed in the past decade. Existing
methods have their own strengths and limitations so there is no one best
approach, and it is instead down to the discretion of the researcher. This
review also aims to discuss some of the current limitations to biological
interpretation of these networks, and important factors to consider for future
studies on molecular networks.",1
368,"In pathology and legal medicine, the histopathological and microbiological
analysis of tissue samples from infected deceased is a valuable information for
developing treatment strategies during a pandemic such as COVID-19. However, a
conventional autopsy carries the risk of disease transmission and may be
rejected by relatives. We propose minimally invasive biopsy with robot
assistance under CT guidance to minimize the risk of disease transmission
during tissue sampling and to improve accuracy. A flexible robotic system for
biopsy sampling is presented, which is applied to human corpses placed inside
protective body bags. An automatic planning and decision system estimates
optimal insertion point. Heat maps projected onto the segmented skin visualize
the distance and angle of insertions and estimate the minimum cost of a
puncture while avoiding bone collisions. Further, we test multiple insertion
paths concerning feasibility and collisions. A custom end effector is designed
for inserting needles and extracting tissue samples under robotic guidance. Our
robotic post-mortem biopsy (RPMB) system is evaluated in a study during the
COVID-19 pandemic on 20 corpses and 10 tissue targets, 5 of them being infected
with SARS-CoV-2. The mean planning time including robot path planning is
(5.72+-1.67) s. Mean needle placement accuracy is (7.19+-4.22) mm.",1
369,"Image datasets are commonly used in psychophysical experiments and in machine
learning research. Most publicly available datasets are comprised of images of
realistic and natural objects. However, while typical machine learning models
lack any domain specific knowledge about natural objects, humans can leverage
prior experience for such data, making comparisons between artificial and
natural learning challenging. Here, we introduce DELAUNAY, a dataset of
abstract paintings and non-figurative art objects labelled by the artists'
names. This dataset provides a middle ground between natural images and
artificial patterns and can thus be used in a variety of contexts, for example
to investigate the sample efficiency of humans and artificial neural networks.
Finally, we train an off-the-shelf convolutional neural network on DELAUNAY,
highlighting several of its intriguing features.",1
370,"Significant efforts have been done in last two decades to develop nanoscale
spectroscopy techniques owning to their great potential for single-molecule
structural detection and in addition, to resolve open questions in
heterogeneous biological systems, such as protein-DNA complexes. Applying
AFM-IR technique has become a powerful leverage for obtaining simultaneous
absorption spectra with a nanoscale spatial resolution for studied proteins,
however the IR-AFM investigation of DNA molecules on surface, as a benchmark
for a nucleoprotein complexes nanocharacterization, has remained elusive.
Herein, we demonstrate methodological approach for acquisition of IR-AFM
mapping modalities with corresponding absorption spectra based on two different
DNA deposition protocols on spermidine and Ni2+ pretreated mica surface. The
nanoscale IR absorbance of distinctly formed DNA morphologies on mica are
demonstrated through series of IR-AFM absorption maps with corresponding IR
spectrum. Our results thus demonstrate the sensitivity of IR-AFM
nanospectroscopy for a nucleic acid research with an open potential to be
employed in further investigation of nucleoprotein complexes.",1
371,"Nuclear Magnetic Resonance (NMR) spectroscopy is one of the major techniques
in structural biology with over 11800 protein structures deposited in the
Protein Data Bank. NMR can elucidate structures and dynamics of small and
medium size proteins in solution, living cells, and solids, but has been
limited by the tedious data analysis process. It typically requires weeks or
months of manual work of trained expert to turn NMR measurements into a protein
structure. Automation of this process is an open problem, formulated in the
field over 30 years ago. Here, we present the first approach that addresses
this challenge. Our method, ARTINA, uses as input only NMR spectra and the
protein sequence, delivering a structure strictly without any human
intervention. Tested on a 100-protein benchmark (1329 2D/3D/4D NMR spectra),
ARTINA demonstrated its ability to solve structures with 1.44 {\AA} median RMSD
to the PDB reference and 91.36% correct NMR resonance assignments. ARTINA can
be used by non-experts, reducing the effort for a protein structure
determination by NMR essentially to the preparation of the sample and the
spectra measurements.",1
372,"Spiking neural networks can compensate for quantization error by encoding
information either in the temporal domain, or by processing discretized
quantities in hidden states of higher precision. In theory, a wide dynamic
range state-space enables multiple binarized inputs to be accumulated together,
thus improving the representational capacity of individual neurons. This may be
achieved by increasing the firing threshold, but make it too high and sparse
spike activity turns into no spike emission. In this paper, we propose the use
of `threshold annealing' as a warm-up method for firing thresholds. We show it
enables the propagation of spikes across multiple layers where neurons would
otherwise cease to fire, and in doing so, achieve highly competitive results on
four diverse datasets, despite using binarized weights. Source code is
available at https://github.com/jeshraghian/snn-tha/",1
373,"Neural mechanisms of touch are typically studied in laboratory settings using
robotic or other types of well-controlled devices. Such stimuli are very
different from highly complex naturalistic human-to-human touch interactions.
The lack of scientifically useful naturalistic stimuli hampers progress,
particularly in social touch research. Vision science, on the other hand, has
benefitted from inventions such as virtual reality systems that have provided
researchers with precision control of naturalistic stimuli. In the field of
touch research, producing and manipulating stimuli is particularly challenging
due to the complexity of skin mechanics. Here we review the history of touch
neuroscience focusing on the contrast between strictly controlled and
naturalistic stimuli and compare with vision science. We discuss new methods
that may overcome the obstacles with precision-controlled tactile stimuli, and
recent successes in naturalistic texture production. In social touch research,
precise tracking and measurement of naturalistic human-to-human touch
interactions offers exciting new possibilities.",1
374,"Our understanding of radiation induced cellular damage has greatly improved
over the past decades. Despite this progress, there are still many obstacles to
fully understanding how radiation interacts with biologically relevant cellular
components to form observable endpoints. One hurdle is the difficulty faced by
members of different research groups in directly comparing results. Multiple
Monte Carlo codes have been developed to simulate damage induction at the DNA
scale, while at the same time various groups have developed models that
describe DNA repair processes with varying levels of detail. These repair
models are intrinsically linked to the damage model employed in their
development, making it difficult to disentangle systematic effects in either
part of the modelling chain. The modelling chain typically consists of track
structure Monte Carlo simulations of the physics interactions creating direct
damages to the DNA; followed by simulations of the production and initial
reactions of chemical species causing indirect damages. After the DNA damage
induction, DNA repair models combine the simulated damage patterns with
biological models to determine the biological consequences of the damage. We
propose a new Standard data format for DNA Damage to unify the interface
between the simulation of damage induction and the biological modelling of cell
repair processes. Such a standard greatly facilitates inter model comparisons,
providing an ideal environment to tease out model assumptions and identify
persistent, underlying mechanisms. Through inter model comparisons, this
unified standard has the potential to greatly advance our understanding of the
underlying mechanisms of radiation induced DNA damage and the resulting
observable biological effects.",1
375,"Phase separation of several different overall neutral polyampholyte species
(with zero net charge) is studied in solution with two oppositely charged ion
species that can form ion-pairs through an association reaction. A field theory
description of the system, that treats polyampholyte charge sequence dependent
electrostatic interactions as well as excluded volume effects, is hereby given.
Interestingly, analysis of the model using random phase approximation and field
theoretic simulation consistently show evidence of a re-entrant polyampholyte
phase separation at high ion concentrations when there is an overall decrease
of volume upon ion-association. As an illustration of the ramifications of our
theoretical framework, several polyampholyte concentration vs ion concentration
phase diagrams under constant temperature conditions are presented to elucidate
the dependence of phase separation behavior on polyampholyte sequence charge
pattern as well as ion-pair dissociation constant, volumetric effects on ion
association, solvent quality, and temperature.",1
376,"Because a fast vaccination rollout against coronavirus disease 2019
(COVID-19) is critical to restore daily life and avoid virus mutations, it is
tempting to have a relaxed vaccination-administration management system.
However, a robust management system can support the enforcement of preventive
measures, and in turn, reduce incidence and deaths. Here, we model a trustable
and reliable management system based on blockchain for vaccine distribution by
extending the Susceptible-Exposed-Infected-Recovery (SEIR) model. The model
includes prevention measures such as mask-wearing, social distance, vaccination
rate, and vaccination efficiency. It also considers negative social behavior,
such as violations of social distance and attempts of using illegitimate
vaccination proofs. By evaluating the model, we show that the proposed system
can reduce up to 2.5 million cases and half a million deaths in the most
demanding scenarios.",1
377,"The brain can learn to solve a wide range of tasks with high temporal and
energetic efficiency. However, most biological models are composed of simple
single compartment neurons and cannot achieve the state-of-art performances of
artificial intelligence. We propose a multi-compartment model of pyramidal
neuron, in which bursts and dendritic input segregation give the possibility to
plausibly support a biological target-based learning. In target-based learning,
the internal solution of a problem (a spatio temporal pattern of bursts in our
case) is suggested to the network, bypassing the problems of error
backpropagation and credit assignment. Finally, we show that this neuronal
architecture naturally support the orchestration of hierarchical imitation
learning, enabling the decomposition of challenging long-horizon
decision-making tasks into simpler subtasks.",1
378,"Neural circuits exhibit complex activity patterns, both spontaneously and
evoked by external stimuli. Information encoding and learning in neural
circuits depend on how well time-varying stimuli can control spontaneous
network activity. We show that in firing-rate networks in the balanced state,
external control of recurrent dynamics, i.e., the suppression of
internally-generated chaotic variability, strongly depends on correlations in
the input. A unique feature of balanced networks is that, because common
external input is dynamically canceled by recurrent feedback, it is far easier
to suppress chaos with independent inputs into each neuron than through common
input. To study this phenomenon we develop a non-stationary dynamic mean-field
theory that determines how the activity statistics and largest Lyapunov
exponent depend on frequency and amplitude of the input, recurrent coupling
strength, and network size, for both common and independent input. We also show
that uncorrelated inputs facilitate learning in balanced networks.",1
379,"In many social mammals, early life social adversity and social integration
largely predict individual health, lifespan and reproductive success. Efforts
in identifying the physiological mechanisms mediating the relationship between
the social environment and individual fitness have so far concentrated on
socially-induced stress, mediated by alterations in neuroendocrine signaling
and immune function. Here, we propose a much-needed alternative mechanism
relying on microbially-mediated effects: social relationships with
conspecifics, both in early life and adulthood, might strongly contribute both
to the transmission of beneficial microbes and to diversifying host
microbiomes. In turn, more valuable and diverse microbiomes would promote
pathogen resistance and optimal health and thus translate into positive fitness
outcomes. This mechanism relies on two emerging findings from empirical
studies, namely that microbiomes (i) are largely socially transmitted via
vertical and horizontal routes, and (ii) play a pervasive role in host
development, physiology, metabolism, and susceptibility to pathogens. We
suggest that the social transmission of microbiomes has the potential to
explain the sociality-fitness nexus, to a similar - or even higher - extent
than chronic social stress, in ways that have yet to be studied empirically in
social mammals.",1
380,"We will study the relation between two well-known theories, genetic evolution
and random matrix theory in the context of many-body systems and chaos theory.
We show that the time evolution of certain chaotic quantum mechanical models is
similar to the evolution of a living cell. It is also suggested that natural
selection can be described by a random matrix theory with statistical
distribution in which the genetic evolution acts as a Gross-Witten-Wadia phase
transition.",1
381,"Inferring correlation among mixed-type biological traits while controlling
for the evolutionary relationship among taxa is of great scientific interest
yet remains computationally challenging. The recently developed phylogenetic
multivariate probit model accommodates binary and continuous traits by assuming
latent parameters underlying binary traits. The most expensive inference step
is to sample the latent parameters from their conditional posterior that is a
high dimensional truncated normal. The current best approach uses the bouncy
particle sampler (BPS) optimized with a linear-order gradient evaluation method
that employs a dynamic programming strategy on the directed acyclic structure
of the phylogeny. Despite its significant improvement upon previous methods,
with increasing sample sizes BPS encounters difficulty in exploring the
parameter space and fails to provide reliable estimates for the across-trait
partial correlation that describes the direct effects among traits. We develop
a new inference scheme that highlights Zigzag Hamiltonian Monte Carlo
(Zigzag-HMC), a variant of traditional HMC that uses Laplace momentum.
Zigzag-HMC can utilize the same gradient evaluation method that speeds up BPS,
yet it is much more efficient. We further improve the efficiency by jointly
updating the latent parameters and correlation elements using a differential
operator splitting technique. In an application exploring HIV-1 evolution that
requires joint sampling from a 11,235-dimensional truncated normal and a
24-dimensional covariance matrix, our method yields a $ 5\times $ speedup
compared to BPS and makes it possible to estimate the direct effects among
important viral mutations and virulence. We also extend the phylogenetic probit
model to categorical traits for broader applicability, and demonstrate its use
to study Aquilegia flower and pollinator co-evolution.",1
382,"In this paper, we discuss the conceptual underpinnings of Modern Coexistence
Theory (MCT), a quantitative framework for understanding ecological
coexistence. In order to use MCT to infer how species are coexisting, one must
relate a complex model (which simulates coexistence in the real world) to
simple models in which previously proposed explanations for coexistence have
been codified. This can be accomplished in three steps: 1) relating the
construct of coexistence to invasion growth rates, 2) mathematically
partitioning the invasion growth rates into coexistence mechanisms (i.e.,
classes of explanations for coexistence), and 3) relating coexistence
mechanisms to simple explanations for coexistence. Previous research has
primarily focused on step 2. Here, we discuss the other crucial steps and their
implications for inferring the mechanisms of coexistence in real communities.
  Our discussion of step 3 -- relating coexistence mechanisms to simple
explanations for coexistence -- serves a heuristic guide for hypothesizing
about the causes of coexistence in new models; but also addresses
misconceptions about coexistence mechanisms. For example, the storage effect
has little to do with bet-hedging or ""storage"" via a robust life-history stage;
relative nonlinearity is more likely to promote coexistence than originally
thought; and fitness-density covariance is an amalgam of a large number of
previously proposed explanations for coexistence (e.g., the
competition-colonization trade-off, heteromyopia, spatially-varying resource
supply ratios). Additionally, we review a number of topics in MCT, including
the role of ""scaling factors""; whether coexistence mechanisms are
approximations; whether the magnitude or sign of invasion growth rates matters
more; whether Hutchinson solved the paradox of the plankton; the
scale-dependence of coexistence mechanisms; and much more.",1
383,"Exercise rehabilitation is an important part in the comprehensive management
of patients with diabetes and there is a need to conduct comprehensively
evaluation of several factors such as the physical fitness, cardiovascular risk
and diabetic disease factors. However, special disease features of diabetes and
its wide heterogeneity make it difficult to apply individualized approaches. In
this study, a novel framework was established based on the Fuzzy Analytic
Hierarchy Process (FAHP) approach to calculate various physiological factors
weights when developing a diabetic exercise prescription. Proposed factors were
investigated with respect to three groups which contains 12 different aspects.
The relative weights were assessed by a database which established through a
questionnaire survey. It is concluded that the physical fitness factors and
cardiovascular risk factors need to be paid more attention to considered in the
formulation of exercise rehabilitation programs than disease factors. And the
cardiopulmonary function of physical fitness factors accounts for the highest
importance. Furthermore, it was found that blood lipids have the lowest
importance among studied factors. The mathematical model of exercise
rehabilitation program for diabetes patients was established, which provided
the theoretical basis for individualized guidance of exercise rehabilitation
program.",1
384,"In the past three decades, neuroimaging has provided important insights into
structure-function relationships in the human brain. Recently, however, the
methods for analyzing functional magnetic resonance imaging (fMRI) data have
come under scrutiny, with studies questioning cross-software comparability, the
validity of statistical inference and interpretation, and the influence of the
spatial filter size on neuroimaging analyses. As most fMRI studies only use a
single filter for analysis, much information on the size and shape of the BOLD
signal in Gaussian scale space remains hidden and constrains the interpretation
of fMRI studies. To investigate the influence of the spatial observation scale
on fMRI analysis, we use a spatial multiscale analysis with a range of Gaussian
filters from 1-20 mm (full width at half maximum) to analyze fMRI data from a
speech repetition paradigm in 25 healthy subjects. We show that analyzing the
fMRI data over a range of Gaussian filter kernels reveals substantial
variability in the neuroanatomical localization and the average signal strength
and size of suprathreshold clusters depending on the filter size. We also
demonstrate how small spatial filters bias the results towards subcortical and
cerebellar clusters. Furthermore, we describe substantially different
scale-dependent cluster size dynamics between cortical and cerebellar clusters.
We discuss how spatial multiscale analysis may substantially improve the
interpretation of fMRI data. We propose to further develop a spatial multiscale
analysis to fully explore the deep structure of the BOLD signal in Gaussian
scale space.",1
385,"Since their proposal in 2016, the FAIR principles have been largely discussed
by different communities and initiatives involved in the development of
infrastructures to enhance support for data findability, accessibility,
interoperability, and reuse. One of the challenges in implementing these
principles lies in defining a well-delimited process with organized and
detailed actions. This paper presents a workflow of actions that is being
adopted in the VODAN BR pilot for generating FAIR (meta)data for COVID-19
research. It provides the understanding of each step of the process,
establishing their contribution. In this work, we also evaluate potential tools
to (semi)automatize (meta)data treatment whenever possible. Although defined
for a particular use case, it is expected that this workflow can be applied for
other epidemical research and in other domains, benefiting the entire
scientific community.",1
386,"Evaluation the prediction of Efficiency index by DVH parameter for SRS
treatment plans using Supervised Machine learning and the performance of
predictive model algorithms of RapidMiner GO in the parameter prediction are
investigated. Dose volume histogram (DVH) based Efficiency index was calculated
for 100 clinical SRS plans generated by Leksell Gamma plan, and the results
were compared to predicted values produced by machine learning toolbox of
RapidMiner Go, algorithms are namely, Generalized linear model (GLR), Decision
Tree Model, Support Vector Machine (SVM), Gradient Boosted Trees (GBT), Random
Forest (RF) and Deep learning Model (DL). Root mean square error (RMSE),
Average absolute error, Absolute relative error, squared correlation and model
building time were determined to evaluate the performance of each algorithm.
The GLR algorithm model had square correlation of 0.974 with the smallest RMSE
of 0.01, relatively high prediction speed, and fast model building time with
2.812 s, according to the results. The RMSE values for all models were between
0.01 upto 0.021, all algorithms performed well. The RMSE of the Gradient
Boosted Tree, Random Forest, and Decision Tree regression algorithms was found
to be greater than 0.01, suggesting that they are not appropriate for
predicting EI in this analysis. RapidMiner GO machine learning models can be
used to predict DVH parameters like EI in SRS treatment planning QA. To
effectively evaluate the parameter, it is necessary to choose a suitable
machine learning algorithm.",1
387,"Quantification and classification of protein structures, such as knotted
proteins, often requires noise-free and complete data. Here we develop a
mathematical pipeline that systematically analyzes protein structures. We
showcase this geometric framework on proteins forming open-ended trefoil knots,
and we demonstrate that the mathematical tool, persistent homology, faithfully
represents their structural homology. This topological pipeline identifies
important geometric features of protein entanglement and clusters the space of
trefoil proteins according to their depth. Persistence landscapes quantify the
topological difference between a family of knotted and unknotted proteins in
the same structural homology class. This difference is localized and
interpreted geometrically with recent advancements in systematic computation of
homology generators. The topological and geometric quantification we find is
robust to noisy input data, which demonstrates the potential of this approach
in contexts where standard knot theoretic tools fail.",1
388,"During the first months, the Covid-19 pandemic has required most countries to
implement complex sequences of non-pharmaceutical interventions, with the aim
of controlling the transmission of the virus in the population. To be able to
take rapid decisions, a detailed understanding of the current situation is
necessary. Estimates of time-varying, instantaneous reproduction numbers
represent a way to quantify the viral transmission in real time. They are often
defined through a mathematical compartmental model of the epidemic, like a
stochastic SEIR model, whose parameters must be estimated from multiple time
series of epidemiological data. Because of very high dimensional parameter
spaces (partly due to the stochasticity in the spread models) and incomplete
and delayed data, inference is very challenging. We propose a state space
formalisation of the model and a sequential Monte Carlo approach which allow to
estimate a daily-varying reproduction number for the Covid-19 epidemic in
Norway with sufficient precision, on the basis of daily hospitalisation and
positive test incidences. The method is in regular use in Norway and is a
powerful instrument for epidemic monitoring and management.",1
389,"Sudden Unexpected Death in Epilepsy (SUDEP) remains a leading cause of death
in people with epilepsy. Despite the constant risk for patients and bereavement
to family members, to date the physiological mechanisms of SUDEP remain
unknown. Here we explore the potential to identify putative predictive signals
of SUDEP from online digital behavioral data using text and sentiment analysis.
Specifically, we analyze Facebook timelines of six epilepsy patients deceased
due to SUDEP, donated by surviving family members. We find preliminary evidence
for behavioral changes detectable by text and sentiment analysis tools. Namely,
in the months preceding their SUDEP event patient social media timelines show:
i) increase in verbosity; ii) increased use of functional words; and iii)
sentiment shifts as measured by different sentiment analysis tools. Combined,
these results suggest that social media engagement, as well as its sentiment,
may serve as possible early-warning signals for SUDEP in people with epilepsy.
While the small sample of patient timelines analyzed in this study prevents
generalization, our preliminary investigation demonstrates the potential of
social media data as complementary data in larger studies of SUDEP and
epilepsy.",1
390,"Model fitting methods have been widely used in neuroscience to infer the
parameters of a biophysical system from its responses to experimental
stimulations. For instance, the parameters of a chemical synapse can be
estimated from its postsynaptic responses to evoked stimuli. However, these
estimates critically depend on the stimulation protocol being used. Experiments
are often conducted with non-adaptative stimulation protocols that may not
yield enough information about the parameters. Here, we propose using Bayesian
active learning (BAL) for synaptic characterization, and to choose the most
informative stimuli by maximizing the mutual information between the data and
the unknown parameters. This requires performing high-dimensional integrations
and optimizations in real time. Current methods are either too time consuming,
or only applicable to specific models. We build on recent developments in
non-linear filtering and parallel computing to provide a general framework for
online BAL, which is fast enough to be used in real-time biological experiments
and can be applied to a wide range of statistical models. Using synthetic data,
we show that our method has the potential to significantly improve the
precision of inferred synaptic parameters. Finally, we explore the situation
where the constraint is not given by the total number of observations but by
the duration of the experiment.",1
391,"Chimeric antigen receptor (CAR) T-cell based immunotherapy has shown its
potential in treating blood cancers, and its application to solid tumors is
currently being extensively investigated. For glioma brain tumors, various CAR
T-cell targets include IL13Ra2, EGFRvIII, HER2, EphA2, GD2, B7-H3, and
chlorotoxin. In this work, we are interested in developing a mathematical model
of IL13Ra2 targeting CAR T-cells for treating glioma. We focus on extending the
work of Kuznetsov et al. (1994) by considering binding of multiple CAR T-cells
to a single glioma cell, and the dynamics of these multi-cellular conjugates.
Our model more accurately describes experimentally observed CAR T-cell killing
assay data than a model which does not consider cell binding. Moreover, we
derive conditions in the CAR T-cell expansion rate that determines treatment
success or failure. Finally, we show that our model captures distinct CAR
T-cell killing dynamics at low, medium, and high antigen receptor densities in
patient-derived brain tumor cells.",1
392,"The 21st century is presenting humankind with unprecedented environmental and
medical challenges. The ability to design novel proteins tailored for specific
purposes could transform our ability to respond timely to these issues. Recent
advances in the field of artificial intelligence are now setting the stage to
make this goal achievable. Protein sequences are inherently similar to natural
languages: Amino acids arrange in a multitude of combinations to form
structures that carry function, the same way as letters form words and
sentences that carry meaning. Therefore, it is not surprising that throughout
the history of Natural Language Processing (NLP), many of its techniques have
been applied to protein research problems. In the last few years, we have
witnessed revolutionary breakthroughs in the field of NLP. The implementation
of Transformer pre-trained models has enabled text generation with human-like
capabilities, including texts with specific properties such as style or
subject. Motivated by its considerable success in NLP tasks, we expect
dedicated Transformers to dominate custom protein sequence generation in the
near future. Finetuning pre-trained models on protein families will enable the
extension of their repertoires with novel sequences that could be highly
divergent but still potentially functional. The combination of control tags
such as cellular compartment or function will further enable the controllable
design of novel protein functions. Moreover, recent model interpretability
methods will allow us to open the 'black box' and thus enhance our
understanding of folding principles. While early initiatives show the enormous
potential of generative language models to design functional sequences, the
field is still in its infancy. We believe that protein language models are a
promising and largely unexplored field and discuss their foreseeable impact on
protein design.",1
393,"Brain development relies on both experience and genetically defined programs.
Time windows where certain brain circuits are particularly receptive to
external stimuli, resulting in heightened plasticity, are referred to as
critical periods. Sleep is thought to be essential for normal brain
development. Importantly, studies have shown that sleep enhances critical
period plasticity and promotes experience-dependent synaptic pruning in the
developing mammalian brain. Therefore, normal plasticity during critical
periods depends on proper sleep. Problems falling and staying asleep occur at a
higher rate in Autism Spectrum Disorder (ASD) relative to typical development.
In this review, we explore the potential link between sleep, critical period
plasticity, and ASD. First, we review the importance of critical period
plasticity in typical development and the role of sleep in this process. Next,
we summarize the evidence linking ASD with deficits in synaptic plasticity in
rodent models of high-confident ASD gene candidates. We then show that almost
all the high-confidence rodent models of ASD that show sleep deficits also
display plasticity deficits. Given how important sleep is for critical period
plasticity, it is essential to understand the connections between synaptic
plasticity, sleep, and brain development in ASD. However, studies investigating
sleep or plasticity during critical periods in ASD mouse models are lacking.
Therefore, we highlight an urgent need to consider developmental trajectory in
studies of sleep and plasticity in neurodevelopmental disorders.",1
394,"Multicellular collective migration is a ubiquitous strategy of cells to
translocate spatially in diverse tissue environments to accomplish a wide
variety of biological phenomena, viz. embryonic development, wound healing, and
tumor progression. Diverse cellular functions and behaviors, for instance, cell
protrusions, active contractions, cell-cell adhesion, biochemical signaling,
remodeling of tissue micro-environment, etc., play their own role concomitantly
to have a single concerted consequence of multicellular migration. Thus
unveiling the driving principles, both biochemical and biophysical, of the
inherently complex process of collective cell migration is an insurmountable
task. Mathematical and computational models, in tandem with experimental data,
help in shedding some light on it. Here we review different factors influencing
Collective Cell Migration and then focus on different mathematical and
computational models - discrete, hybrid, and continuum - which helps in
revealing different aspects of multicellular migration. Finally, we discuss the
applications of these modeling frameworks specific to cancer",1
395,"Infectious disease transmission dynamics are particularly sensitive to social
contact patterns, and the precautions people take to limit disease
transmission. It depends on the age distribution of the community. Thus,
knowing the age$-$specific prevalence and incidence of infectious diseases is
critical for predicting future disease burden and the efficacy of interventions
like immunization. This study uses an SEIR age$-$structured multi-group
epidemic model to understand how social contact affects disease control. We
created location$-$specific social contact matrices in the community to see how
social mixing has affected illness spread. We estimated the basic reproduction
number $(R_0)$ of the system and plotted its global behavior in terms of $R_0$.
Optimal control for the problem has also been established quantitatively. The
proposed model's transmission rate for India from September 1, 2020, to
December 31, 2020, has also been estimated. We replicated the lifting of
non-pharmaceutical therapies by allowing participants to return to work in
phases and studied the impact of this. Our findings imply that identifying
symptomatic sick people aged $20-49$ can help lower the number of infected
people when schools are closed. When some schools are partially open, awareness
of symptomatic infected persons helps reduce cases. The simulation results also
suggest that limiting contact at school and other meeting areas could
significantly lower the number of instances. Using the least square approach,
it was discovered that the time$-$dependent transmission rate is more realistic
than the constant spread rate for COVID$-19$ in India. To reduce the COVID$-19$
in burden, we found that gradually loosening control methods could flatten and
lower other peaks. Our findings may help health policymakers decide on timely
age$-$based immunization distribution strategies and hence control the disease.",1
396,"Neuroinflammation is a significant aspect of many neurological diseases of
Homo sapiens, and the genes that are differentially expressed in this process
should be well understood to gather the nature of such diseases. We have
conducted a meta-analysis (based on a combined adjusted P value and logFC
scheme) of 6 multi-species (Homo sapiens, Mus musculus) datasets (available on
GEO, short for Gene Expression Omnibus) obtained through microarray technology.
Our analysis shows that the genes coding pleckstrin homology domain and
galectin-9 proteins take part in neuroinflammation in microglia.",1
397,"We studied the effects of spatial configuration on collective dynamics in a
nearest-neighbour and diffusively coupled lattice of heterogeneous nodes. The
networks contained nodes from two populations, which differed in their
intrinsic excitability. Initially, these populations were uniformly and
randomly distributed throughout the lattice. We then developed an iterative
algorithm for perturbing the arrangement of the network such that nodes from
the same population were increasingly likely to be adjacent to one another. We
found that the global input strength, or network drive, necessary to transition
the network from a state of quiescence to a state of synchronised and
oscillatory activity was decreased as network sortedness was increased.
Moreover, for weak coupling, we found that regimes of partial synchronisation
exist (i.e., 2:1 resonance in the activity of the two populations), which were
dependent both on network drive (sometimes in a non-monotonic fashion) and
network sortedness.",1
398,"SIRS models capture transmission dynamics of infectious diseases for which
immunity is not lifelong. Extending these models by a W compartment for
individuals with waning immunity, the boosting of the immune system upon
repeated exposure may be incorporated. Previous analyses assumed identical
waning rates from R to W and from W to S. This implicitly assumes equal length
for the period of full immunity and of waned immunity. We relax this
restriction, and allow an asymmetric partitioning of the total immune period.
Stability switches of the endemic equilibrium are investigated with a
combination of analytic and numerical tools. Then, continuation methods are
applied to track bifurcations along the equilibrium branch. We find rich
dynamics: Hopf bifurcations, endemic double bubbles, and regions of
bistability. Our results highlight that the length of the period in which
waning immunity can be boosted is a crucial parameter significantly influencing
long term epidemiological dynamics.",1
399,"With a kind of magnetism, the human retina draws the eye of neuroscientist
and physicist alike. It is attractive as a self-organizing system, which forms
as a part of the central nervous system via biochemical and mechanical cues.
The retina is also intriguing as an electro-optical device, converting photons
into voltages to perform on-the-fly filtering before the signals are sent to
our brain. Here, we consider how the advent of stem cell derived in vitro
analogs of the retina, termed retina organoids, opens up an exploration of the
interplay between optics, electrics, and mechanics in a complex neuronal
network, all in a Petri dish. This review presents state-of-the-art retina
organoid protocols by emphasizing links to the biochemical and mechanical
signals of in vivo retinogenesis. Electrophysiological recording of active
signal processing becomes possible as retina organoids generate light sensitive
and synaptically connected photoreceptors. Experimental biophysical tools
provide data to steer the development of mathematical models operating at
different levels of coarse-graining. In concert, they provide a means to study
how mechanical factors guide retina self-assembly. In turn, this understanding
informs the engineering of mechanical signals required to tailor the growth of
neuronal network morphology. Tackling the complex developmental and
computational processes in the retina requires an interdisciplinary endeavor
combining experiment and theory, physics, and biology. The reward is enticing:
in the next few years, retina organoids could offer a glimpse inside the
machinery of simultaneous cellular self-assembly and signal processing, all in
an in vitro setting.",1
400,"Efficient contact tracing and isolation is an effective strategy to control
epidemics. It was used effectively during the Ebola epidemic and successfully
implemented in several parts of the world during the ongoing COVID-19 pandemic.
An important consideration in contact tracing is the budget on the number of
individuals asked to quarantine -- the budget is limited for socioeconomic
reasons. In this paper, we present a Markov Decision Process (MDP) framework to
formulate the problem of using contact tracing to reduce the size of an
outbreak while asking a limited number of people to quarantine. We formulate
each step of the MDP as a combinatorial problem, MinExposed, which we
demonstrate is NP-Hard; as a result, we develop an LP-based approximation
algorithm. Though this algorithm directly solves MinExposed, it is often
impractical in the real world due to information constraints. To this end, we
develop a greedy approach based on insights from the analysis of the previous
algorithm, which we show is more interpretable. A key feature of the greedy
algorithm is that it does not need complete information of the underlying
social contact network. This makes the heuristic implementable in practice and
is an important consideration. Finally, we carry out experiments on simulations
of the MDP run on real-world networks, and show how the algorithms can help in
bending the epidemic curve while limiting the number of isolated individuals.
Our experimental results demonstrate that the greedy algorithm and its variants
are especially effective, robust, and practical in a variety of realistic
scenarios, such as when the contact graph and specific transmission
probabilities are not known. All code can be found in our GitHub repository:
https://github.com/gzli929/ContactTracing.",2
401,"We focus on the confounding bias between language and location in the visual
grounding pipeline, where we find that the bias is the major visual reasoning
bottleneck. For example, the grounding process is usually a trivial
language-location association without visual reasoning, e.g., grounding any
language query containing sheep to the nearly central regions, due to that most
queries about sheep have ground-truth locations at the image center. First, we
frame the visual grounding pipeline into a causal graph, which shows the
causalities among image, query, target location and underlying confounder.
Through the causal graph, we know how to break the grounding bottleneck:
deconfounded visual grounding. Second, to tackle the challenge that the
confounder is unobserved in general, we propose a confounder-agnostic approach
called: Referring Expression Deconfounder (RED), to remove the confounding
bias. Third, we implement RED as a simple language attention, which can be
applied in any grounding method. On popular benchmarks, RED improves various
state-of-the-art grounding methods by a significant margin. Code will soon be
available at: https://github.com/JianqiangH/Deconfounded_VG.",2
402,"In this paper, we address the problem of finding a correspondence, or
matching, between the functions of two programs in binary form, which is one of
the most common task in binary diffing. We introduce a new formulation of this
problem as a particular instance of a graph edit problem over the call graphs
of the programs. In this formulation, the quality of a mapping is evaluated
simultaneously with respect to both function content and call graph
similarities. We show that this formulation is equivalent to a network
alignment problem. We propose a solving strategy for this problem based on
max-product belief propagation. Finally, we implement a prototype of our
method, called QBinDiff, and propose an extensive evaluation which shows that
our approach outperforms state of the art diffing tools.",2
403,"In this paper, we present a novel algorithm to address the Network Alignment
problem. It is inspired from a previous message passing framework of Bayati et
al. [2] and includes several modifications designed to significantly speed up
the message updates as well as to enforce their convergence. Experiments show
that our proposed model outperforms other state-of-the-art solvers. Finally, we
propose an application of our method in order to address the Binary Diffing
problem. We show that our solution provides better assignment than the
reference differs in almost all submitted instances and outline the importance
of leveraging the graphical structure of binary programs.",2
404,"This abstract proposes an approach towards goal-oriented modeling of the
detection and modeling complex social phenomena in multiparty discourse in an
online political strategy game. We developed a two-tier approach that first
encodes sociolinguistic behavior as linguistic features then use reinforcement
learning to estimate the advantage afforded to any player. In the first tier,
sociolinguistic behavior, such as Friendship and Reasoning, that speakers use
to influence others are encoded as linguistic features to identify the
persuasive strategies applied by each player in simultaneous two-party
dialogues. In the second tier, a reinforcement learning approach is used to
estimate a graph-aware reward function to quantify the advantage afforded to
each player based on their standing in this multiparty setup. We apply this
technique to the game Diplomacy, using a dataset comprising of over 15,000
messages exchanged between 78 users. Our graph-aware approach shows robust
performance compared to a context-agnostic setup.",2
405,"We identify properties of universal adversarial perturbations (UAPs) that
distinguish them from standard adversarial perturbations. Specifically, we show
that targeted UAPs generated by projected gradient descent exhibit two
human-aligned properties: semantic locality and spatial invariance, which
standard targeted adversarial perturbations lack. We also demonstrate that UAPs
contain significantly less signal for generalization than standard adversarial
perturbations -- that is, UAPs leverage non-robust features to a smaller extent
than standard adversarial perturbations.",2
406,"Session-based recommendation (SBR) is a challenging task, which aims at
recommending next items based on anonymous interaction sequences. Despite the
superior performance of existing methods for SBR, there are still several
limitations: (i) Almost all existing works concentrate on single interest
extraction and fail to disentangle multiple interests of user, which easily
results in suboptimal representations for SBR. (ii) Furthermore, previous
methods also ignore the multi-form temporal information, which is significant
signal to obtain current intention for SBR. To address the limitations
mentioned above, we propose a novel method, called \emph{Temporal aware
Multi-Interest Graph Neural Network} (TMI-GNN) to disentangle multi-interest
and yield refined intention representations with the injection of two level
temporal information. Specifically, by appending multiple interest nodes, we
construct a multi-interest graph for current session, and adopt the GNNs to
model the item-item relation to capture adjacent item transitions,
item-interest relation to disentangle the multi-interests, and interest-item
relation to refine the item representation. Meanwhile, we incorporate
item-level time interval signals to guide the item information propagation, and
interest-level time distribution information to assist the scattering of
interest information. Experiments on three benchmark datasets demonstrate that
TMI-GNN outperforms other state-of-the-art methods consistently.",2
407,"Approximate message passing (AMP) is a promising technique for unknown signal
reconstruction of certain high-dimensional linear systems with non-Gaussian
signaling. A distinguished feature of the AMP-type algorithms is that their
dynamics can be rigorously described by state evolution. However, state
evolution does not necessarily guarantee the convergence of iterative
algorithms. To solve the convergence problem of AMP-type algorithms in
principle, this paper proposes a memory AMP (MAMP) under a sufficient statistic
condition, named sufficient statistic MAMP (SS-MAMP). We show that the
covariance matrices of SS-MAMP are L-banded and convergent. Given an arbitrary
MAMP, we can construct an SS-MAMP by damping, which not only ensures the
convergence of MAMP but also preserves the orthogonality of MAMP, i.e., its
dynamics can be rigorously described by state evolution. As a byproduct, we
prove that the Bayes-optimal orthogonal/vector AMP (BO-OAMP/VAMP) is an
SS-MAMP. As a result, we reveal two interesting properties of BO-OAMP/VAMP for
large systems: 1) the covariance matrices are L-banded and are convergent, and
2) damping and memory are useless (i.e., do not bring performance improvement).
As an example, we construct a sufficient statistic Bayes-optimal MAMP
(SS-BO-MAMP), which is Bayes optimal if its state evolution has a unique fixed
point. In addition, the mean square error (MSE) of SS-BO-MAMP is not worse than
the original BO-MAMP. Finally, simulations are provided to verify the validity
and accuracy of the theoretical results.",2
408,"Graph Neural Networks (GNNs) have recently emerged as a robust framework for
graph-structured data. They have been applied to many problems such as
knowledge graph analysis, social networks recommendation, and even Covid19
detection and vaccine developments. However, unlike other deep neural networks
such as Feed Forward Neural Networks (FFNNs), few analyses such as verification
and property inferences exist, potentially due to dynamic behaviors of GNNs,
which can take arbitrary graphs as input, whereas FFNNs which only take fixed
size numerical vectors as inputs.
  This paper proposes an approach to analyze GNNs by converting them into FFNNs
and reusing existing FFNNs analyses. We discuss various designs to ensure the
scalability and accuracy of the conversions. We illustrate our method on a
study case of node classification. We believe that our approach opens new
research directions for understanding and analyzing GNNs.",2
409,"This paper develops a method to learn optimal controls from data for bilinear
systems without a priori knowledge of the system dynamics. Given an unknown
bilinear system, we first characterize when the available data is suitable to
solve the optimal control problem. This characterization leads us to propose an
online control experiment design procedure that guarantees that any input/state
trajectory can be represented as a linear combination of collected input/state
data matrices. Leveraging this data-based representation, we transform the
original optimal control problem into an equivalent data-based optimization
problem with bilinear constraints. We solve the latter by iteratively employing
a convex-concave procedure to convexify it and find a locally optimal control
sequence. Simulations show that the performance of the proposed data-based
approach is comparable with model-based methods.",2
410,"In recent times, a large number of people have been involved in establishing
their own businesses. Unlike humans, chatbots can serve multiple customers at a
time, are available 24/7 and reply in less than a fraction of a second. Though
chatbots perform well in task-oriented activities, in most cases they fail to
understand personalized opinions, statements or even queries which later impact
the organization for poor service management. Lack of understanding
capabilities in bots disinterest humans to continue conversations with them.
Usually, chatbots give absurd responses when they are unable to interpret a
user's text accurately. Extracting the client reviews from conversations by
using chatbots, organizations can reduce the major gap of understanding between
the users and the chatbot and improve their quality of products and
services.Thus, in our research we incorporated all the key elements that are
necessary for a chatbot to analyse and understand an input text precisely and
accurately. We performed sentiment analysis, emotion detection, intent
classification and named-entity recognition using deep learning to develop
chatbots with humanistic understanding and intelligence. The efficiency of our
approach can be demonstrated accordingly by the detailed analysis.",2
411,"Many social media users prefer consuming content in the form of videos rather
than text. However, in order for content creators to produce videos with a high
click-through rate, much editing is needed to match the footage to the music.
This posts additional challenges for more amateur video makers. Therefore, we
propose a novel attention-based model VMT (Video-Music Transformer) that
automatically generates piano scores from video frames. Using music generated
from models also prevent potential copyright infringements that often come with
using existing music. To the best of our knowledge, there is no work besides
the proposed VMT that aims to compose music for video. Additionally, there
lacks a dataset with aligned video and symbolic music. We release a new dataset
composed of over 7 hours of piano scores with fine alignment between pop music
videos and MIDI files. We conduct experiments with human evaluation on VMT,
SeqSeq model (our baseline), and the original piano version soundtrack. VMT
achieves consistent improvements over the baseline on music smoothness and
video relevance. In particular, with the relevance scores and our case study,
our model has shown the capability of multimodality on frame-level actors'
movement for music generation. Our VMT model, along with the new dataset,
presents a promising research direction toward composing the matching
soundtrack for videos. We have released our code at
https://github.com/linchintung/VMT",2
412,"We present the novel Wasserstein graph clustering for dynamically changing
graphs. The Wasserstein clustering penalizes the topological discrepancy
between graphs. The Wasserstein clustering is shown to outperform the widely
used k-means clustering. The method applied in more accurate determination of
the state spaces of dynamically changing functional brain networks.",2
413,"The recent success of deep learning applications has coincided with those
widely available powerful computational resources for training sophisticated
machine learning models with huge datasets. Nonetheless, training large models
such as convolutional neural networks using model parallelism (as opposed to
data parallelism) is challenging because the complex nature of communication
between model shards makes it difficult to partition the computation
efficiently across multiple machines with an acceptable trade-off. This paper
presents SplitBrain, a high performance distributed deep learning framework
supporting hybrid data and model parallelism. Specifically, SplitBrain provides
layer-specific partitioning that co-locates compute intensive convolutional
layers while sharding memory demanding layers. A novel scalable group
communication is proposed to further improve the training throughput with
reduced communication overhead. The results show that SplitBrain can achieve
nearly linear speedup while saving up to 67\% of memory consumption for data
and model parallel VGG over CIFAR-10.",2
414,"We consider Bayesian optimization of the output of a network of functions,
where each function takes as input the output of its parent nodes, and where
the network takes significant time to evaluate. Such problems arise, for
example, in reinforcement learning, engineering design, and manufacturing.
While the standard Bayesian optimization approach observes only the final
output, our approach delivers greater query efficiency by leveraging
information that the former ignores: intermediate output within the network.
This is achieved by modeling the nodes of the network using Gaussian processes
and choosing the points to evaluate using, as our acquisition function, the
expected improvement computed with respect to the implied posterior on the
objective. Although the non-Gaussian nature of this posterior prevents
computing our acquisition function in closed form, we show that it can be
efficiently maximized via sample average approximation. In addition, we prove
that our method is asymptotically consistent, meaning that it finds a globally
optimal solution as the number of evaluations grows to infinity, thus
generalizing previously known convergence results for the expected improvement.
Notably, this holds even though our method might not evaluate the domain
densely, instead leveraging problem structure to leave regions unexplored.
Finally, we show that our approach dramatically outperforms standard Bayesian
optimization methods in several synthetic and real-world problems.",2
415,"We present the Olsson.wl Mathematica package which aims to find linear
transformations for some classes of multivariable hypergeometric functions. It
is based on a well-known method developed by P. O. M. Olsson in J. Math. Phys.
5, 420 (1964) in order to derive the analytic continuations of the Appell $F_1$
double hypergeometric series from the linear transformations of the Gauss
$_2F_1$ hypergeometric function. We provide a brief description of Olsson's
method and demonstrate the commands of the package, along with examples. We
also provide a companion package, called ROC2.wl and dedicated to the
derivation of the regions of convergence of double hypergeometric series. This
package can be used independently of Olsson.wl.",2
416,"Despite recent advances in modern machine learning algorithms, the opaqueness
of their underlying mechanisms continues to be an obstacle in adoption. To
instill confidence and trust in artificial intelligence systems, Explainable
Artificial Intelligence has emerged as a response to improving modern machine
learning algorithms' explainability. Inductive Logic Programming (ILP), a
subfield of symbolic artificial intelligence, plays a promising role in
generating interpretable explanations because of its intuitive logic-driven
framework. ILP effectively leverages abductive reasoning to generate
explainable first-order clausal theories from examples and background
knowledge. However, several challenges in developing methods inspired by ILP
need to be addressed for their successful application in practice. For example,
existing ILP systems often have a vast solution space, and the induced
solutions are very sensitive to noises and disturbances. This survey paper
summarizes the recent advances in ILP and a discussion of statistical
relational learning and neural-symbolic algorithms, which offer synergistic
views to ILP. Following a critical review of the recent advances, we delineate
observed challenges and highlight potential avenues of further ILP-motivated
research toward developing self-explanatory artificial intelligence systems.",2
417,"Recent advances in deep generative models have led to immense progress in 3D
shape synthesis. While existing models are able to synthesize shapes
represented as voxels, point-clouds, or implicit functions, these methods only
indirectly enforce the plausibility of the final 3D shape surface. Here we
present a 3D shape synthesis framework (SurfGen) that directly applies
adversarial training to the object surface. Our approach uses a differentiable
spherical projection layer to capture and represent the explicit zero
isosurface of an implicit 3D generator as functions defined on the unit sphere.
By processing the spherical representation of 3D object surfaces with a
spherical CNN in an adversarial setting, our generator can better learn the
statistics of natural shape surfaces. We evaluate our model on large-scale
shape datasets, and demonstrate that the end-to-end trained model is capable of
generating high fidelity 3D shapes with diverse topology.",2
418,"We propose an approach to modeling irregularly spaced sequences of discrete
events. We begin with a continuous-time variant of the Transformer, which was
originally formulated (Vaswani et al., 2017) for sequences without timestamps.
We embed a possible event (or other boolean fact) at time $t$ by using
attention over the events that occurred at times $< t$ (and the facts that were
true when they occurred). We control this attention using pattern-matching
logic rules that relate events and facts that share participants. These rules
determine which previous events will be attended to, as well as how to
transform the embeddings of the events and facts into the attentional queries,
keys, and values. Other logic rules describe how to change the set of facts in
response to events. Our approach closely follows Mei et al. (2020a), and adopts
their Datalog Through Time formalism for logic rules. As in that work, a domain
expert first writes a set of logic rules that establishes the set of possible
events and other facts at each time $t$. Each possible event or other fact is
embedded using a neural architecture that is derived from the rules that
established it. Our only difference from Mei et al. (2020a) is that we derive a
flatter, attention-based neural architecture whereas they used a more serial
LSTM architecture. We find that our attention-based approach performs about
equally well on the RoboCup dataset, where the logic rules play an important
role in improving performance. We also compared these two methods with two
previous attention-based methods (Zuo et al., 2020; Zhang et al., 2020a) on
simpler synthetic and real domains without logic rules, and found our proposed
approach to be at least as good, and sometimes better, than each of the other
three methods.",2
419,"Vision transformers (ViT) have recently attracted considerable attentions,
but the huge computational cost remains an issue for practical deployment.
Previous ViT pruning methods tend to prune the model along one dimension
solely, which may suffer from excessive reduction and lead to sub-optimal model
quality. In contrast, we advocate a multi-dimensional ViT compression paradigm,
and propose to harness the redundancy reduction from attention head, neuron and
sequence dimensions jointly. We firstly propose a statistical dependence based
pruning criterion that is generalizable to different dimensions for identifying
deleterious components. Moreover, we cast the multi-dimensional compression as
an optimization, learning the optimal pruning policy across the three
dimensions that maximizes the compressed model's accuracy under a computational
budget. The problem is solved by our adapted Gaussian process search with
expected improvement. Experimental results show that our method effectively
reduces the computational cost of various ViT models. For example, our method
reduces 40\% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models,
outperforming previous state-of-the-arts.",2
420,"A key challenge for AI is to build embodied systems that operate in
dynamically changing environments. Such systems must adapt to changing task
contexts and learn continuously. Although standard deep learning systems
achieve state of the art results on static benchmarks, they often struggle in
dynamic scenarios. In these settings, error signals from multiple contexts can
interfere with one another, ultimately leading to a phenomenon known as
catastrophic forgetting. In this article we investigate biologically inspired
architectures as solutions to these problems. Specifically, we show that the
biophysical properties of dendrites and local inhibitory systems enable
networks to dynamically restrict and route information in a context-specific
manner. Our key contributions are as follows. First, we propose a novel
artificial neural network architecture that incorporates active dendrites and
sparse representations into the standard deep learning framework. Next, we
study the performance of this architecture on two separate benchmarks requiring
task-based adaptation: Meta-World, a multi-task reinforcement learning
environment where a robotic agent must learn to solve a variety of manipulation
tasks simultaneously; and a continual learning benchmark in which the model's
prediction task changes throughout training. Analysis on both benchmarks
demonstrates the emergence of overlapping but distinct and sparse subnetworks,
allowing the system to fluidly learn multiple tasks with minimal forgetting.
Our neural implementation marks the first time a single architecture has
achieved competitive results on both multi-task and continual learning
settings. Our research sheds light on how biological properties of neurons can
inform deep learning systems to address dynamic scenarios that are typically
impossible for traditional ANNs to solve.",2
421,"We consider large-scale Markov decision processes (MDPs) with an unknown cost
function and employ stochastic convex optimization tools to address the problem
of imitation learning, which consists of learning a policy from a finite set of
expert demonstrations.
  We adopt the apprenticeship learning formalism, which carries the assumption
that the true cost function can be represented as a linear combination of some
known features. Existing inverse reinforcement learning algorithms come with
strong theoretical guarantees, but are computationally expensive because they
use reinforcement learning or planning algorithms as a subroutine. On the other
hand, state-of-the-art policy gradient based algorithms (like IM-REINFORCE,
IM-TRPO, and GAIL), achieve significant empirical success in challenging
benchmark tasks, but are not well understood in terms of theory. With an
emphasis on non-asymptotic guarantees of performance, we propose a method that
directly learns a policy from expert demonstrations, bypassing the intermediate
step of learning the cost function, by formulating the problem as a single
convex optimization problem over occupancy measures. We develop a
computationally efficient algorithm and derive high confidence regret bounds on
the quality of the extracted policy, utilizing results from stochastic convex
optimization and recent works in approximate linear programming for solving
forward MDPs.",2
422,"With the growth of web applications, REST APIs have become the primary
communication method between services. In order to ensure system reliability
and security, software quality can be assured by effective testing methods.
Black box fuzz testing is one of the effective methods to perform tests on a
large scale. However, conventional black box fuzz testing generates random data
without judging the quality of the input.
  We implement a black box fuzz testing method for REST APIs. It resolves the
issues of blind mutations without knowing the effectiveness by Test Coverage
Level feedback. We also enhance the mutation strategies by reducing the testing
complexity for REST APIs, generating more appropriate test cases to cover
possible paths.
  We evaluate our method by testing two large open-source projects and 89 bugs
are reported and confirmed. In addition, we find 351 bugs from 64 remote API
services in APIs.guru.
  The work is in https://github.com/iasthc/hsuan-fuzz.",2
423,"Large scale datasets created from crowdsourced labels or openly available
data have become crucial to provide training data for large scale learning
algorithms. While these datasets are easier to acquire, the data are frequently
noisy and unreliable, which is motivating research on weakly supervised
learning techniques. In this paper we propose original ideas that help us to
leverage such datasets in the context of change detection. First, we propose
the guided anisotropic diffusion (GAD) algorithm, which improves semantic
segmentation results using the input images as guides to perform edge
preserving filtering. We then show its potential in two weakly-supervised
learning strategies tailored for change detection. The first strategy is an
iterative learning method that combines model optimisation and data cleansing
using GAD to extract the useful information from a large scale change detection
dataset generated from open vector data. The second one incorporates GAD within
a novel spatial attention layer that increases the accuracy of weakly
supervised networks trained to perform pixel-level predictions from image-level
labels. Improvements with respect to state-of-the-art are demonstrated on 4
different public datasets.",2
424,"Stochastic and soft optimal policies resulting from entropy-regularized
Markov decision processes (ER-MDP) are desirable for exploration and imitation
learning applications. Motivated by the fact that such policies are sensitive
with respect to the state transition probabilities, and the estimation of these
probabilities may be inaccurate, we study a robust version of the ER-MDP model,
where the stochastic optimal policies are required to be robust with respect to
the ambiguity in the underlying transition probabilities. Our work is at the
crossroads of two important schemes in reinforcement learning (RL), namely,
robust MDP and entropy regularized MDP. We show that essential properties that
hold for the non-robust ER-MDP and robust unregularized MDP models also hold in
our settings, making the robust ER-MDP problem tractable. We show how our
framework and results can be integrated into different algorithmic schemes
including value or (modified) policy iteration, which would lead to new robust
RL and inverse RL algorithms to handle uncertainties. Analyses on computational
complexity and error propagation under conventional uncertainty settings are
also provided.",2
425,"We study joint unicast and multigroup multicast transmission in single-cell
massive multiple-input-multiple-output (MIMO) systems, under maximum ratio
transmission. For the unicast transmission, the objective is to maximize the
weighted sum spectral efficiency (SE) of the unicast user terminals (UTs) and
for the multicast transmission the objective is to maximize the minimum SE of
the multicast UTs. These two problems are coupled to each other in a
conflicting manner, due to their shared power resource and interference. To
address this, we formulate a multiobjective optimization problem (MOOP). We
derive the Pareto boundary of the MOOP analytically and determine the values of
the system parameters to achieve any desired Pareto optimal point. Moreover, we
prove that the Pareto region is convex, hence the system should serve the
unicast and multicast UTs at the same time-frequency resource.",2
426,"Cell-free Massive MIMO systems consist of a large number of geographically
distributed access points (APs) that serve users by coherent joint
transmission. Downlink power allocation is important in these systems, to
determine which APs should transmit to which users and with what power. If the
system is implemented correctly, it can deliver a more uniform user performance
than conventional cellular networks. To this end, previous works have shown how
to perform system-wide max-min fairness power allocation when using maximum
ratio precoding. In this paper, we first generalize this method to arbitrary
precoding, and then train a neural network to perform approximately the same
power allocation but with reduced computational complexity. Finally, we train
one neural network per AP to mimic system-wide max-min fairness power
allocation, but using only local information. By learning the structure of the
local propagation environment, this method outperforms the state-of-the-art
distributed power allocation method from the Cell-free Massive MIMO literature.",2
427,"Recently, hyperspectral imaging (HSI) has attracted increasing research
attention, especially for the ones based on a coded aperture snapshot spectral
imaging (CASSI) system. Existing deep HSI reconstruction models are generally
trained on paired data to retrieve original signals upon 2D compressed
measurements given by a particular optical hardware mask in CASSI, during which
the mask largely impacts the reconstruction performance and could work as a
""model hyperparameter"" governing on data augmentations. This mask-specific
training style will lead to a hardware miscalibration issue, which sets up
barriers to deploying deep HSI models among different hardware and noisy
environments. To address this challenge, we introduce mask uncertainty for HSI
with a complete variational Bayesian learning treatment and explicitly model it
through a mask decomposition inspired by real hardware. Specifically, we
propose a novel Graph-based Self-Tuning (GST) network to reason uncertainties
adapting to varying spatial structures of masks among different hardware.
Moreover, we develop a bilevel optimization framework to balance HSI
reconstruction and uncertainty estimation, accounting for the hyperparameter
property of masks. Extensive experimental results and model discussions
validate the effectiveness (over 33/30 dB) of the proposed GST method under two
miscalibration scenarios and demonstrate a highly competitive performance
compared with the state-of-the-art well-calibrated methods. Our code and
pre-trained model are available at
https://github.com/Jiamian-Wang/mask_uncertainty_spectral_SCI",2
428,"Future cellular networks are expected to support new communication paradigms
such as machine-type communication (MTC) services along with human-type
communication (HTC) services. This requires base stations to serve a large
number of devices in relatively short channel coherence intervals which renders
allocation of orthogonal pilot sequence per-device approaches impractical.
Furthermore, the stringent power constraints, place-and-play type connectivity
and various data rate requirements of MTC devices make it impossible for the
traditional cellular architecture to accommodate MTC and HTC services together.
Massive multiple-input-multiple-output (MaMIMO) technology has the potential to
allow the coexistence of HTC and MTC services, thanks to its inherent spatial
multiplexing properties and low transmission power requirements. In this work,
we investigate the performance of a single cell under a shared physical channel
assumption for MTC and HTC services and propose a novel scheme for sharing the
time-frequency resources. The analysis reveals that MaMIMO can significantly
enhance the performance of such a setup and allow the inclusion of MTC services
into the cellular networks without requiring additional resources.",2
429,"Homomorphic encryption (HE) enables secure offloading of computation to the
cloud by providing computation on encrypted data (ciphertexts). HE is based on
noisy encryption schemes such that noise accumulates as we apply more
computation to the data. The limited number of operations applicable to the
data prevents practical applications from exploiting HE. Bootstrapping enables
an unlimited number of operations or fully HE (FHE) by refreshing the
ciphertext. Unfortunately, bootstrapping requires a significant amount of
additional computation and memory bandwidth. Prior works have proposed hardware
accelerators for computation primitives of FHE. However, to the best of our
knowledge, this is the first to propose a hardware FHE accelerator tailored to
support bootstrapping efficiently.
  In particular, we propose BTS -- Bootstrappable, Technology-driven, Secure
accelerator architecture for FHE. We identify the challenges of supporting
bootstrapping in the accelerator and analyze the off-chip memory bandwidth and
computation required. In particular, given the limitations of modern memory
technology, we identify the HE parameter sets that are efficient for FHE
acceleration. Based on the insights from our analysis, we propose BTS that
effectively exploits parallelism innate in HE operations by arranging a massive
number of processing elements in a grid. We present the design and
microarchitecture of BTS, including the network-on-chip that exploits the
deterministic communication pattern. BTS shows 5,556$\times$ and 1,306$\times$
improved execution time on ResNet-20 and logistic regression over CPU, using
373.6mm$^2$ chip area and up to 133.8W of power.",2
430,"This paper compares the sum rates and rate regions achieved by power-domain
NOMA (non-orthogonal multiple access) and standard massive MIMO (multiple-input
multiple-output) techniques. We prove analytically that massive MIMO always
outperforms NOMA in i.i.d.~Rayleigh fading channels, if a sufficient number of
antennas are used at the base stations. The simulation results show that the
crossing point occurs already when having 20-30 antennas, which is far less
than what is considered for the next generation cellular networks.",2
431,"An instance $I$ of the Stable Matching Problem (SMP) is given by a bipartite
graph with a preference list of neighbors for every vertex. A swap in $I$ is
the exchange of two consecutive vertices in a preference list. A swap can be
viewed as a smallest perturbation of $I$. Boehmer et al. (2021) designed a
polynomial-time algorithm to find the minimum number of swaps required to turn
a given maximal matching into a stable matching. To generalize this result to
the many-to-many version of SMP, we introduce a new representation of SMP as an
extended bipartite graph and reduce the problem to submodular minimization. It
is a natural problem to establish computational complexity of deciding whether
at most $k$ swaps are enough to turn $I$ into an instance where one of the
maximum matchings is stable. Using a hardness result of Gupta et al. (2020), we
prove that it is NP-hard to decide whether at most $k$ swaps are enough to turn
$I$ into an instance with a stable perfect matching. Moreover, this problem
parameterized by $k$ is W[1]-hard. We also obtain a lower bound on the running
time for solving the problem using the Exponential Time Hypothesis.",2
432,"Liquid level control is very important in industrial field, where the liquid
level is required, and to prevent overflows. The coupled-tank is a common
system in industrial control processes. The system consists of two tanks
connected together and the liquid flows between them. Tanks contain an inlet
and outlet for each tank. The main principle of controlling this system is to
maintain a constant level of liquid in both tanks when there are an inflow and
outflow of liquid in each tank. To control liquid level in the coupled tank
system, the mathematical model of the system had been derived and evaluated as
a form of linear model. The mathematical model of coupled tank was developed to
apply to both conventional and fuzzy control systems where the dynamic behavior
of the system was considered. When the system had been designed the
corresponding model was implemented in simulation by using Matlab and Simulink
tools.",2
433,"We study the robustness of the synchronization of coupled phase oscillators.
When fluctuations of phase differences in lines caused by disturbance exceed a
certain threshold, the state cannot return to synchrony thus leading to
desynchronization. Our main result is the deviation of explicit formulas of a
variance matrix that characterizes the severity of these fluctuations. We
highlight the utility of these results in two general problems: vulnerable line
identification and network design. We find that the vulnerability of lines can
be encoded by the cycle space of graphs. It is analytically shown that a line
in large-size cycles is more vulnerable than those in small-size cycles and
adding a new line or increasing coupling strength of a line reduces the
vulnerability of the lines in any cycle including this line, while it does not
affect the vulnerability of the other lines.",2
434,"The distributed consensus mechanism is the backbone of the rapidly developing
blockchain network. Blockchain platforms consume vast amounts of electricity
based on the current consensus mechanism of Proof of Work. Here, we point out
an advanced consensus mechanism named Proof of Stake that can eliminate the
extensive energy consumption of the current PoW-based blockchain. We
comprehensively elucidate the current and projected energy consumption and
carbon footprint of the PoW and PoS based Bitcoin and Ethereum blockchain
platforms.",2
435,"Leveraging autonomous systems in safety-critical scenarios requires verifying
their behaviors in the presence of uncertainties and black-box components that
influence the system dynamics. In this article, we develop a framework for
verifying partially-observable, discrete-time dynamical systems with unmodelled
dynamics against temporal logic specifications from a given input-output
dataset. The verification framework employs Gaussian process (GP) regression to
learn the unknown dynamics from the dataset and abstract the continuous-space
system as a finite-state, uncertain Markov decision process (MDP). This
abstraction relies on space discretization and transition probability intervals
that capture the uncertainty due to the error in GP regression by using
reproducible kernel Hilbert space analysis as well as the uncertainty induced
by discretization. The framework utilizes existing model checking tools for
verification of the uncertain MDP abstraction against a given temporal logic
specification. We establish the correctness of extending the verification
results on the abstraction to the underlying partially-observable system. We
show that the computational complexity of the framework is polynomial in the
size of the dataset and discrete abstraction. The complexity analysis
illustrates a trade-off between the quality of the verification results and the
computational burden to handle larger datasets and finer abstractions. Finally,
we demonstrate the efficacy of our learning and verification framework on
several case studies with linear, nonlinear, and switched dynamical systems.",2
436,"While neural networks have shown remarkable success on classification tasks
in terms of average-case performance, they often fail to perform well on
certain groups of the data. Such group information may be expensive to obtain;
thus, recent works in robustness and fairness have proposed ways to improve
worst-group performance even when group labels are unavailable for the training
data. However, these methods generally underperform methods that utilize group
information at training time. In this work, we assume access to a small number
of group labels alongside a larger dataset without group labels. We propose
BARACK, a simple two-step framework to utilize this partial group information
to improve worst-group performance: train a model to predict the missing group
labels for the training data, and then use these predicted group labels in a
robust optimization objective. Theoretically, we provide generalization bounds
for our approach in terms of the worst-group performance, showing how the
generalization error scales with respect to both the total number of training
points and the number of training points with group labels. Empirically, our
method outperforms the baselines that do not use group information, even when
only 1-33% of points have group labels. We provide ablation studies to support
the robustness and extensibility of our framework.",2
437,"Deep Reinforcement Learning (RL) is a powerful framework for solving complex
real-world problems. Large neural networks employed in the framework are
traditionally associated with better generalization capabilities, but their
increased size entails the drawbacks of extensive training duration,
substantial hardware resources, and longer inference times. One way to tackle
this problem is to prune neural networks leaving only the necessary parameters.
State-of-the-art concurrent pruning techniques for imposing sparsity perform
demonstrably well in applications where data distributions are fixed. However,
they have not yet been substantially explored in the context of RL. We close
the gap between RL and single-shot pruning techniques and present a general
pruning approach to the Offline RL. We leverage a fixed dataset to prune neural
networks before the start of RL training. We then run experiments varying the
network sparsity level and evaluating the validity of pruning at initialization
techniques in continuous control tasks. Our results show that with 95% of the
network weights pruned, Offline-RL algorithms can still retain performance in
the majority of our experiments. To the best of our knowledge, no prior work
utilizing pruning in RL retained performance at such high levels of sparsity.
  Moreover, pruning at initialization techniques can be easily integrated into
any existing Offline-RL algorithms without changing the learning objective.",2
438,"Order-independent transparency schemes rely on low-order approximations of
transmittance as a function of depth. We introduce a new wavelet representation
of this function and an algorithm for building and evaluating it efficiently on
a GPU. We then extend the order-independent Phenomenological Transparency
algorithm to our representation and introduce a new phenomenological
approximation of chromatic aberration under refraction. This generates
comparable image quality to reference A-buffering for challenging cases such as
smoke coverage, more realistic refraction, and comparable or better performance
and bandwidth to the state-of-the-art Moment transparency with a simpler
implementation.",2
439,"An improvement in technology is linearly related to time and time-relevant
problems. It has been seen that as time progresses, the number of problems
humans face also increases. However, technology to resolve these problems tends
to improve as well. One of the earliest existing problems which started with
the invention of vehicles was parking. The ease of resolving this problem using
technology has evolved over the years but the problem of parking still remains
unsolved. The main reason behind this is that parking does not only involve one
problem but it consists of a set of problems within itself. One of these
problems is the occupancy detection of the parking slots in a distributed
parking ecosystem. In a distributed system, users would find preferable parking
spaces as opposed to random parking spaces. In this paper, we propose a
web-based application as a solution for parking space detection in different
parking spaces. The solution is based on Computer Vision (CV) and is built
using the Django framework written in Python 3.0. The solution works to resolve
the occupancy detection problem along with providing the user the option to
determine the block based on availability and his preference. The evaluation
results for our proposed system are promising and efficient. The proposed
system can also be integrated with different systems and be used for solving
other relevant parking problems.",2
440,"Data privacy is a major issue for many decades, several techniques have been
developed to make sure individuals' privacy but still world has seen privacy
failures. In 2006, Cynthia Dwork gave the idea of Differential Privacy which
gave strong theoretical guarantees for data privacy. Many companies and
research institutes developed differential privacy libraries, but in order to
get the differentially private results, users have to tune the privacy
parameters. In this paper, we minimized these tune-able parameters. The
DP-framework is developed which compares the differentially private results of
three Python based DP libraries. We also introduced a new very simple DP
library (GRAM-DP), so the people with no background of differential privacy can
still secure the privacy of the individuals in the dataset while releasing
statistical results in public.",2
441,"This paper introduces a new framework to predict visual attention of
omnidirectional images. The key setup of our architecture is the simultaneous
prediction of the saliency map and a corresponding scanpath for a given
stimulus. The framework implements a fully encoder-decoder convolutional neural
network augmented by an attention module to generate representative saliency
maps. In addition, an auxiliary network is employed to generate probable
viewport center fixation points through the SoftArgMax function. The latter
allows to derive fixation points from feature maps. To take advantage of the
scanpath prediction, an adaptive joint probability distribution model is then
applied to construct the final unbiased saliency map by leveraging the encoder
decoder-based saliency map and the scanpath-based saliency heatmap. The
proposed framework was evaluated in terms of saliency and scanpath prediction,
and the results were compared to state-of-the-art methods on Salient360!
dataset. The results showed the relevance of our framework and the benefits of
such architecture for further omnidirectional visual attention prediction
tasks.",2
442,"Deep neural networks are vulnerable to adversarial examples, which can fool
deep models by adding subtle perturbations. Although existing attacks have
achieved promising results, it still leaves a long way to go for generating
transferable adversarial examples under the black-box setting. To this end,
this paper proposes to improve the transferability of adversarial examples, and
applies dual-stage feature-level perturbations to an existing model to
implicitly create a set of diverse models. Then these models are fused by the
longitudinal ensemble during the iterations. The proposed method is termed
Dual-Stage Network Erosion (DSNE). We conduct comprehensive experiments both on
non-residual and residual networks, and obtain more transferable adversarial
examples with the computational cost similar to the state-of-the-art method. In
particular, for the residual networks, the transferability of the adversarial
examples can be significantly improved by biasing the residual block
information to the skip connections. Our work provides new insights into the
architectural vulnerability of neural networks and presents new challenges to
the robustness of neural networks.",2
443,"Endoscopic images typically contain several artifacts. The artifacts
significantly impact image analysis result in computer-aided diagnosis.
Convolutional neural networks (CNNs), a type of deep learning, can removes such
artifacts. Various architectures have been proposed for the CNNs, and the
accuracy of artifact removal varies depending on the choice of architecture.
Therefore, it is necessary to determine the artifact removal accuracy,
depending on the selected architecture. In this study, we focus on endoscopic
surgical instruments as artifacts, and determine and discuss the artifact
removal accuracy using seven different CNN architectures.",2
444,"Training deep models for RGB-D salient object detection (SOD) often requires
a large number of labeled RGB-D images. However, RGB-D data is not easily
acquired, which limits the development of RGB-D SOD techniques. To alleviate
this issue, we present a Dual-Semi RGB-D Salient Object Detection Network
(DS-Net) to leverage unlabeled RGB images for boosting RGB-D saliency
detection. We first devise a depth decoupling convolutional neural network
(DDCNN), which contains a depth estimation branch and a saliency detection
branch. The depth estimation branch is trained with RGB-D images and then used
to estimate the pseudo depth maps for all unlabeled RGB images to form the
paired data. The saliency detection branch is used to fuse the RGB feature and
depth feature to predict the RGB-D saliency. Then, the whole DDCNN is assigned
as the backbone in a teacher-student framework for semi-supervised learning.
Moreover, we also introduce a consistency loss on the intermediate attention
and saliency maps for the unlabeled data, as well as a supervised depth and
saliency loss for labeled data. Experimental results on seven widely-used
benchmark datasets demonstrate that our DDCNN outperforms state-of-the-art
methods both quantitatively and qualitatively. We also demonstrate that our
semi-supervised DS-Net can further improve the performance, even when using an
RGB image with the pseudo depth map.",2
445,"Preliminary mission design requires an efficient and accurate approximation
to the low-thrust rendezvous trajectories, which might be generally
three-dimensional and involve multiple revolutions. In this paper, a new
shaping method using cubic spline functions is developed for the analytical
approximation, which shows advantages in the optimality and computational
efficiency. The rendezvous constraints on the boundary states and transfer time
are all satisfied analytically, under the assumption that the boundary
conditions and segment numbers of cubic spline functions are designated in
advance. Two specific shapes are then formulated according to whether they have
free optimization parameters. The shape without free parameters provides an
efficient and robust estimation, while the other one allows a subsequent
optimization for the satisfaction of additional constraints such as the
constraint on the thrust magnitude. Applications of the proposed method in
combination with the particle swarm optimization algorithm are discussed
through two typical interplanetary rendezvous missions, that is, an inclined
multi-revolution trajectory from the Earth to asteroid Dionysus and a
multi-rendezvous trajectory of sample return. Simulation examples show that the
proposed method is superior to existing methods in terms of providing good
estimation for the global search and generating suitable initial guess for the
subsequent trajectory optimization.",2
446,"Meta-learning traditionally relies on backpropagation through entire tasks to
iteratively improve a model's learning dynamics. However, this approach is
computationally intractable when scaled to complex tasks. We propose a
distributed evolutionary meta-learning strategy using Tensor Processing Units
(TPUs) that is highly parallel and scalable to arbitrarily long tasks with no
increase in memory cost. Using a Prototypical Network trained with evolution
strategies on the Omniglot dataset, we achieved an accuracy of 98.4% on a
5-shot classification problem. Our algorithm used as much as 40 times less
memory than automatic differentiation to compute the gradient, with the
resulting model achieving accuracy within 1.3% of a backpropagation-trained
equivalent (99.6%). We observed better classification accuracy as high as 99.1%
with larger population configurations. We further experimentally validate the
stability and performance of ES-ProtoNet across a variety of training
conditions (varying population size, model size, number of workers, shot, way,
ES hyperparameters, etc.). Our contributions are twofold: we provide the first
assessment of evolutionary meta-learning in a supervised setting, and create a
general framework for distributed evolution strategies on TPUs.",2
447,"Over the past decade, fake news and misinformation have turned into a major
problem that has impacted different aspects of our lives, including politics
and public health. Inspired by natural human behavior, we present an approach
that automates the detection of fake news. Natural human behavior is to
cross-check new information with reliable sources. We use Natural Language
Processing (NLP) and build a machine learning (ML) model that automates the
process of cross-checking new information with a set of predefined reliable
sources. We implement this for Twitter and build a model that flags fake
tweets. Specifically, for a given tweet, we use its text to find relevant news
from reliable news agencies. We then train a Random Forest model that checks if
the textual content of the tweet is aligned with the trusted news. If it is
not, the tweet is classified as fake. This approach can be generally applied to
any kind of information and is not limited to a specific news story or a
category of information. Our implementation of this approach gives a $70\%$
accuracy which outperforms other generic fake-news classification models. These
results pave the way towards a more sensible and natural approach to fake news
detection.",2
448,"In this paper we study some theoretical and numerical issues of the
Boussinesq/Full dispersion system. This is a a three-parameter system of pde's
that models the propagation of internal waves along the interface of two-fluid
layers with rigid lid condition for the upper layer, and under a Boussinesq
regime for the upper layer and a full dispersion regime for the lower layer. We
first discretize in space the periodic initial-value problem with a
Fourier-Galerkin spectral method and prove error estimates for several ranges
of values of the parameters. Solitary waves of the model systems are then
studied numerically in several ways. The numerical generation is analyzed by
approximating the ode system with periodic boundary conditions for the
solitary-wave profiles with a Fourier spectral scheme, implemented in a
collocation form, and solving iteratively the corresponding algebraic system in
Fourier space with the Petviashvili method accelerated with the minimal
polynomial extrapolation technique. Motivated by the numerical results, a new
result of existence of solitary waves is proved. In the last part of the paper,
the dynamics of these solitary waves is studied computationally, To this end,
the semidiscrete systems obtained from the Fourier-Galerkin discretization in
space are integrated numerically in time by a Runge-Kutta Composition method of
order four. The fully discrete scheme is used to explore numerically the
stability of solitary waves, their collisions, and the resolution of other
initial conditions into solitary waves.",2
449,"Neural machine translation (NMT) systems aim to map text from one language
into another. While there are a wide variety of applications of NMT, one of the
most important is translation of natural language. A distinguishing factor of
natural language is that words are typically ordered according to the rules of
the grammar of a given language. Although many advances have been made in
developing NMT systems for translating natural language, little research has
been done on understanding how the word ordering of and lexical similarity
between the source and target language affect translation performance. Here, we
investigate these relationships on a variety of low-resource language pairs
from the OpenSubtitles2016 database, where the source language is English, and
find that the more similar the target language is to English, the greater the
translation performance. In addition, we study the impact of providing NMT
models with part of speech of words (POS) in the English sequence and find
that, for Transformer-based models, the more dissimilar the target language is
from English, the greater the benefit provided by POS.",2
450,"Popular online platforms such as Google Search have the capacity to expose
billions of users to partisan and unreliable news. Yet, the content they show
real users is understudied due to the technical challenges of independently
obtaining such data, and the lack of data sharing agreements that include it.
Here we advance on existing digital trace methods using a two-wave study in
which we captured not only the URLs participants clicked on while browsing the
web (engagement), but also the URLs they saw while using Google Search
(exposure). Using surveys paired with engagement and exposure data collected
around the 2018 and 2020 US elections, we found that strong Republicans engaged
with more partisan and unreliable news than strong Democrats did, despite the
two groups being exposed to similar amounts of partisan and unreliable news in
their Google search results. Our results suggest the search engine is not
pushing strong partisans into filter bubbles, but strong Republicans are
asymmetrically selecting into echo chambers. These findings hold across both
study waves, align with work on social media and web browsing, and provide a
rare look at the relationship between exposure and engagement. Our research
highlights the importance of users' choices, and our approach moves the field
closer to the independent, longitudinal, and cross-platform studies it needs to
evaluate the impact of online search and social media platforms.",2
451,"Sparse stealth attack constructions that minimize the mutual information
between the state variables and the observations are proposed. The attack
construction is formulated as the design of a multivariate Gaussian
distribution that aims to minimize the mutual information while limiting the
Kullback-Leibler divergence between the distribution of the observations under
attack and the distribution of the observations without attack. The sparsity
constraint is incorporated as a support constraint of the attack distribution.
Two heuristic greedy algorithms for the attack construction are proposed. The
first algorithm assumes that the attack vector consists of independent entries,
and therefore, requires no communication between different attacked locations.
The second algorithm considers correlation between the attack vector entries
and achieves a better disruption to stealth tradeoff at the cost of requiring
communication between different locations. We numerically evaluate the
performance of the proposed attack constructions on IEEE test systems and show
that it is feasible to construct stealth attacks that generate significant
disruption with a low number of compromised sensors.",2
452,"The biggest challenge of building chatbots is training data. The required
data must be realistic and large enough to train chatbots. We create a tool to
get actual training data from Facebook messenger of a Facebook page. After text
preprocessing steps, the newly obtained dataset generates FVnC and Sample
dataset. We use the Retraining of BERT for Vietnamese (PhoBERT) to extract
features of our text data. K-Means and DBSCAN clustering algorithms are used
for clustering tasks based on output embeddings from PhoBERT$_{base}$. We apply
V-measure score and Silhouette score to evaluate the performance of clustering
algorithms. We also demonstrate the efficiency of PhoBERT compared to other
models in feature extraction on the Sample dataset and wiki dataset. A
GridSearch algorithm that combines both clustering evaluations is also proposed
to find optimal parameters. Thanks to clustering such a number of
conversations, we save a lot of time and effort to build data and storylines
for training chatbot.",2
453,"Zero-shot object detection aims at incorporating class semantic vectors to
realize the detection of (both seen and) unseen classes given an unconstrained
test image. In this study, we reveal the core challenges in this research area:
how to synthesize robust region features (for unseen objects) that are as
intra-class diverse and inter-class separable as the real samples, so that
strong unseen object detectors can be trained upon them. To address these
challenges, we build a novel zero-shot object detection framework that contains
an Intra-class Semantic Diverging component and an Inter-class Structure
Preserving component. The former is used to realize the one-to-more mapping to
obtain diverse visual features from each class semantic vector, preventing
miss-classifying the real unseen objects as image backgrounds. While the latter
is used to avoid the synthesized features too scattered to mix up the
inter-class and foreground-background relationship. To demonstrate the
effectiveness of the proposed approach, comprehensive experiments on PASCAL
VOC, COCO, and DIOR datasets are conducted. Notably, our approach achieves the
new state-of-the-art performance on PASCAL VOC and COCO and it is the first
study to carry out zero-shot object detection in remote sensing imagery.",2
454,"Emerging edge applications require both a fast response latency and complex
processing. This is infeasible without expensive hardware that can process
complex operations -- such as object detection -- within a short time. Many
approach this problem by addressing the complexity of the models -- via model
compression, pruning and quantization -- or compressing the input. In this
paper, we propose a different perspective when addressing the performance
challenges. Croesus is a multi-stage approach to edge-cloud systems that
provides the ability to find the balance between accuracy and performance.
Croesus consists of two stages (that can be generalized to multiple stages): an
initial and a final stage. The initial stage performs the computation in
real-time using approximate/best-effort computation at the edge. The final
stage performs the full computation at the cloud, and uses the results to
correct any errors made at the initial stage. In this paper, we demonstrate the
implications of such an approach on a video analytics use-case and show how
multi-stage processing yields a better balance between accuracy and
performance. Moreover, we study the safety of multi-stage transactions via two
proposals: multi-stage serializability (MS-SR) and multi-stage invariant
confluence with Apologies (MS-IA).",2
455,"Dynamic program slicing can significantly reduce the code developers need to
inspect by narrowing it down to only a subset of relevant program statements.
However, despite an extensive body of research showing its usefulness, dynamic
slicing is still short from production-level use due to the high cost of
runtime instrumentation.
  As an alternative, we propose statistical program slicing, a novel hybrid
dynamic-static slicing technique that explores the trade-off between accuracy
and runtime cost. Our approach relies on modern hardware support for control
flow monitoring and a novel, cooperative heap memory tracing mechanism combined
with static program analysis for data flow tracking. We evaluate statistical
slicing for debugging on 21 failures from 6 widely deployed applications and
show it recovers 94% of the program statements on a dynamic slice with only 5%
overhead.",2
456,"This paper proposes a category-level 6D object pose and shape estimation
approach iCaps, which allows tracking 6D poses of unseen objects in a category
and estimating their 3D shapes. We develop a category-level auto-encoder
network using depth images as input, where feature embeddings from the
auto-encoder encode poses of objects in a category. The auto-encoder can be
used in a particle filter framework to estimate and track 6D poses of objects
in a category. By exploiting an implicit shape representation based on signed
distance functions, we build a LatentNet to estimate a latent representation of
the 3D shape given the estimated pose of an object. Then the estimated pose and
shape can be used to update each other in an iterative way. Our category-level
6D object pose and shape estimation pipeline only requires 2D detection and
segmentation for initialization. We evaluate our approach on a publicly
available dataset and demonstrate its effectiveness. In particular, our method
achieves comparably high accuracy on shape estimation.",2
457,"Comparison of data representations is a complex multi-aspect problem that has
not enjoyed a complete solution yet. We propose a method for comparing two data
representations. We introduce the Representation Topology Divergence (RTD),
measuring the dissimilarity in multi-scale topology between two point clouds of
equal size with a one-to-one correspondence between points. The data point
clouds are allowed to lie in different ambient spaces. The RTD is one of the
few TDA-based practical methods applicable to real machine learning datasets.
Experiments show that the proposed RTD agrees with the intuitive assessment of
data representation similarity and is sensitive to its topological structure.
We apply RTD to gain insights on neural networks representations in computer
vision and NLP domains for various problems: training dynamics analysis, data
distribution shift, transfer learning, ensemble learning, disentanglement
assessment.",2
458,"Machine learning systems often experience a distribution shift between
training and testing. In this paper, we introduce a simple variational
objective whose optima are exactly the set of all representations on which risk
minimizers are guaranteed to be robust to any distribution shift that preserves
the Bayes predictor, e.g., covariate shifts. Our objective has two components.
First, a representation must remain discriminative for the task, i.e., some
predictor must be able to simultaneously minimize the source and target risk.
Second, the representation's marginal support needs to be the same across
source and target. We make this practical by designing self-supervised learning
methods that only use unlabelled data and augmentations to train robust
representations. Our objectives achieve state-of-the-art results on DomainBed,
and give insights into the robustness of recent methods, such as CLIP.",2
459,"Despite advances in deep algorithmic music generation, evaluation of
generated samples often relies on human evaluation, which is subjective and
costly. We focus on designing a homogeneous, objective framework for evaluating
samples of algorithmically generated music. Any engineered measures to evaluate
generated music typically attempt to define the samples' musicality, but do not
capture qualities of music such as theme or mood. We do not seek to assess the
musical merit of generated music, but instead explore whether generated samples
contain meaningful information pertaining to emotion or mood/theme. We achieve
this by measuring the change in predictive performance of a music mood/theme
classifier after augmenting its training data with generated samples. We
analyse music samples generated by three models -- SampleRNN, Jukebox, and DDSP
-- and employ a homogeneous framework across all methods to allow for objective
comparison. This is the first attempt at augmenting a music genre
classification dataset with conditionally generated music. We investigate the
classification performance improvement using deep music generation and the
ability of the generators to make emotional music by using an additional,
emotion annotation of the dataset. Finally, we use a classifier trained on real
data to evaluate the label validity of class-conditionally generated samples.",2
460,"Object motion and object appearance are commonly used information in multiple
object tracking (MOT) applications, either for associating detections across
frames in tracking-by-detection methods or direct track predictions for
joint-detection-and-tracking methods. However, not only are these two types of
information often considered separately, but also they do not help optimize the
usage of visual information from the current frame of interest directly. In
this paper, we present PatchTrack, a Transformer-based
joint-detection-and-tracking system that predicts tracks using patches of the
current frame of interest. We use the Kalman filter to predict the locations of
existing tracks in the current frame from the previous frame. Patches cropped
from the predicted bounding boxes are sent to the Transformer decoder to infer
new tracks. By utilizing both object motion and object appearance information
encoded in patches, the proposed method pays more attention to where new tracks
are more likely to occur. We show the effectiveness of PatchTrack on recent MOT
benchmarks, including MOT16 (MOTA 73.71%, IDF1 65.77%) and MOT17 (MOTA 73.59%,
IDF1 65.23%). The results are published on
https://motchallenge.net/method/MOT=4725&chl=10.",2
461,"Occlusion poses a major challenge for person re-identification (ReID).
Existing approaches typically rely on outside tools to infer visible body
parts, which may be suboptimal in terms of both computational efficiency and
ReID accuracy. In particular, they may fail when facing complex occlusions,
such as those between pedestrians. Accordingly, in this paper, we propose a
novel method named Quality-aware Part Models (QPM) for occlusion-robust ReID.
First, we propose to jointly learn part features and predict part quality
scores. As no quality annotation is available, we introduce a strategy that
automatically assigns low scores to occluded body parts, thereby weakening the
impact of occluded body parts on ReID results. Second, based on the predicted
part quality scores, we propose a novel identity-aware spatial attention (ISA)
module. In this module, a coarse identity-aware feature is utilized to
highlight pixels of the target pedestrian, so as to handle the occlusion
between pedestrians. Third, we design an adaptive and efficient approach for
generating global features from common non-occluded regions with respect to
each image pair. This design is crucial, but is often ignored by existing
methods. QPM has three key advantages: 1) it does not rely on any outside tools
in either the training or inference stages; 2) it handles occlusions caused by
both objects and other pedestrians;3) it is highly computationally efficient.
Experimental results on four popular databases for occluded ReID demonstrate
that QPM consistently outperforms state-of-the-art methods by significant
margins. The code of QPM will be released.",2
462,"Nowadays, sharding is deemed as a promising way to save traditional
blockchain protocols from their low scalability. However, such technique also
brings several potential risks and huge communication overheads. An improper
design may give rise to the inconsistent state among different committees.
Further, the communication overheads arising from cross-shard transactions
unfortunately reduce the system's performance. In this paper, we first
summarize five essential issues that all sharding blockchain designers face.
For each issue, we discuss its key challenge and propose our suggested
solutions. In order to break the performance bottlenecks, we propose a
reputation mechanism for selecting leaders. The term of reputation in our
design reflects each node's honest computation resources. In addition, we
introduce a referee committee and partial sets in each committee, and design a
recovery procedure in case the leader is malicious. Under the design, we prove
that malicious leaders will not hurt the system and will be evicted.
Furthermore, we conduct a series of simulations to evaluate our design. The
results show that selecting leaders by the reputation can dramatically improve
the system performance.",2
463,"Deep neural networks are parametrized by several thousands or millions of
parameters, and have shown tremendous success in many classification problems.
However, the large number of parameters makes it difficult to integrate these
models into edge devices such as smartphones and wearable devices. To address
this problem, knowledge distillation (KD) has been widely employed, that uses a
pre-trained high capacity network to train a much smaller network, suitable for
edge devices. In this paper, for the first time, we study the applicability and
challenges of using KD for time-series data for wearable devices. Successful
application of KD requires specific choices of data augmentation methods during
training. However, it is not yet known if there exists a coherent strategy for
choosing an augmentation approach during KD. In this paper, we report the
results of a detailed study that compares and contrasts various common choices
and some hybrid data augmentation strategies in KD based human activity
analysis. Research in this area is often limited as there are not many
comprehensive databases available in the public domain from wearable devices.
Our study considers databases from small scale publicly available to one
derived from a large scale interventional study into human activity and
sedentary behavior. We find that the choice of data augmentation techniques
during KD have a variable level of impact on end performance, and find that the
optimal network choice as well as data augmentation strategies are specific to
a dataset at hand. However, we also conclude with a general set of
recommendations that can provide a strong baseline performance across
databases.",2
464,"Inference of causal relations from data now has become an important field in
artificial intelligence. During the past 16 years, causality analysis (in a
quantitative sense) has been developed independently in physics from first
principles. This short note is a brief summary of this line of work, including
part of the theory and several representative applications.",2
465,"Large-scale or high-resolution geologic models usually comprise a huge number
of grid blocks, which can be computationally demanding and time-consuming to
solve with numerical simulators. Therefore, it is advantageous to upscale
geologic models (e.g., hydraulic conductivity) from fine-scale (high-resolution
grids) to coarse-scale systems. Numerical upscaling methods have been proven to
be effective and robust for coarsening geologic models, but their efficiency
remains to be improved. In this work, a deep-learning-based method is proposed
to upscale the fine-scale geologic models, which can assist to improve
upscaling efficiency significantly. In the deep learning method, a deep
convolutional neural network (CNN) is trained to approximate the relationship
between the coarse grid of hydraulic conductivity fields and the hydraulic
heads, which can then be utilized to replace the numerical solvers while
solving the flow equations for each coarse block. In addition, physical laws
(e.g., governing equations and periodic boundary conditions) can also be
incorporated into the training process of the deep CNN model, which is termed
the theory-guided convolutional neural network (TgCNN). With the physical
information considered, dependence on the data volume of training the deep
learning models can be reduced greatly. Several subsurface flow cases are
introduced to test the performance of the proposed deep-learning-based
upscaling method, including 2D and 3D cases, and isotropic and anisotropic
cases. The results show that the deep learning method can provide equivalent
upscaling accuracy to the numerical method, and efficiency can be improved
significantly compared to numerical upscaling.",2
466,"Graph matching is the process of computing the similarity between two graphs.
Depending on the requirement, it can be exact or inexact. Exact graph matching
requires a strict correspondence between nodes of two graphs, whereas inexact
matching allows some flexibility or tolerance during the graph matching. In
this chapter, we describe an approximate inexact graph matching by reducing the
size of the graphs using different centrality measures. Experimental evaluation
shows that it can reduce running time for inexact graph matching.",2
467,"In recent years, significant progress has been made on the research of crowd
counting. However, as the challenging scale variations and complex scenes
existed in crowds, neither traditional convolution networks nor recent
Transformer architectures with fixed-size attention could handle the task well.
To address this problem, this paper proposes a scene-adaptive attention
network, termed SAANet. First of all, we design a deformable attention in-built
Transformer backbone, which learns adaptive feature representations with
deformable sampling locations and dynamic attention weights. Then we propose
the multi-level feature fusion and count-attentive feature enhancement modules
further, to strengthen feature representation under the global image context.
The learned representations could attend to the foreground and are adaptive to
different scales of crowds. We conduct extensive experiments on four
challenging crowd counting benchmarks, demonstrating that our method achieves
state-of-the-art performance. Especially, our method currently ranks No.1 on
the public leaderboard of the NWPU-Crowd benchmark. We hope our method could be
a strong baseline to support future research in crowd counting. The source code
will be released to the community.",2
468,"Understanding how the predictions of deep learning models are formed during
the training process is crucial to improve model performance and fix model
defects, especially when we need to investigate nontrivial training strategies
such as active learning, and track the root cause of unexpected training
results such as performance degeneration.
  In this work, we propose a time-travelling visual solution DeepVisualInsight
(DVI), aiming to manifest the spatio-temporal causality while training a deep
learning image classifier. The spatio-temporal causality demonstrates how the
gradient-descent algorithm and various training data sampling techniques can
influence and reshape the layout of learnt input representation and the
classification boundaries in consecutive epochs. Such causality allows us to
observe and analyze the whole learning process in the visible low dimensional
space. Technically, we propose four spatial and temporal properties and design
our visualization solution to satisfy them. These properties preserve the most
important information when inverse-)projecting input samples between the
visible low-dimensional and the invisible high-dimensional space, for causal
analyses. Our extensive experiments show that, comparing to baseline
approaches, we achieve the best visualization performance regarding the
spatial/temporal properties and visualization efficiency. Moreover, our case
study shows that our visual solution can well reflect the characteristics of
various training scenarios, showing good potential of DVI as a debugging tool
for analyzing deep learning training processes.",2
469,"The latest advances of statistical physics have shown remarkable performance
of machine learning in identifying phase transitions. In this paper, we apply
domain adversarial neural network (DANN) based on transfer learning to studying
non-equilibrium and equilibrium phase transition models, which are percolation
model and directed percolation (DP) model, respectively. With the DANN, only a
small fraction of input configurations (2d images) needs to be labeled, which
is automatically chosen, in order to capture the critical point. To learn the
DP model, the method is refined by an iterative procedure in determining the
critical point, which is a prerequisite for the data collapse in calculating
the critical exponent $\nu_{\perp}$. We then apply the DANN to a
two-dimensional site percolation with configurations filtered to include only
the largest cluster which may contain the information related to the order
parameter. The DANN learning of both models yields reliable results which are
comparable to the ones from Monte Carlo simulations. Our study also shows that
the DANN can achieve quite high accuracy at much lower cost, compared to the
supervised learning.",2
470,"Search engines based on keyword retrieval can no longer adapt to the way of
information acquisition in the era of intelligent Internet of Things due to the
return of keyword related Internet pages. How to quickly, accurately and
effectively obtain the information needed by users from massive Internet data
has become one of the key issues urgently needed to be solved. We propose an
intelligent question-answering system based on structured KB and unstructured
data, called OpenQA, in which users can give query questions and the model can
quickly give accurate answers back to users. We integrate KBQA structured
question answering based on semantic parsing and deep representation learning,
and two-stage unstructured question answering based on retrieval and neural
machine reading comprehension into OpenQA, and return the final answer with the
highest probability through the Transformer answer selection module in OpenQA.
We carry out preliminary experiments on our constructed dataset, and the
experimental results prove the effectiveness of the proposed intelligent
question answering system. At the same time, the core technology of each module
of OpenQA platform is still in the forefront of academic hot spots, and the
theoretical essence and enrichment of OpenQA will be further explored based on
these academic hot spots.",2
471,"Globally, there is a substantial unmet need to diagnose various diseases
effectively. The complexity of the different disease mechanisms and underlying
symptoms of the patient population presents massive challenges to developing
the early diagnosis tool and effective treatment. Machine Learning (ML), an
area of Artificial Intelligence (AI), enables researchers, physicians, and
patients to solve some of these issues. Based on relevant research, this review
explains how Machine Learning (ML) and Deep Learning (DL) are being used to
help in the early identification of numerous diseases. To begin, a bibliometric
study of the publication is given using data from the Scopus and Web of Science
(WOS) databases. The bibliometric study of 1216 publications was undertaken to
determine the most prolific authors, nations, organizations, and most cited
articles. The review then summarizes the most recent trends and approaches in
Machine Learning-based Disease Diagnosis (MLBDD), considering the following
factors: algorithm, disease types, data type, application, and evaluation
metrics. Finally, the paper highlights key results and provides insight into
future trends and opportunities in the MLBDD area.",2
472,"We prove complex contraction for zero-free regions of counting weighted set
cover problem in which an element can appear in an unbounded number of sets,
thus obtaining fully polynomial-time approximation schemes(FPTAS) via
Barvinok's algorithmic paradigm\cite{barvinok2016combinatorics}. Relying on the
computation tree expansion, our approach does not need proof of correlation
decay in the real axis. We directly look in the complex plane for a region that
contracts into its interior as the tree recursion procedure goes from leaves to
the root.
  For the class of problems under the framework of weighted set covers, we are
able to give a general approach for describing the contraction regions and draw
a unified algorithmic conclusion. Several previous results, including counting
(weighted-)edge covers, counting bipartite independent sets and counting
monotone CNFs can be completely or partially covered by our main theorem. In
contrast to the correlation decay method which also depends on tree expansions
and needs different potential functions for different problems, our approach is
more generic in the sense that our contraction region for different problems
shares a common shape in the complex plane.",2
473,"Although Terahertz communication systems can provide high data rates, it
needs high directional beamforming at transmitters and receivers to achieve
such rates over a long distance. Therefore, an efficient beam training method
is vital to accelerate the link establishment. In this study, we propose a
low-complexity beam training scheme of terahertz communication system which
uses a low-cost small-scale hybrid architecture to assist a large-scale array
for data transmission. The proposed scheme includes two key stages: (1) coarse
AoAs/AoDs estimation for beam subset optimization in auxiliary array stage, and
(2) accurate AoAs/AoDs estimation by exploiting channel sparsity in data
transmission array stage. The analysis shows that the complexity of the scheme
is linear with the number of main paths, and thus greatly reduces the
complexity of beam training. Simulation results have verified the better
performance in spectral efficiency of the proposed scheme than that of the
related work.",2
474,"Graph neural networks (GNN) have shown great success in learning from
graph-structured data. They are widely used in various applications, such as
recommendation, fraud detection, and search. In these domains, the graphs are
typically large, containing hundreds of millions or billions of nodes. To
tackle this challenge, we develop DistDGLv2, a system that extends DistDGL for
training GNNs in a mini-batch fashion, using distributed hybrid CPU/GPU
training to scale to large graphs. DistDGLv2 places graph data in distributed
CPU memory and performs mini-batch computation in GPUs. DistDGLv2 distributes
the graph and its associated data (initial features) across the machines and
uses this distribution to derive a computational decomposition by following an
owner-compute rule. DistDGLv2 follows a synchronous training approach and
allows ego-networks forming mini-batches to include non-local nodes. To
minimize the overheads associated with distributed computations, DistDGLv2 uses
a multi-level graph partitioning algorithm with min-edge cut along with
multiple balancing constraints. This localizes computation in both machine
level and GPU level and statically balance the computations. DistDGLv2 deploys
an asynchronous mini-batch generation pipeline that makes all computation and
data access asynchronous to fully utilize all hardware (CPU, GPU, network,
PCIe). The combination allows DistDGLv2 to train high-quality models while
achieving high parallel efficiency and memory scalability. We demonstrate
DistDGLv2 on various GNN workloads. Our results show that DistDGLv2 achieves
2-3X speedup over DistDGL and 18X speedup over Euler. It takes only 5-10
seconds to complete an epoch on graphs with 100s millions of nodes on a cluster
with 64 GPUs.",2
475,"Bounding-box annotation form has been the most frequently used method for
visual object localization tasks. However, bounding-box annotation relies on a
large amount of precisely annotating bounding boxes, and it is expensive and
laborious. It is impossible to be employed in practical scenarios and even
redundant for some applications (such as tiny person localization) that the
size would not matter. Therefore, we propose a novel point-based framework for
the person localization task by annotating each person as a coarse point
(CoarsePoint) instead of an accurate bounding box that can be any point within
the object extent. Then, the network predicts the person's location as a 2D
coordinate in the image. Although this greatly simplifies the data annotation
pipeline, the CoarsePoint annotation inevitably decreases label reliability
(label uncertainty) and causes network confusion during training. As a result,
we propose a point self-refinement approach that iteratively updates point
annotations in a self-paced way. The proposed refinement system alleviates the
label uncertainty and progressively improves localization performance.
Experimental results show that our approach has achieved comparable object
localization performance while saving up to 80$\%$ of annotation cost.",2
476,"Device activity detection and channel estimation for massive grant-free
access under frequency-selective fading have unfortunately been an outstanding
problem. This paper aims to address the challenge. Specifically, we present an
orthogonal frequency division multiplexing (OFDM)-based massive grant-free
access scheme for a wideband system with one M-antenna base station (BS), N
single-antenna Internet of Things (IoT) devices, and P channel taps. We obtain
two different but equivalent models for the received pilot signals under
frequency-selective Rayleigh fading. Based on each model, we formulate device
activity detection as a non-convex maximum likelihood estimation (MLE) problem
and propose an iterative algorithm to obtain a stationary point using optimal
techniques. The two proposed MLE-based methods have the identical computational
complexity order O(NPL^2), irrespective of M, and degrade to the existing
MLE-based device activity detection method when P=1. Conventional channel
estimation methods can be readily applied for channel estimation of detected
active devices under frequency-selective Rayleigh fading, based on one of the
derived models for the received pilot signals. Numerical results show that the
two proposed methods have different preferable system parameters and complement
each other to offer promising device activity detection design for grant-free
massive access under frequency-selective Rayleigh fading.",2
477,"Log anomaly detection is a key component in the field of artificial
intelligence for IT operations (AIOps). Considering log data of variant
domains, retraining the whole network for unknown domains is inefficient in
real industrial scenarios especially for low-resource domains. However,
previous deep models merely focused on extracting the semantics of log sequence
in the same domain, leading to poor generalization on multi-domain logs.
Therefore, we propose a unified Transformer-based framework for log anomaly
detection (\ourmethod{}), which is comprised of the pretraining and
adapter-based tuning stage. Our model is first pretrained on the source domain
to obtain shared semantic knowledge of log data. Then, we transfer the
pretrained model to the target domain via the adapter-based tuning. The
proposed method is evaluated on three public datasets including one source
domain and two target domains. The experimental results demonstrate that our
simple yet efficient approach, with fewer trainable parameters and lower
training costs in the target domain, achieves state-of-the-art performance on
three benchmarks.",2
478,"We study the properties of ultrametric matrices aiming to design methods for
fast ultrametric matrix-vector multiplication. We show how to encode such a
matrix as a tree structure in quadratic time and demonstrate how to use the
resulting representation to perform matrix-vector multiplications in linear
time. Accompanying this article, we provide an implementation of the proposed
algorithms and present empirical results on their practical performance.",2
479,"We provide a rough sketch of a simple system design for exposure notification
of COVID-19 infections based on copresence at cluster events -- locations and
times where a threshold number of tested-positive (TP) individuals were
present. Unlike other designs, such as DP3T or the Apple-Google
exposure-notification system, this design does not track or notify based on
detecting direct proximity to TP individuals.
  The design makes use of existing or in-development tests for COVID-19 that
are relatively cheap and return results in less than an hour, and that have
high specificity but may have lower sensitivity. It also uses readily available
location tracking for mobile phones and similar devices. It reports events at
which TP individuals were present but does not link events with individuals or
with other events in an individual's history. Participating individuals are
notified of detected cluster events. They can then compare these locally to
their own location history. Detected cluster events can be publicized through
public channels. Thus, individuals not participating in the reporting system
can still be notified of exposure.
  A proper security analysis is beyond the scope of this design sketch. We do,
however, discuss resistance to various adversaries and attacks on privacy as
well as false-reporting attacks.",2
480,"We propose a 3-D material style transfer framework for reconstructing
invisible (or faded) appearance properties in complex natural materials. Our
algorithm addresses the technical challenge of transferring appearance
properties from one object to another of the same material when both objects
have intricate, noncorresponding color patterns. Eggshells, exoskeletons, and
minerals, for example, have patterns composed of highly randomized layers of
organic and inorganic compounds. These materials pose a challenge as the
distribution of compounds that determine surface color changes from object to
object and within local pattern regions. Our solution adapts appearance
observations from a material property distribution in an exemplar to the
material property distribution of a target object to reconstruct its unknown
appearance. We use measured reflectance in 3-D bispectral textures to record
changing material property distributions. Our novel implementation of spherical
harmonics uses principles from chemistry and biology to learn relationships
between color (hue and saturation) and material composition and concentration
in an exemplar. The encoded relationships are transformed to the property
distribution of a target for color recovery and material assignment.
Quantitative and qualitative evaluation methods show that we replicate color
patterns more accurately than methods that only rely on shape correspondences
and coarse-level perceptual differences. We demonstrate applications of our
work for reconstructing color in extinct fossils, restoring faded artifacts and
generating synthetic textures.",2
481,"Large-scale datasets are important for the development of deep learning
models. Such datasets usually require a heavy workload of annotations, which
are extremely time-consuming and expensive. To accelerate the annotation
procedure, multiple annotators may be employed to label different subsets of
the data. However, the inconsistency and bias among different annotators are
harmful to the model training, especially for qualitative and subjective
tasks.To address this challenge, in this paper, we propose a novel contrastive
regression framework to address the disjoint annotations problem, where each
sample is labeled by only one annotator and multiple annotators work on
disjoint subsets of the data. To take account of both the intra-annotator
consistency and inter-annotator inconsistency, two strategies are
employed.Firstly, a contrastive-based loss is applied to learn the relative
ranking among different samples of the same annotator, with the assumption that
the ranking of samples from the same annotator is unanimous. Secondly, we apply
the gradient reversal layer to learn robust representations that are invariant
to different annotators. Experiments on the facial expression prediction task,
as well as the image quality assessment task, verify the effectiveness of our
proposed framework.",2
482,"The theory of eigenvalues and eigenvectors is one of the fundamental and
essential components in tensor analysis. Computing the dominant eigenpair of an
essentially nonnegative tensor is an important topic in tensor computation
because of the critical applications in network resource allocations. In this
paper, we consider the aforementioned topic and there are two main
contributions. First, we show that an irreducible essentially nonnegative
tensor has a unique positive dominant eigenvalue with a unique positive
normalized eigenvector. Second, we present a homotopy method to compute the
dominant eigenpair and prove that it converges to the desired dominant
eigenpair whether the given tensor is irreducible or reducible based on an
approximation technique. Finally, we implement the method using a
prediction-correction approach for path following and some numerical results
are reported to illustrate the efficiency of the proposed algorithm.",2
483,"We hypothesize that empirically studying the sample complexity of offline
reinforcement learning (RL) is crucial for the practical applications of RL in
the real world. Several recent works have demonstrated the ability to learn
policies directly from offline data. In this work, we ask the question of the
dependency on the number of samples for learning from offline data. Our
objective is to emphasize that studying sample complexity for offline RL is
important, and is an indicator of the usefulness of existing offline
algorithms. We propose an evaluation approach for sample complexity analysis of
offline RL.",2
484,"Manufacturing companies face challenges when it comes to quickly adapting
their production control to fluctuating demands or changing requirements.
Control approaches that encapsulate production functions as services have shown
to be promising in order to increase the flexibility of Cyber-Physical
Production Systems. But an existing challenge of such approaches is finding a
production plan based on provided functionalities for a demanded product,
especially when there is no direct (i.e., syntactic) match between demanded and
provided functions. While there is a variety of approaches to production
planning, flexible production poses specific requirements that are not covered
by existing research. In this contribution, we first capture these requirements
for flexible production environments. Afterwards, an overview of current
Artificial Intelligence approaches that can be utilized in order to overcome
the aforementioned challenges is given. For this purpose, we focus on planning
algorithms, but also consider models of production systems that can act as
inputs to these algorithms. Approaches from both symbolic AI planning as well
as approaches based on Machine Learning are discussed and eventually compared
against the requirements. Based on this comparison, a research agenda is
derived.",2
485,"Identifying anomalies refers to detecting samples that do not resemble the
training data distribution. Many generative models have been used to find
anomalies, and among them, generative adversarial network (GAN)-based
approaches are currently very popular. GANs mainly rely on the rich contextual
information of these models to identify the actual training distribution.
Following this analogy, we suggested a new unsupervised model based on GANs --a
combination of an autoencoder and a GAN. Further, a new scoring function was
introduced to target anomalies where a linear combination of the internal
representation of the discriminator and the generator's visual representation,
plus the encoded representation of the autoencoder, come together to define the
proposed anomaly score. The model was further evaluated on benchmark datasets
such as SVHN, CIFAR10, and MNIST, as well as a public medical dataset of
leukemia images. In all the experiments, our model outperformed its existing
counterparts while slightly improving the inference time.",2
486,"We prove in this paper that optimizing wide ReLU neural networks (NNs) with
at least one hidden layer using l2-regularization on the parameters enforces
multi-task learning due to representation-learning - also in the limit of width
to infinity. This is in contrast to multiple other results in the literature,
in which idealized settings are assumed and where wide (ReLU)-NNs loose their
ability to benefit from multi-task learning in the infinite width limit. We
deduce the ability of multi-task learning from proving an exact quantitative
macroscopic characterization of the learned NN in an appropriate function
space.",2
487,"OWLOOP is an Application Programming Interface (API) for using the Ontology
Web Language (OWL) by the means of Object-Oriented Programming (OOP). It is
common to design software architectures using the OOP paradigm for increasing
their modularity. If the components of an architecture also exploit OWL
ontologies for knowledge representation and reasoning, they would require to be
interfaced with OWL axioms. Since OWL does not adhere to the OOP paradigm, such
an interface often leads to boilerplate code affecting modularity, and OWLOOP
is designed to address this issue as well as the associated computational
aspects. We present an extension of the OWL-API to provide a general-purpose
interface between OWL axioms subject to reasoning and modular OOP objects
hierarchies.",2
488,"Deep stereo matching has made significant progress in recent years. However,
state-of-the-art methods are based on expensive 4D cost volume, which limits
their use in real-world applications. To address this issue, 3D correlation
maps and iterative disparity updates have been proposed. Regarding that in
real-world platforms, such as self-driving cars and robots, the Lidar is
usually installed. Thus we further introduce the sparse Lidar point into the
iterative updates, which alleviates the burden of network updating the
disparity from zero states. Furthermore, we propose training the network in a
self-supervised way so that it can be trained on any captured data for better
generalization ability. Experiments and comparisons show that the presented
method is effective and achieves comparable results with related methods.",2
489,"We share our experience with the recently released WILDS benchmark, a
collection of ten datasets dedicated to developing models and training
strategies which are robust to domain shifts. Several experiments yield a
couple of critical observations which we believe are of general interest for
any future work on WILDS. Our study focuses on two datasets: iWildCam and FMoW.
We show that (1) Conducting separate cross-validation for each evaluation
metric is crucial for both datasets, (2) A weak correlation between validation
and test performance might make model development difficult for iWildCam, (3)
Minor changes in the training of hyper-parameters improve the baseline by a
relatively large margin (mainly on FMoW), (4) There is a strong correlation
between certain domains and certain target labels (mainly on iWildCam). To the
best of our knowledge, no prior work on these datasets has reported these
observations despite their obvious importance. Our code is public.",2
490,"The age of information (AoI) performance metric for point-to-point wireless
communication systems is analytically studied under Rician-faded channels and
when the receiver is equipped with multiple antennas. The general scenario of a
non-linear AoI function is considered, which includes the conventional linear
AoI as a special case. The stop-and-wait transmission policy is adopted, where
the source node samples and then transmits new data only upon the successful
reception of previous data. This approach can serve as a performance benchmark
for any queuing system used in practice. New analytical and closed-form
expressions are derived with respect to the average AoI and average peak AoI
for the considered system configuration. We particularly focus on the energy
efficiency of the said mode of operation, whereas some useful engineering
insights are provided.",2
491,"In this paper, we propose a dual-module network architecture that employs a
domain discriminative feature module to encourage the domain invariant feature
module to learn more domain invariant features. The proposed architecture can
be applied to any model that utilizes domain invariant features for
unsupervised domain adaptation to improve its ability to extract domain
invariant features. We conduct experiments with the Domain-Adversarial Training
of Neural Networks (DANN) model as a representative algorithm. In the training
process, we supply the same input to the two modules and then extract their
feature distribution and prediction results respectively. We propose a
discrepancy loss to find the discrepancy of the prediction results and the
feature distribution between the two modules. Through the adversarial training
by maximizing the loss of their feature distribution and minimizing the
discrepancy of their prediction results, the two modules are encouraged to
learn more domain discriminative and domain invariant features respectively.
Extensive comparative evaluations are conducted and the proposed approach
outperforms the state-of-the-art in most unsupervised domain adaptation tasks.",2
492,"Web browsers are integral parts of everyone's daily life. They are commonly
used for security-critical and privacy sensitive tasks, like banking
transactions and checking medical records. Unfortunately, modern web browsers
are too complex to be bug free (e.g., 25 million lines of code in Chrome), and
their role as an interface to the cyberspace makes them an attractive target
for attacks. Accordingly, web browsers naturally become an arena for
demonstrating advanced exploitation techniques by attackers and
state-of-the-art defenses by browser vendors. Web browsers, arguably, are the
most exciting place to learn the latest security issues and techniques, but
remain as a black art to most security researchers because of their
fast-changing characteristics and complex code bases.
  To bridge this gap, this paper attempts to systematize the security landscape
of modern web browsers by studying the popular classes of security bugs, their
exploitation techniques, and deployed defenses. More specifically, we first
introduce a unified architecture that faithfully represents the security design
of four major web browsers. Second, we share insights from a 10-year
longitudinal study on browser bugs. Third, we present a timeline and context of
mitigation schemes and their effectiveness. Fourth, we share our lessons from a
full-chain exploit used in 2020 Pwn2Own competition. and the implication of bug
bounty programs to web browser security. We believe that the key takeaways from
this systematization can shed light on how to advance the status quo of modern
web browsers, and, importantly, how to create secure yet complex software in
the future.",2
493,"A fast algorithm for the large-scale joint inversion of gravity and magnetic
data is developed. It uses a nonlinear Gramian constraint to impose correlation
between density and susceptibility of reconstructed models. The global
objective function is formulated in the space of the weighted parameters, but
the Gramian constraint is implemented in the original space, and the nonlinear
constraint is imposed using two separate Lagrange parameters, one for each
model domain. This combined approach provides more similarity between the
reconstructed models. It is assumed that the measured data are obtained on a
uniform grid and that a consistent regular discretization of the volume domain
is imposed. The sensitivity matrices exhibit a block Toeplitz Toeplitz block
structure for each depth layer of the model domain. Forward and transpose
operations with the matrices can be implemented efficiently using two
dimensional fast Fourier transforms. This makes it feasible to solve for large
scale problems with respect to both computational costs and memory demands, and
to solve the nonlinear problem by applying iterative methods that rely only on
matrix vector multiplications. As such, the use of the regularized reweighted
conjugate gradient algorithm, in conjunction with the structure of the
sensitivity matrices, leads to a fast methodology for large-scale joint
inversion of geophysical data sets. Numerical simulations demonstrate that it
is possible to apply a nonlinear joint inversion algorithm, with $L_p$-norm
stabilisers, for the reconstruction of large model domains on a standard laptop
computer. It is demonstrated, that the p=1 choice provides sparse reconstructed
solutions with sharp boundaries, and $p=2$ provides smooth and blurred models.
Gravity and magnetic data obtained over an area in northwest of Mesoproterozoic
St. Francois Terrane, southeast of Missouri, USA are inverted.",2
494,"We present Tracer Tokens, a hardware token of privacy-preserving contact
tracing utilizing Exposure Notification \cite{GAEN} protocol. Through
subnetworks, we show that any disease spread by proximity can be traced such as
seasonal flu, cold, regional strains of COVID-19, or Tuberculosis. Further, we
show this protocol to notify $n^n$ users in parallel, providing a speed of
information unmatched by current contact tracing methods.",2
495,"This technical report is devoted to explaining how the actor loss of soft
actor critic is obtained, as well as the associated gradient estimate. It gives
the necessary mathematical background to derive all the presented equations,
from the theoretical actor loss to the one implemented in practice. This
necessitates a comparison of the reparameterization trick used in soft actor
critic with the nabla log trick, which leads to open questions regarding the
most efficient method to use.",2
496,"In this paper we introduce a new problem within the growing literature of
interpretability for convolution neural networks (CNNs). While previous work
has focused on the question of how to visually interpret CNNs, we ask what it
is that we care to interpret, that is, which layers and neurons are worth our
attention? Due to the vast size of modern deep learning network architectures,
automated, quantitative methods are needed to rank the relative importance of
neurons so as to provide an answer to this question. We present a new
statistical method for ranking the hidden neurons in any convolutional layer of
a network. We define importance as the maximal correlation between the
activation maps and the class score. We provide different ways in which this
method can be used for visualization purposes with MNIST and ImageNet, and show
a real-world application of our method to air pollution prediction with
street-level images.",2
497,"Harnessing distributed computing environments to build scalable inference
algorithms for very large data sets is a core challenge across the broad
mathematical sciences. Here we provide a theoretical framework to do so along
with fully implemented examples of scalable algorithms with performance
guarantees. We begin by formalizing the class of statistics which admit
straightforward calculation in such environments through independent
parallelization. We then show how to use such statistics to approximate
arbitrary functional operators, thereby providing practitioners with a generic
approximate inference procedure that does not require data to reside entirely
in memory. We characterize the $L^2$ approximation properties of our approach,
and then use it to treat two canonical examples that arise in large-scale
statistical analyses: sample quantile calculation and local polynomial
regression. A variety of avenues and extensions remain open for future work.",2
498,"Existing works on grant-free access, proposed to support massive machine-type
communication (mMTC) for the Internet of things (IoT), mainly concentrate on
narrow band systems under flat fading. However, little is known about massive
grant-free access for wideband systems under frequency-selective fading. This
paper investigates massive grant-free access in a wideband system under
frequency-selective fading. First, we present an orthogonal frequency division
multiplexing (OFDM)-based massive grant-free access scheme. Then, we propose
two different but equivalent models for the received pilot signal, which are
essential for designing various device activity detection and channel
estimation methods for OFDM-based massive grant-free access. One directly
models the received signal for actual devices, whereas the other can be
interpreted as a signal model for virtual devices. Next, we investigate
statistical device activity detection under frequency-selective Rayleigh fading
based on the two signal models. We first model device activities as unknown
deterministic quantities and propose three maximum likelihood (ML)
estimation-based device activity detection methods with different detection
accuracies and computation times. We also model device activities as random
variables with a known joint distribution and propose three maximum a posterior
probability (MAP) estimation-based device activity methods, which further
enhance the accuracies of the corresponding ML estimation-based methods.
Optimization techniques and matrix analysis are applied in designing and
analyzing these methods. Finally, numerical results show that the proposed
statistical device activity detection methods outperform existing
state-of-the-art device activity detection methods under frequency-selective
Rayleigh fading.",2
499,"Multinomial Logit (MNL) is one of the most popular discrete choice models and
has been widely used to model ranking data. However, there is a long-standing
technical challenge of learning MNL from many real-world ranking data: exact
calculation of the MNL likelihood of \emph{partial rankings} is generally
intractable. In this work, we develop a scalable method for approximating the
MNL likelihood of general partial rankings in polynomial time complexity. We
also extend the proposed method to learn mixture of MNL. We demonstrate that
the proposed methods are particularly helpful for applications to choice-based
network formation modeling, where the formation of new edges in a network is
viewed as individuals making choices of their friends over a candidate set. The
problem of learning mixture of MNL models from partial rankings naturally
arises in such applications. And the proposed methods can be used to learn MNL
models from network data without the strong assumption that temporal orders of
all the edge formation are available. We conduct experiments on both synthetic
and real-world network data to demonstrate that the proposed methods achieve
more accurate parameter estimation and better fitness of data compared to
conventional methods.",2
500,"Bayesian models of behavior have provided computational level explanations in
a range of psychophysical tasks. One fundamental experimental paradigm is the
production or reproduction task, in which subjects are instructed to generate
an action that either reproduces a previously sensed stimulus magnitude or
achieves a target response. This type of task therefore distinguishes itself
from other psychophysical tasks in that the responses are on a continuum and
effort plays an important role with increasing response magnitude. Based on
Bayesian decision theory we present an inference method to recover perceptual
uncertainty, response variability, and the cost function underlying human
responses. Crucially, the cost function is parameterized such that effort is
explicitly included. We present a hybrid inference method employing MCMC
sampling utilizing appropriate proposal distributions and an inner loop
utilizing amortized inference with a neural network that approximates the mode
of the optimal response distribution. We show how this model can be utilized to
avoid unidentifiability of experimental designs and that parameters can be
recovered through validation on synthetic and application to experimental data.
Our approach will enable behavioral scientists to perform Bayesian inference of
decision making parameters in production and reproduction tasks.",2
501,"For training recurrent neural network models of nonlinear dynamical systems
from an input/output training dataset based on rather arbitrary convex and
twice-differentiable loss functions and regularization terms, we propose the
use of sequential least squares for determining the optimal network parameters
and hidden states. In addition, to handle non-smooth regularization terms such
as L1, L0, and group-Lasso regularizers, as well as to impose possibly
non-convex constraints such as integer and mixed-integer constraints, we
combine sequential least squares with the alternating direction method of
multipliers (ADMM). The performance of the resulting algorithm, that we call
NAILS (Nonconvex ADMM Iterations and Least Squares), is tested in a nonlinear
system identification benchmark.",2
502,"Web-based interactions can be frequently represented by an attributed graph,
and node clustering in such graphs has received much attention lately. Multiple
efforts have successfully applied Graph Convolutional Networks (GCN), though
with some limits on accuracy as GCNs have been shown to suffer from
over-smoothing issues. Though other methods (particularly those based on
Laplacian Smoothing) have reported better accuracy, a fundamental limitation of
all the work is a lack of scalability. This paper addresses this open problem
by relating the Laplacian smoothing to the Generalized PageRank and applying a
random-walk based algorithm as a scalable graph filter. This forms the basis
for our scalable deep clustering algorithm, RwSL, where through a
self-supervised mini-batch training mechanism, we simultaneously optimize a
deep neural network for sample-cluster assignment distribution and an
autoencoder for a clustering-oriented embedding. Using 6 real-world datasets
and 6 clustering metrics, we show that RwSL achieved improved results over
several recent baselines. Most notably, we show that RwSL, unlike all other
deep clustering frameworks, can continue to scale beyond graphs with more than
one million nodes, i.e., handle web-scale. We also demonstrate how RwSL could
perform node clustering on a graph with 1.8 billion edges using only a single
GPU.",2
503,"The next-generation Industrial Internet of Things (IIoT) inherently requires
smart devices featuring rich connectivity, local intelligence, and autonomous
behavior. Emerging Multiprocessor System-on-Chip (MPSoC) platforms along with
comprehensive support for virtualization will represent two key building blocks
for smart devices in future IIoT edge infrastructures. We review representative
existing solutions, highlighting the aspects that are most relevant for
integration in IIoT solutions. From the analysis, we derive a reference
architecture for a general virtualization-ready edge IIoT node. We then analyze
the implications and benefits for a concrete use case scenario and identify the
crucial research challenges to be faced to bridge the gap towards full support
for virtualization-ready IIoT nodes",2
504,"Achieving high channel estimation accuracy and reducing hardware cost as well
as power dissipation constitute substantial challenges in the design of massive
multiple-input multiple-output (MIMO) systems. To resolve these difficulties,
sophisticated pilot designs have been conceived for the family of
energy-efficient hybrid analog-digital (HAD) beamforming architecture relying
on adaptive-resolution analog-to-digital converters (RADCs). In this paper, we
jointly optimize the pilot sequences, the number of RADC quantization bits and
the hybrid receiver combiner in the uplink of multiuser massive MIMO systems.
We solve the associated mean square error (MSE) minimization problem of channel
estimation in the context of correlated Rayleigh fading channels subject to
practical constraints. The associated mixed-integer problem is quite
challenging due to the nonconvex nature of the objective function and of the
constraints. By relying on advanced fractional programming (FP) techniques, we
first recast the original problem into a more tractable yet equivalent form,
which allows the decoupling of the fractional objective function. We then
conceive a pair of novel algorithms for solving the resultant problems for
codebook-based and codebook-free pilot schemes, respectively. To reduce the
design complexity, we also propose a simplified algorithm for the
codebook-based pilot scheme. Our simulation results confirm the superiority of
the proposed algorithms over the relevant state-of-the-art benchmark schemes.",2
505,"We present an information-theoretic regularization technique for few-shot
novel view synthesis based on neural implicit representation. The proposed
approach minimizes potential reconstruction inconsistency that happens due to
insufficient viewpoints by imposing the entropy constraint of the density in
each ray. In addition, to alleviate the potential degenerate issue when all
training images are acquired from almost redundant viewpoints, we further
incorporate the spatially smoothness constraint into the estimated images by
restricting information gains from a pair of rays with slightly different
viewpoints. The main idea of our algorithm is to make reconstructed scenes
compact along individual rays and consistent across rays in the neighborhood.
The proposed regularizers can be plugged into most of existing neural volume
rendering techniques based on NeRF in a straightforward way. Despite its
simplicity, we achieve consistently improved performance compared to existing
neural view synthesis methods by large margins on multiple standard benchmarks.
Our project website is available at
\url{http://cvlab.snu.ac.kr/research/InfoNeRF}.",2
506,"Continual learning requires models to learn new tasks while maintaining
previously learned knowledge. Various algorithms have been proposed to address
this real challenge. Till now, rehearsal-based methods, such as experience
replay, have achieved state-of-the-art performance. These approaches save a
small part of the data of the past tasks as a memory buffer to prevent models
from forgetting previously learned knowledge. However, most of them treat every
new task equally, i.e., fixed the hyperparameters of the framework while
learning different new tasks. Such a setting lacks the consideration of the
relationship/similarity between past and new tasks. For example, the previous
knowledge/features learned from dogs are more beneficial for the identification
of cats (new task), compared to those learned from buses. In this regard, we
propose a meta learning algorithm based on bi-level optimization to adaptively
tune the relationship between the knowledge extracted from the past and new
tasks. Therefore, the model can find an appropriate direction of gradient
during continual learning and avoid the serious overfitting problem on memory
buffer. Extensive experiments are conducted on three publicly available
datasets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet). The experimental
results demonstrate that the proposed method can consistently improve the
performance of all baselines.",2
507,"Graph sampling theory extends the traditional sampling theory to graphs with
topological structures. As a key part of the graph sampling theory, subset
selection chooses nodes on graphs as samples to reconstruct the original
signal. Due to the eigen-decomposition operation for Laplacian matrices of
graphs, however, existing subset selection methods usually require
high-complexity calculations. In this paper, with an aim of enhancing the
computational efficiency of subset selection on graphs, we propose a novel
objective function based on the optimal experimental design. Theoretical
analysis shows that this function enjoys an $\alpha$-supermodular property with
a provable lower bound on $\alpha$. The objective function, together with an
approximate of the low-pass filter on graphs, suggests a fast subset selection
method that does not require any eigen-decomposition operation. Experimental
results show that the proposed method exhibits high computational efficiency,
while having competitive results compared to the state-of-the-art ones,
especially when the sampling rate is low.",2
508,"The goal of this paper is to conduct a comprehensive study on the facial
sketch synthesis (FSS) problem. However, due to the high costs in obtaining
hand-drawn sketch datasets, there lacks a complete benchmark for assessing the
development of FSS algorithms over the last decade. As such, we first introduce
a high-quality dataset for FSS, named FS2K, which consists of 2,104
image-sketch pairs spanning three types of sketch styles, image backgrounds,
lighting conditions, skin colors, and facial attributes. FS2K differs from
previous FSS datasets in difficulty, diversity, and scalability, and should
thus facilitate the progress of FSS research. Second, we present the
largest-scale FSS study by investigating 139 classical methods, including 24
handcrafted feature based facial sketch synthesis approaches, 37 general
neural-style transfer methods, 43 deep image-to-image translation methods, and
35 image-to-sketch approaches. Besides, we elaborate comprehensive experiments
for existing 19 cutting-edge models. Third, we present a simple baseline for
FSS, named FSGAN. With only two straightforward components, i.e., facial-aware
masking and style-vector expansion, FSGAN surpasses the performance of all
previous state-of-the-art models on the proposed FS2K dataset, by a large
margin. Finally, we conclude with lessons learned over the past years, and
point out several unsolved challenges. Our open-source code is available at
https://github.com/DengPingFan/FSGAN.",2
509,"We consider the $C^1$-Virtual Element Method (VEM) for the conforming
numerical approximation of some variants of the Cahn-Hilliard equation on
polygonal meshes. In particular, we focus on the discretization of the
advective Cahn-Hilliard problem and the Cahn-Hilliard inpainting problem. We
present the numerical approximation and several numerical results to assess the
efficacy of the proposed methodology.",2
510,"We introduce Consistent Assignment for Representation Learning (CARL), an
unsupervised learning method to learn visual representations by combining ideas
from self-supervised contrastive learning and deep clustering. By viewing
contrastive learning from a clustering perspective, CARL learns unsupervised
representations by learning a set of general prototypes that serve as energy
anchors to enforce different views of a given image to be assigned to the same
prototype. Unlike contemporary work on contrastive learning with deep
clustering, CARL proposes to learn the set of general prototypes in an online
fashion, using gradient descent without the necessity of using
non-differentiable algorithms or K-Means to solve the cluster assignment
problem. CARL surpasses its competitors in many representations learning
benchmarks, including linear evaluation, semi-supervised learning, and transfer
learning.",2
511,"Due to the exponentially increasing reach of social media, it is essential to
focus on its negative aspects as it can potentially divide society and incite
people into violence. In this paper, we present our system description of work
on the shared task ComMA@ICON, where we have to classify how aggressive the
sentence is and if the sentence is gender-biased or communal biased. These
three could be the primary reasons to cause significant problems in society. As
team Hypers we have proposed an approach that utilizes different pretrained
models with Attention and mean pooling methods. We were able to get Rank 3 with
0.223 Instance F1 score on Bengali, Rank 2 with 0.322 Instance F1 score on
Multi-lingual set, Rank 4 with 0.129 Instance F1 score on Meitei and Rank 5
with 0.336 Instance F1 score on Hindi. The source code and the pretrained
models of this work can be found here.",2
512,"Deep convolutional neural networks have been demonstrated to be effective for
SISR in recent years. On the one hand, residual connections and dense
connections have been used widely to ease forward information and backward
gradient flows to boost performance. However, current methods use residual
connections and dense connections separately in most network layers in a
sub-optimal way. On the other hand, although various networks and methods have
been designed to improve computation efficiency, save parameters, or utilize
training data of multiple scale factors for each other to boost performance, it
either do super-resolution in HR space to have a high computation cost or can
not share parameters between models of different scale factors to save
parameters and inference time. To tackle these challenges, we propose an
efficient single image super-resolution network using dual path connections
with multiple scale learning named as EMSRDPN. By introducing dual path
connections inspired by Dual Path Networks into EMSRDPN, it uses residual
connections and dense connections in an integrated way in most network layers.
Dual path connections have the benefits of both reusing common features of
residual connections and exploring new features of dense connections to learn a
good representation for SISR. To utilize the feature correlation of multiple
scale factors, EMSRDPN shares all network units in LR space between different
scale factors to learn shared features and only uses a separate reconstruction
unit for each scale factor, which can utilize training data of multiple scale
factors to help each other to boost performance, meanwhile which can save
parameters and support shared inference for multiple scale factors to improve
efficiency. Experiments show EMSRDPN achieves better performance and comparable
or even better parameter and inference efficiency over SOTA methods.",2
513,"This paper presents a key recovery attack on the cryptosystem proposed by Lau
and Tan in a talk at ACISP 2018. The Lau-Tan cryptosystem uses Gabidulin codes
as the underlying decodable code. To hide the algebraic structure of Gabidulin
codes, the authors chose a matrix of column rank $n$ to mix with a generator
matrix of the secret Gabidulin code. The other part of the public key, however,
reveals crucial information about the private key. Our analysis shows that the
problem of recovering the private key can be reduced to solving a multivariate
linear system over the base field, rather than solving a multivariate quadratic
system as claimed by the authors. Solving the linear system for any nonzero
solution permits us to recover the private key. Apparently, this attack costs
polynomial time, and therefore completely breaks the cryptosystem.",2
514,"Subresultant of two univariate polynomials is a fundamental object in
computational algebra and geometry with many applications (for instance,
parametric GCD and parametric multiplicity of roots). In this paper, we
generalize the theory of subresultants of two polynomials to arbitrary number
of polynomials, resulting in multi-polynomial subresultants. Specifically,
  1. we propose a definition of multi-polynomial subresultants, which is an
expression in terms of roots;
  2. we illustrate the usefulness of the proposed definition via the following
two fundamental applications:
  - parametric GCD of multi-polynomials, and
  - parametric multiplicity of roots of a polynomial;
  3. we provide several expressions for the multi-polynomials subresultants in
terms of coefficients, for computation.",2
515,"Hyperdimensional Computing (HDC), also known as Vector-Symbolic Architectures
(VSA), is a promising framework for the development of cognitive architectures
and artificial intelligence systems, as well as for technical applications and
emerging neuromorphic and nanoscale hardware. HDC/VSA operate with
hypervectors, i.e., distributed vector representations of large fixed dimension
(usually > 1000). One of the key ingredients of HDC/VSA are the methods for
encoding data of various types (from numeric scalars and vectors to graphs)
into hypervectors. In this paper, we propose an approach for the formation of
hypervectors of sequences that provides both an equivariance with respect to
the shift of sequences and preserves the similarity of sequences with identical
elements at nearby positions. Our methods represent the sequence elements by
compositional hypervectors and exploit permutations of hypervectors for
representing the order of sequence elements. We experimentally explored the
proposed representations using a diverse set of tasks with data in the form of
symbolic strings. Although our approach is feature-free as it forms the
hypervector of a sequence from the hypervectors of its symbols at their
positions, it demonstrated the performance on a par with the methods that apply
various features, such as subsequences. The proposed techniques were designed
for the HDC/VSA model known as Sparse Binary Distributed Representations.
However, they can be adapted to hypervectors in formats of other HDC/VSA
models, as well as for representing sequences of types other than symbolic
strings.",2
516,"With the increasing popularity of large-scale antenna arrays, the subarraying
technology becomes more attractive. In this paper, we propose two effective
subarraying methods right after formulating the subarray synthesis as a
compressive sensing (CS) problem: i) Orthogonal matching pursuit based subarray
synthesis (OMP-SS), a common CS approach which can be used for the subarray
synthesis to attain the subarray information (the subarray number, the number
of elements per subarray and corresponding excitation coeffcients) and ii)
Off-grid orthogonal matching pursuit based subarray synthesis (OGOMP-SS), an
advanced approach for optimizing antenna elements positions and the subarray
information mentioned above simultaneously. In addition, two user-defined modes
are designed for different application scenarios, wherein, mode-1 is to
optimize the pattern synthesis performance for the given the number of
subarrays, and mode-2 is to obtain the minimum number of subarrays for the
cases when the pattern synthsis accuracy is satisfied. Finally, our simulation
results reveal that it is of paramount significance to optimize antenna
elements positions for the subarray synthesis performance on the one hand and
demonstrate the excellent performances of proposed schemes in comparison with
other competitive state-of-the-art subarray synthesis methods on the other
hand.",2
517,"The inputs and/or outputs of some neural nets are weight matrices of other
neural nets. Indirect encodings or end-to-end compression of weight matrices
could help to scale such approaches. Our goal is to open a discussion on this
topic, starting with recurrent neural networks for character-level language
modelling whose weight matrices are encoded by the discrete cosine transform.
Our fast weight version thereof uses a recurrent neural network to parameterise
the compressed weights. We present experimental results on the enwik8 dataset.",2
518,"Knowledge distillation has made remarkable achievements in model compression.
However, most existing methods demand original training data, while real data
in practice are often unavailable due to privacy, security and transmission
limitation. To address this problem, we propose a conditional generative
data-free knowledge distillation (CGDD) framework to train efficient portable
network without any real data. In this framework, except using the knowledge
extracted from teacher model, we introduce preset labels as additional
auxiliary information to train the generator. Then, the trained generator can
produce meaningful training samples of specified category as required. In order
to promote distillation process, except using conventional distillation loss,
we treat preset label as ground truth label so that student network is directly
supervised by the category of synthetic training sample. Moreover, we force
student network to mimic the attention maps of teacher model and further
improve its performance. To verify the superiority of our method, we design a
new evaluation metric is called as relative accuracy to directly compare the
effectiveness of different distillation methods. Trained portable network
learned with proposed data-free distillation method obtains 99.63%, 99.07% and
99.84% relative accuracy on CIFAR10, CIFAR100 and Caltech101, respectively. The
experimental results demonstrate the superiority of proposed method.",2
519,"In this study, we formulate the model reduction problem of a stable and
positive network system as a constrained Riemannian optimization problem with
the $H^2$-error objective function of the original and reduced network systems.
We improve the reduction performance of the clustering-based method, which is
one of the most known methods for model reduction of positive network systems,
by using the output of the clustering-based method as the initial point for the
proposed method. The proposed method reduces the dimension of the network
system while preserving the properties of stability, positivity, and
interconnection structure by applying the Riemannian augmented Lagrangian
method (RALM) and deriving the Riemannian gradient of the Lagrangian. To check
the efficiency of our method, we conduct a numerical experiment and compare it
with the clustering-based method in the sense of $H^2$-error and
$H^\infty$-error.",2
520,"In recent years, gradient based Meta-RL (GMRL) methods have achieved
remarkable successes in either discovering effective online hyperparameter for
one single task (Xu et al., 2018) or learning good initialisation for
multi-task transfer learning (Finn et al., 2017). Despite the empirical
successes, it is often neglected that computing meta gradients via vanilla
backpropagation is ill-defined. In this paper, we argue that the stochastic
meta-gradient estimation adopted by many existing MGRL methods are in fact
biased; the bias comes from two sources: 1) the compositional bias that is
inborn in the structure of compositional optimisation problems and 2) the bias
of multi-step Hessian estimation caused by direct automatic differentiation. To
better understand the meta gradient biases, we perform the first of its kind
study to quantify the amount for each of them. We start by providing a unifying
derivation for existing GMRL algorithms, and then theoretically analyse both
the bias and the variance of existing gradient estimation methods. On
understanding the underlying principles of bias, we propose two mitigation
solutions based on off-policy correction and multi-step Hessian estimation
techniques. Comprehensive ablation studies have been conducted and results
reveals: (1) The existence of these two biases and how they influence the
meta-gradient estimation when combined with different estimator/sample
size/step and learning rate. (2) The effectiveness of these mitigation
approaches for meta-gradient estimation and thereby the final return on two
practical Meta-RL algorithms: LOLA-DiCE and Meta-gradient Reinforcement
Learning.",2
521,"We show that the structural liveness problem for immediate observation nets
(introduced by Esparza et al., 2019) is PSPACE-complete.",2
522,"In this article, we present the SILKNOW project (Silk heritage in the
Knowledge Society: from punched card to Big Data, Deep Learning and
visual/tangible simulations) (2018-2021). This project aimed to use Semantic
Web technologies to give greater visibility to silk objects produced and
consumed in Europe between the 15th and 19th centuries. Silk is a particularly
important material in European history, and it has produced some exceptional
objects of great historical interest. However, it is a threatened heritage that
is little known to the general public. We show the interest of using Semantic
Web technologies to give more visibility to such a heritage, by describing the
results we have obtained. We present the methodology used to develop a
knowledge graph, and in particular the different steps that were necessary to
create the underlying data model, based on the CIDOC CRM or CIDOC Conceptual
Reference Model. We also propose a CIDOC CRM-compatible extension to express
the complex semantics of the creation and production process of ancient silk
fabrics.",2
523,"Session-based recommendation (SBR) is proposed to recommend items within
short sessions given that user profiles are invisible in various scenarios
nowadays, such as e-commerce and short video recommendation. There is a common
scenario that user specifies a target category of items as a global filter,
however previous SBR settings mainly consider the item sequence and overlook
the rich target category information in this scenario. Therefore, we define a
new task called Category-aware Session-Based Recommendation (CSBR), focusing on
the above scenario, in which the user-specified category can be efficiently
utilized by the recommendation system. To address the challenges of the
proposed task, we develop a novel method called Intention Adaptive Graph Neural
Network (IAGNN), which takes advantage of relationship between items and their
categories to achieve an accurate recommendation result. Specifically, we
construct a category-aware graph with both item and category nodes to represent
the complex transition information in the session. An intention-adaptive graph
neural network on the category-aware graph is utilized to capture user
intention by transferring the historical interaction information to the
user-specified category domain. Extensive experiments on three real-world
datasets are conducted to show our IAGNN outperforms the state-of-the-art
baselines in the new task.",2
524,"Rate of convergence results are presented for a new class of explicit Euler
schemes, which approximate stochastic differential equations (SDEs) with
superlinearly growing drift coefficients that satisfy a particular form of
strong monotonicity. The new, distinct feature of this class of explicit
schemes is the preservation of the monotonicity condition for the new, suitably
controlled drift coefficients that guaranty the finiteness of moments of the
numerical solutions up to a desired order.",2
525,"Triangular flows, also known as Kn\""{o}the-Rosenblatt measure couplings,
comprise an important building block of normalizing flow models for generative
modeling and density estimation, including popular autoregressive flow models
such as real-valued non-volume preserving transformation models (Real NVP). We
present statistical guarantees and sample complexity bounds for triangular flow
statistical models. In particular, we establish the statistical consistency and
the finite sample convergence rates of the Kullback-Leibler estimator of the
Kn\""{o}the-Rosenblatt measure coupling using tools from empirical process
theory. Our results highlight the anisotropic geometry of function classes at
play in triangular flows, shed light on optimal coordinate ordering, and lead
to statistical guarantees for Jacobian flows. We conduct numerical experiments
on synthetic data to illustrate the practical implications of our theoretical
findings.",2
526,"We demonstrate that a neural network pre-trained on text and fine-tuned on
code solves Mathematics problems by program synthesis. We turn questions into
programming tasks, automatically generate programs, and then execute them,
perfectly solving university-level problems from MIT's large Mathematics
courses (Single Variable Calculus 18.01, Multivariable Calculus 18.02,
Differential Equations 18.03, Introduction to Probability and Statistics 18.05,
Linear Algebra 18.06, and Mathematics for Computer Science 6.042), Columbia
University's COMS3251 Computational Linear Algebra course, as well as questions
from a MATH dataset (on Prealgebra, Algebra, Counting and Probability, Number
Theory, and Precalculus), the latest benchmark of advanced mathematics problems
specifically designed to assess mathematical reasoning. We explore prompt
generation methods that enable Transformers to generate question solving
programs for these subjects, including solutions with plots. We generate
correct answers for a random sample of questions in each topic. We quantify the
gap between the original and transformed questions and perform a survey to
evaluate the quality and difficulty of generated questions. This is the first
work to automatically solve, grade, and generate university-level Mathematics
course questions at scale. This represents a milestone for higher education.",2
527,"Deep neural networks (DNNs) are powerful tools for compressing and distilling
information. Due to their scale and complexity, often involving billions of
inter-dependent internal degrees of freedom, exact analysis approaches often
fall short. A common strategy in such cases is to identify slow degrees of
freedom that average out the erratic behavior of the underlying fast
microscopic variables. Here, we identify such a separation of scales occurring
in over-parameterized deep convolutional neural networks (CNNs) at the end of
training. It implies that neuron pre-activations fluctuate in a nearly Gaussian
manner with a deterministic latent kernel. While for CNNs with infinitely many
channels these kernels are inert, for finite CNNs they adapt and learn from
data in an analytically tractable manner. The resulting thermodynamic theory of
deep learning yields accurate predictions on several deep non-linear CNN toy
models. In addition, it provides new ways of analyzing and understanding CNNs.",2
528,"A trivializing map is a field transformation whose Jacobian determinant
exactly cancels the interaction terms in the action, providing a representation
of the theory in terms of a deterministic transformation of a distribution from
which sampling is trivial. Recently, a proof-of-principle study by Albergo,
Kanwar and Shanahan [arXiv:1904.12072] demonstrated that approximations of
trivializing maps can be `machine-learned' by a class of invertible,
differentiable neural models called \textit{normalizing flows}. By ensuring
that the Jacobian determinant can be computed efficiently, asymptotically exact
sampling from the theory of interest can be performed by drawing samples from a
simple distribution and passing them through the network. From a theoretical
perspective, this approach has the potential to become more efficient than
traditional Markov Chain Monte Carlo sampling techniques, where
autocorrelations severely diminish the sampling efficiency as one approaches
the continuum limit. A major caveat is that it is not yet understood how the
size of models and the cost of training them is expected to scale. As a first
step, we have conducted an exploratory scaling study using two-dimensional
$\phi^4$ with up to $20^2$ lattice sites. Although the scope of our study is
limited to a particular model architecture and training algorithm, initial
results paint an interesting picture in which training costs grow very quickly
indeed. We describe a candidate explanation for the poor scaling, and outline
our intentions to clarify the situation in future work.",2
529,"Transfer learning allows us to exploit knowledge gained from one task to
assist in solving another but relevant task. In modern computer vision
research, the question is which architecture performs better for a given
dataset. In this paper, we compare the performance of 14 pre-trained ImageNet
models on the histopathologic cancer detection dataset, where each model has
been configured as a naive model, feature extractor model, or fine-tuned model.
Densenet161 has been shown to have high precision whilst Resnet101 has a high
recall. A high precision model is suitable to be used when follow-up
examination cost is high, whilst low precision but a high recall/sensitivity
model can be used when the cost of follow-up examination is low. Results also
show that transfer learning helps to converge a model faster.",2
530,"Detecting 3D lanes from the camera is a rising problem for autonomous
vehicles. In this task, the correct camera pose is the key to generating
accurate lanes, which can transform an image from perspective-view to the
top-view. With this transformation, we can get rid of the perspective effects
so that 3D lanes would look similar and can accurately be fitted by low-order
polynomials. However, mainstream 3D lane detectors rely on perfect camera poses
provided by other sensors, which is expensive and encounters multi-sensor
calibration issues. To overcome this problem, we propose to predict 3D lanes by
estimating camera pose from a single image with a two-stage framework. The
first stage aims at the camera pose task from perspective-view images. To
improve pose estimation, we introduce an auxiliary 3D lane task and geometry
constraints to benefit from multi-task learning, which enhances consistencies
between 3D and 2D, as well as compatibility in the above two tasks. The second
stage targets the 3D lane task. It uses previously estimated pose to generate
top-view images containing distance-invariant lane appearances for predicting
accurate 3D lanes. Experiments demonstrate that, without ground truth camera
pose, our method outperforms the state-of-the-art perfect-camera-pose-based
methods and has the fewest parameters and computations. Codes are available at
https://github.com/liuruijin17/CLGo.",2
531,"In this paper, we mainly study quaternary linear codes and their binary
subfield codes. First we obtain a general explicit relationship between
quaternary linear codes and their binary subfield codes in terms of generator
matrices and defining sets. Second, we construct quaternary linear codes via
simplicial complexes and determine the weight distributions of these codes.
Third, the weight distributions of the binary subfield codes of these
quaternary codes are also computed by employing the general characterization.
Furthermore, we present two infinite families of optimal linear codes with
respect to the Griesmer Bound, and a class of binary almost optimal codes with
respect to the Sphere Packing Bound. We also need to emphasize that we obtain
at least 9 new quaternary linear codes.",2
532,"We are making the case that empirical results from social psychology and
social neuroscience along with the framework of dynamics can be of inspiration
to the development of more intelligent artificial agents. We specifically argue
that the complex human cognitive architecture owes a large portion of its
expressive power to its ability to engage in social and cultural learning. In
the first section, we aim at demonstrating that social learning plays a key
role in the development of intelligence. We do so by discussing social and
cultural learning theories and investigating the abilities that various animals
have at learning from others; we also explore findings from social neuroscience
that examine human brains during social interaction and learning. Then, we
discuss three proposed lines of research that fall under the umbrella of Social
NeuroAI and can contribute to developing socially intelligent embodied agents
in complex environments. First, neuroscientific theories of cognitive
architecture, such as the global workspace theory and the attention schema
theory, can enhance biological plausibility and help us understand how we could
bridge individual and social theories of intelligence. Second, intelligence
occurs in time as opposed to over time, and this is naturally incorporated by
the powerful framework offered by dynamics. Third, social embodiment has been
demonstrated to provide social interactions between virtual agents and humans
with a more sophisticated array of communicative signals. To conclude, we
provide a new perspective on the field of multiagent robot systems, exploring
how it can advance by following the aforementioned three axes.",2
533,"We present PiFeNet, an efficient and accurate real-time 3D detector for
pedestrian detection from point clouds. We address two challenges that 3D
object detection frameworks encounter when detecting pedestrians: low
expressiveness of pillar features and small occupation areas of pedestrians in
point clouds. Firstly, we introduce a stackable Pillar Aware Attention (PAA)
module for enhanced pillar features extraction while suppressing noises in the
point clouds. By integrating multi-point-aware-pooling, point-wise,
channel-wise, and task-aware attention into a simple module, the representation
capabilities are boosted while requiring little additional computing resources.
We also present Mini-BiFPN, a small yet effective feature network that creates
bidirectional information flow and multi-level cross-scale feature fusion to
better integrate multi-resolution features. Our approach is ranked 1st in KITTI
pedestrian BEV and 3D leaderboards while running at 26 frames per second (FPS),
and achieves state-of-the-art performance on Nuscenes detection benchmark.",2
534,"Extensive air showers created by high-energy particles interacting with the
Earth atmosphere can be detected using imaging atmospheric Cherenkov telescopes
(IACTs). The IACT images can be analyzed to distinguish between the events
caused by gamma rays and by hadrons and to infer the parameters of the event
such as the energy of the primary particle. We use convolutional neural
networks (CNNs) to analyze Monte Carlo-simulated images from the telescopes of
the TAIGA experiment. The analysis includes selection of the images
corresponding to the showers caused by gamma rays and estimating the energy of
the gamma rays. We compare performance of the CNNs using images from a single
telescope and the CNNs using images from two telescopes as inputs.",2
535,"In clinics, a radiology report is crucial for guiding a patient's treatment.
Unfortunately, report writing imposes a heavy burden on radiologists. To
effectively reduce such a burden, we hereby present an automatic, multi-modal
approach for report generation from chest x-ray. Our approach, motivated by the
observation that the descriptions in radiology reports are highly correlated
with the x-ray images, features two distinct modules: (i) Learned knowledge
base. To absorb the knowledge embedded in the above-mentioned correlation, we
automatically build a knowledge base based on textual embedding. (ii)
Multi-modal alignment. To promote the semantic alignment among reports, disease
labels and images, we explicitly utilize textual embedding to guide the
learning of the visual feature space. We evaluate the performance of the
proposed model using metrics from both natural language generation and clinic
efficacy on the public IU and MIMIC-CXR datasets. Our ablation study shows that
each module contributes to improving the quality of generated reports.
Furthermore, with the aid of both modules, our approach clearly outperforms
state-of-the-art methods.",2
536,"Automatic radiology report generation is critical in clinics which can
relieve experienced radiologists from the heavy workload and remind
inexperienced radiologists of misdiagnosis or missed diagnose. Existing
approaches mainly formulate radiology report generation as an image captioning
task and adopt the encoder-decoder framework. However, in the medical domain,
such pure data-driven approaches suffer from the following problems: 1) visual
and textual bias problem; 2) lack of expert knowledge. In this paper, we
propose a knowledge-enhanced radiology report generation approach introduces
two types of medical knowledge: 1) General knowledge, which is input
independent and provides the broad knowledge for report generation; 2) Specific
knowledge, which is input dependent and provides the fine-grained knowledge for
report generation. To fully utilize both the general and specific knowledge, we
also propose a knowledge-enhanced multi-head attention mechanism. By merging
the visual features of the radiology image with general knowledge and specific
knowledge, the proposed model can improve the quality of generated reports.
Experimental results on two publicly available datasets IU-Xray and MIMIC-CXR
show that the proposed knowledge enhanced approach outperforms state-of-the-art
image captioning based methods. Ablation studies also demonstrate that both
general and specific knowledge can help to improve the performance of radiology
report generation.",2
537,"Predicting human motion from historical pose sequence is crucial for a
machine to succeed in intelligent interactions with humans. One aspect that has
been obviated so far, is the fact that how we represent the skeletal pose has a
critical impact on the prediction results. Yet there is no effort that
investigates across different pose representation schemes. We conduct an
indepth study on various pose representations with a focus on their effects on
the motion prediction task. Moreover, recent approaches build upon
off-the-shelf RNN units for motion prediction. These approaches process input
pose sequence sequentially and inherently have difficulties in capturing
long-term dependencies. In this paper, we propose a novel RNN architecture
termed AHMR (Attentive Hierarchical Motion Recurrent network) for motion
prediction which simultaneously models local motion contexts and a global
context. We further explore a geodesic loss and a forward kinematics loss for
the motion prediction task, which have more geometric significance than the
widely employed L2 loss. Interestingly, we applied our method to a range of
articulate objects including human, fish, and mouse. Empirical results show
that our approach outperforms the state-of-the-art methods in short-term
prediction and achieves much enhanced long-term prediction proficiency, such as
retaining natural human-like motions over 50 seconds predictions. Our codes are
released.",2
538,"Recent self-supervised learning methods are able to learn high-quality image
representations and are closing the gap with supervised methods. However, these
methods are unable to acquire new knowledge incrementally -- they are, in fact,
mostly used only as a pre-training phase with IID data. In this work we
investigate self-supervised methods in continual learning regimes without
additional memory or replay. To prevent forgetting of previous knowledge, we
propose the usage of functional regularization. We will show that naive
functional regularization, also known as feature distillation, leads to low
plasticity and therefore seriously limits continual learning performance. To
address this problem, we propose Projected Functional Regularization where a
separate projection network ensures that the newly learned feature space
preserves information of the previous feature space, while allowing for the
learning of new features. This allows us to prevent forgetting while
maintaining the plasticity of the learner. Evaluation against other incremental
learning approaches applied to self-supervision demonstrates that our method
obtains competitive performance in different scenarios and on multiple
datasets.",2
539,"Graph neural network (GNN) has shown convincing performance in learning
powerful node representations that preserve both node attributes and graph
structural information. However, many GNNs encounter problems in effectiveness
and efficiency when they are designed with a deeper network structure or handle
large-sized graphs. Several sampling algorithms have been proposed for
improving and accelerating the training of GNNs, yet they ignore understanding
the source of GNN performance gain. The measurement of information within graph
data can help the sampling algorithms to keep high-value information while
removing redundant information and even noise. In this paper, we propose a
Metric-Guided (MeGuide) subgraph learning framework for GNNs. MeGuide employs
two novel metrics: Feature Smoothness and Connection Failure Distance to guide
the subgraph sampling and mini-batch based training. Feature Smoothness is
designed for analyzing the feature of nodes in order to retain the most
valuable information, while Connection Failure Distance can measure the
structural information to control the size of subgraphs. We demonstrate the
effectiveness and efficiency of MeGuide in training various GNNs on multiple
datasets.",2
540,"Accurate decoding of surface electromyography (sEMG) is pivotal for
muscle-to-machine-interfaces (MMI) and their application for e.g.
rehabilitation therapy. sEMG signals have high inter-subject variability, due
to various factors, including skin thickness, body fat percentage, and
electrode placement. Therefore, obtaining high generalization quality of a
trained sEMG decoder is quite challenging. Usually, machine learning based sEMG
decoders are either trained on subject-specific data, or at least recalibrated
for each user, individually. Even though, deep learning algorithms produced
several state of the art results for sEMG decoding,however, due to the limited
amount of availability of sEMG data, the deep learning models are prone to
overfitting. Recently, transfer learning for domain adaptation improved
generalization quality with reduced training time on various machine learning
tasks. In this study, we investigate the effectiveness of transfer learning
using weight initialization for recalibration of two different pretrained deep
learning models on a new subjects data, and compare their performance to
subject-specific models. To the best of our knowledge, this is the first study
that thoroughly investigated weight-initialization based transfer learning for
sEMG classification and compared transfer learning to subject-specific
modeling. We tested our models on three publicly available databases under
various settings. On average over all settings, our transfer learning approach
improves 5~\%-points on the pretrained models without fine-tuning and
12~\%-points on the subject-specific models, while being trained on average
22~\% fewer epochs. Our results indicate that transfer learning enables faster
training on fewer samples than user-specific models, and improves the
performance of pretrained models as long as enough data is available.",2
541,"Optimal transport (OT) and its entropy regularized offspring have recently
gained a lot of attention in both machine learning and AI domains. In
particular, optimal transport has been used to develop probability metrics
between probability distributions. We introduce in this paper an independence
criterion based on entropy regularized optimal transport. Our criterion can be
used to test for independence between two samples. We establish non-asymptotic
bounds for our test statistic, and study its statistical behavior under both
the null and alternative hypothesis. Our theoretical results involve tools from
U-process theory and optimal transport theory. We present experimental results
on existing benchmarks, illustrating the interest of the proposed criterion.",2
542,"We prove algorithmic results showing that a number of natural parameterized
problems are in the restricted-space parameterized classes Para-L and FPT+XL.
The first class comprises problems solvable in f(k) n^{O(1)} time using g(k) +
O(log n)) bits of space (k is the parameter and n is the input size; f and g
are computable functions). The second class comprises problems solvable under
the same time bound, but using g(k) log n bits of space instead.
  Earlier work on these classes has focused largely on their structural aspects
and their relationships with various other classes. We complement this with
Para-L and FPT+XL algorithms for a restriction of Hitting Set, some graph
deletion problems where the target class has an infinite forbidden set
characterization, a number of problems parameterized by vertex cover number,
and Feedback Vertex Set.",2
543,"This work focuses upon the analysis of hand gestures involved in the process
of hand washing. There are six standard hand hygiene gestures for washing hands
as provided by World Health Organisation hand hygiene guidelines. In this
paper, hand features such as contours of hands, the centroid of the hands, and
extreme hand points along the largest contour are extracted with the use of the
computer vision library, OpenCV. These hand features are extracted for each
data frame in a hand hygiene video. A robust hand hygiene dataset of video
recordings was created in the project. A subset of this dataset is used in this
work. Extracted hand features are further grouped into classes based on the KNN
algorithm with a cross-fold validation technique for the classification and
prediction of the unlabelled data. A mean accuracy score of >95% is achieved
and proves that the KNN algorithm with an appropriate input value of K=5 is
efficient for classification. A complete dataset with six distinct hand hygiene
classes will be used with the KNN classifier for future work.",2
544,"Information-theoretic measures have been widely adopted in the design of
features for learning and decision problems. Inspired by this, we look at the
relationship between i) a weak form of information loss in the Shannon sense
and ii) the operation loss in the minimum probability of error (MPE) sense when
considering a family of lossy continuous representations (features) of a
continuous observation. We present several results that shed light on this
interplay. Our first result offers a lower bound on a weak form of information
loss as a function of its respective operation loss when adopting a discrete
lossy representation (quantization) instead of the original raw observation.
From this, our main result shows that a specific form of vanishing information
loss (a weak notion of asymptotic informational sufficiency) implies a
vanishing MPE loss (or asymptotic operational sufficiency) when considering a
general family of lossy continuous representations. Our theoretical findings
support the observation that the selection of feature representations that
attempt to capture informational sufficiency is appropriate for learning, but
this selection is a rather conservative design principle if the intended goal
is achieving MPE in classification. Supporting this last point, and under some
structural conditions, we show that it is possible to adopt an alternative
notion of informational sufficiency (strictly weaker than pure sufficiency in
the mutual information sense) to achieve operational sufficiency in learning.",2
545,"Computing the Bayesian posterior of a neural network is a challenging task
due to the high-dimensionality of the parameter space. Anchored ensembles
approximate the posterior by training an ensemble of neural networks on
anchored losses designed for the optima to follow the Bayesian posterior.
Training an ensemble, however, becomes computationally expensive as its number
of members grows since the full training procedure is repeated for each member.
In this note, we present Sequential Anchored Ensembles (SAE), a lightweight
alternative to anchored ensembles. Instead of training each member of the
ensemble from scratch, the members are trained sequentially on losses sampled
with high auto-correlation, hence enabling fast convergence of the neural
networks and efficient approximation of the Bayesian posterior. SAE outperform
anchored ensembles, for a given computational budget, on some benchmarks while
showing comparable performance on the others and achieved 2nd and 3rd place in
the light and extended tracks of the NeurIPS 2021 Approximate Inference in
Bayesian Deep Learning competition.",2
546,"Convolution neural networks (CNNs) have succeeded in compressive image
sensing. However, due to the inductive bias of locality and weight sharing, the
convolution operations demonstrate the intrinsic limitations in modeling the
long-range dependency. Transformer, designed initially as a
sequence-to-sequence model, excels at capturing global contexts due to the
self-attention-based architectures even though it may be equipped with limited
localization abilities. This paper proposes CSformer, a hybrid framework that
integrates the advantages of leveraging both detailed spatial information from
CNN and the global context provided by transformer for enhanced representation
learning. The proposed approach is an end-to-end compressive image sensing
method, composed of adaptive sampling and recovery. In the sampling module,
images are measured block-by-block by the learned sampling matrix. In the
reconstruction stage, the measurement is projected into dual stems. One is the
CNN stem for modeling the neighborhood relationships by convolution, and the
other is the transformer stem for adopting global self-attention mechanism. The
dual branches structure is concurrent, and the local features and global
representations are fused under different resolutions to maximize the
complementary of features. Furthermore, we explore a progressive strategy and
window-based transformer block to reduce the parameter and computational
complexity. The experimental results demonstrate the effectiveness of the
dedicated transformer-based architecture for compressive sensing, which
achieves superior performance compared to state-of-the-art methods on different
datasets.",2
547,"Cloud systems are becoming increasingly powerful and complex. It is highly
challenging to identify anomalous execution behaviors and pinpoint problems by
examining the overwhelming intermediate results/states in complex application
workflows. Domain scientists urgently need a friendly and functional interface
to understand the quality of the computing services and the performance of
their applications in real time. To meet these needs, we explore data generated
by job schedulers and investigate general performance metrics (e.g.,
utilization of CPU, memory and disk I/O). Specifically, we propose an
interactive visual analytics approach, BatchLens, to provide both providers and
users of cloud service with an intuitive and effective way to explore the
status of system batch jobs and help them conduct root-cause analysis of
anomalous behaviors in batch jobs. We demonstrate the effectiveness of
BatchLens through a case study on the public Alibaba bench workload trace
datasets.",2
548,"This work explores how to learn robust and generalizable state representation
from image-based observations with deep reinforcement learning methods.
Addressing the computational complexity, stringent assumptions, and
representation collapse challenges in the existing work of bisimulation metric,
we devise Simple State Representation (SimSR) operator, which achieves
equivalent functionality while reducing the complexity by an order in
comparison with bisimulation metric. SimSR enables us to design a
stochastic-approximation-based method that can practically learn the mapping
functions (encoders) from observations to latent representation space. Besides
the theoretical analysis, we experimented and compared our work with recent
state-of-the-art solutions in visual MuJoCo tasks. The results show that our
model generally achieves better performance and has better robustness and good
generalization.",2
549,"During the SARS-Cov-2 pandemic, mask-wearing became an effective tool to
prevent spreading and contracting the virus. The ability to monitor the
mask-wearing rate in the population would be useful for determining public
health strategies against the virus. However, artificial intelligence
technologies for detecting face masks have not been deployed at a large scale
in real-life to measure the mask-wearing rate in public. In this paper, we
present a two-step face mask detection approach consisting of two separate
modules: 1) face detection and alignment and 2) face mask classification. This
approach allowed us to experiment with different combinations of face detection
and face mask classification modules. More specifically, we experimented with
PyramidKey and RetinaFace as face detectors while maintaining a lightweight
backbone for the face mask classification module. Moreover, we also provide a
relabeled annotation of the test set of the AIZOO dataset, where we rectified
the incorrect labels for some face images. The evaluation results on the AIZOO
and Moxa 3K datasets showed that the proposed face mask detection pipeline
surpassed the state-of-the-art methods. The proposed pipeline also yielded a
higher mAP on the relabeled test set of the AIZOO dataset than the original
test set. Since we trained the proposed model using in-the-wild face images, we
can successfully deploy our model to monitor the mask-wearing rate using public
CCTV images.",2
550,"Recently, there have been many advances in autonomous driving society,
attracting a lot of attention from academia and industry. However, existing
works mainly focus on cars, extra development is still required for
self-driving truck algorithms and models. In this paper, we introduce an
intelligent self-driving truck system. Our presented system consists of three
main components, 1) a realistic traffic simulation module for generating
realistic traffic flow in testing scenarios, 2) a high-fidelity truck model
which is designed and evaluated for mimicking real truck response in real-world
deployment, 3) an intelligent planning module with learning-based decision
making algorithm and multi-mode trajectory planner, taking into account the
truck's constraints, road slope changes, and the surrounding traffic flow. We
provide quantitative evaluations for each component individually to demonstrate
the fidelity and performance of each part. We also deploy our proposed system
on a real truck and conduct real world experiments which shows our system's
capacity of mitigating sim-to-real gap. Our code is available at
https://github.com/InceptioResearch/IITS",2
551,"This paper proposes two bottom-up interpretable neural network (NN)
constructions for universal approximation, namely Triangularly-constructed NN
(TNN) and Semi-Quantized Activation NN (SQANN). The notable properties are (1)
resistance to catastrophic forgetting (2) existence of proof for arbitrarily
high accuracies on training dataset (3) for an input \(x\), users can identify
specific samples of training data whose activation ``fingerprints"" are similar
to that of \(x\)'s activations. Users can also identify samples that are out of
distribution.",2
552,"String vibration represents an active field of research in acoustics.
Small-amplitude vibration is often assumed, leading to simplified physical
models that can be simulated efficiently. However, the inclusion of nonlinear
phenomena due to larger string stretchings is necessary to capture important
features, and efficient numerical algorithms are currently lacking in this
context. Of the available techniques, many lead to schemes which may only be
solved iteratively, resulting in high computational cost, and the additional
concerns of existence and uniqueness of solutions. Slow and fast waves are
present concurrently in the transverse and longitudinal directions of motion,
adding further complications concerning numerical dispersion. This work
presents a linearly-implicit scheme for the simulation of the geometrically
exact nonlinear string model. The scheme conserves a numerical energy,
expressed as the sum of quadratic terms only, and including an auxiliary state
variable yielding the nonlinear effects. This scheme allows to treat the
transverse and longitudinal waves separately, using a mixed finite
difference/modal scheme for the two directions of motion, thus allowing to
accurately resolve the wave speeds at reference sample rates. Numerical
experiments are presented throughout.",2
553,"Transparency and fairness issues in Deep Reinforcement Learning may stem from
the black-box nature of deep neural networks used to learn its policy, value
functions etc. This paper proposes a way to circumvent the issues through the
bottom-up design of neural networks (NN) with detailed interpretability, where
each neuron or layer has its own meaning and utility that corresponds to
humanly understandable concept. With deliberate design, we show that lavaland
problems can be solved using NN model with few parameters. Furthermore, we
introduce the Self Reward Design (SRD), inspired by the Inverse Reward Design,
so that our interpretable design can (1) solve the problem by pure design
(although imperfectly) (2) be optimized via SRD (3) perform avoidance of
unknown states by recognizing the inactivations of neurons aggregated as the
activation in \(w_{unknown}\).",2
554,"Unrefinable partitions are a subset of partitions into distinct parts which
satisfy an additional unrefinability property. More precisely, no parts of such
partitions can be written as the sum of different integers which are not parts.
We address in this paper the algorithmic aspects related to unrefinable
partitions, such as testing whether a given partition is unrefinable or not and
enumerating all the partitions whose sum is a given number. We design two
algorithms to solve the two mentioned problems and we discuss their complexity.",2
555,"In this paper, we consider the distributed optimization problem where $n$
agents, each possessing a local cost function, collaboratively minimize the
average of the local cost functions over a connected network. To solve the
problem, we propose a distributed random reshuffling (D-RR) algorithm that
combines the classical distributed gradient descent (DGD) method and Random
Reshuffling (RR). We show that D-RR inherits the superiority of RR for both
smooth strongly convex and smooth nonconvex objective functions. In particular,
for smooth strongly convex objective functions, D-RR achieves
$\mathcal{O}(1/T^2)$ rate of convergence (here, $T$ counts the total number of
iterations) in terms of the squared distance between the iterate and the unique
minimizer. When the objective function is assumed to be smooth nonconvex and
has Lipschitz continuous component functions, we show that D-RR drives the
squared norm of gradient to $0$ at a rate of $\mathcal{O}(1/T^{2/3})$. These
convergence results match those of centralized RR (up to constant factors).",2
556,"The analysis of long sequence data remains challenging in many real-world
applications. We propose a novel architecture, ChunkFormer, that improves the
existing Transformer framework to handle the challenges while dealing with long
time series. Original Transformer-based models adopt an attention mechanism to
discover global information along a sequence to leverage the contextual data.
Long sequential data traps local information such as seasonality and
fluctuations in short data sequences. In addition, the original Transformer
consumes more resources by carrying the entire attention matrix during the
training course. To overcome these challenges, ChunkFormer splits the long
sequences into smaller sequence chunks for the attention calculation,
progressively applying different chunk sizes in each stage. In this way, the
proposed model gradually learns both local and global information without
changing the total length of the input sequences. We have extensively tested
the effectiveness of this new architecture on different business domains and
have proved the advantage of such a model over the existing Transformer-based
models.",2
557,"Learning powerful representations is one central theme of graph neural
networks (GNNs). It requires refining the critical information from the input
graph, instead of the trivial patterns, to enrich the representations. Towards
this end, graph attention and pooling methods prevail. They mostly follow the
paradigm of ""learning to attend"". It maximizes the mutual information between
the attended subgraph and the ground-truth label. However, this training
paradigm is prone to capture the spurious correlations between the trivial
subgraph and the label. Such spurious correlations are beneficial to
in-distribution (ID) test evaluations, but cause poor generalization in the
out-of-distribution (OOD) test data. In this work, we revisit the GNN modeling
from the causal perspective. On the top of our causal assumption, the trivial
information serves as a confounder between the critical information and the
label, which opens a backdoor path between them and makes them spuriously
correlated. Hence, we present a new paradigm of deconfounded training (DTP)
that better mitigates the confounding effect and latches on the critical
information, to enhance the representation and generalization ability.
Specifically, we adopt the attention modules to disentangle the critical
subgraph and trivial subgraph. Then we make each critical subgraph fairly
interact with diverse trivial subgraphs to achieve a stable prediction. It
allows GNNs to capture a more reliable subgraph whose relation with the label
is robust across different distributions. We conduct extensive experiments on
synthetic and real-world datasets to demonstrate the effectiveness.",2
558,"The flourishing blossom of deep learning has witnessed the rapid development
of text recognition in recent years. However, the existing text recognition
methods are mainly for English texts, whereas ignoring the pivotal role of
Chinese texts. As another widely-spoken language, Chinese text recognition in
all ways has extensive application markets. Based on our observations, we
attribute the scarce attention on Chinese text recognition to the lack of
reasonable dataset construction standards, unified evaluation methods, and
results of the existing baselines. To fill this gap, we manually collect
Chinese text datasets from publicly available competitions, projects, and
papers, then divide them into four categories including scene, web, document,
and handwriting datasets. Furthermore, we evaluate a series of representative
text recognition methods on these datasets with unified evaluation methods to
provide experimental results. By analyzing the experimental results, we
surprisingly observe that state-of-the-art baselines for recognizing English
texts cannot perform well on Chinese scenarios. We consider that there still
remain numerous challenges under exploration due to the characteristics of
Chinese texts, which are quite different from English texts. The code and
datasets are made publicly available at
https://github.com/FudanVI/benchmarking-chinese-text-recognition.",2
559,"Linear dynamical systems are canonical models for learning-based control of
plants with uncertain dynamics. The setting consists of a stochastic
differential equation that captures the state evolution of the plant
understudy, while the true dynamics matrices are unknown and need to be learned
from the observed data of state trajectory. An important issue is to ensure
that the system is stabilized and destabilizing control actions due to model
uncertainties are precluded as soon as possible. A reliable stabilization
procedure for this purpose that can effectively learn from unstable data to
stabilize the system in a finite time is not currently available. In this work,
we propose a novel Bayesian learning algorithm that stabilizes unknown
continuous-time stochastic linear systems. The presented algorithm is flexible
and exposes effective stabilization performance after a remarkably short time
period of interacting with the system.",2
560,"Various applications of farm animal imaging are based on the estimation of
weights of certain body parts and cuts from the CT images of animals. In many
cases, the complexity of the problem is increased by the enormous variability
of postures in CT images due to the scanning of non-sedated, living animals. In
this paper, we propose a general and robust approach for the estimation of the
weights of cuts and body parts from the CT images of (possibly) living animals.
We adapt multi-atlas based segmentation driven by elastic registration and
joint feature and model selection for the regression component to cape with the
large number of features and low number of samples. The proposed technique is
evaluated and illustrated through real applications in rabbit breeding
programs, showing r^2 scores 12% higher than previous techniques and methods
that used to drive the selection so far. The proposed technique is easily
adaptable to similar problems, consequently, it is shared in an open source
software package for the benefit of the community.",2
561,"Recently, text classification model based on graph neural network (GNN) has
attracted more and more attention. Most of these models adopt a similar network
paradigm, that is, using pre-training node embedding initialization and
two-layer graph convolution. In this work, we propose TextRGNN, an improved GNN
structure that introduces residual connection to deepen the convolution network
depth. Our structure can obtain a wider node receptive field and effectively
suppress the over-smoothing of node features. In addition, we integrate the
probabilistic language model into the initialization of graph node embedding,
so that the non-graph semantic information of can be better extracted. The
experimental results show that our model is general and efficient. It can
significantly improve the classification accuracy whether in corpus level or
text level, and achieve SOTA performance on a wide range of text classification
datasets.",2
562,"In the framework of virtual element discretizazions, we address the problem
of imposing non homogeneous Dirichlet boundary conditions in a weak form, both
on polygonal/polyhedral domains and on two/three dimensional domains with
curved boundaries. We consider a Nitsche's type method [43,41], and the
stabilized formulation of the Lagrange multiplier method proposed by Barbosa
and Hughes in [9]. We prove that also for the virtual element method (VEM),
provided the stabilization parameter is suitably chosen (large enough for
Nitsche's method and small enough for the Barbosa-Hughes Lagrange multiplier
method), the resulting discrete problem is well posed, and yields convergence
with optimal order on polygonal/polyhedral domains. On smooth two/three
dimensional domains, we combine both methods with a projection approach similar
to the one of [31]. We prove that, given a polygonal/polyhedral approximation
$\Omega_h$ of the domain $\Omega$, an optimal convergence rate can be achieved
by using a suitable correction depending on high order derivatives of the
discrete solution along outward directions (not necessarily orthogonal) at the
boundary facets of $\Omega_h$. Numerical experiments validate the theory.",2
563,"Learning continually and online from a continuous stream of data is
challenging, especially for a reinforcement learning agent with sequential
data. When the environment only provides observations giving partial
information about the state of the environment, the agent must learn the agent
state based on the data stream of experience. We refer to the state learned
directly from the data stream of experience as the agent state. Recurrent
neural networks can learn the agent state, but the training methods are
computationally expensive and sensitive to the hyper-parameters, making them
unideal for online learning. This work introduces methods based on the
generate-and-test approach to learn the agent state. A generate-and-test
algorithm searches for state features by generating features and testing their
usefulness. In this process, features useful for the agent's performance on the
task are preserved, and the least useful features get replaced with newly
generated features. We study the effectiveness of our methods on two online
multi-step prediction problems. The first problem, trace conditioning, focuses
on the agent's ability to remember a cue for a prediction multiple steps into
the future. In the second problem, trace patterning, the agent needs to learn
patterns in the observation signals and remember them for future predictions.
We show that our proposed methods can effectively learn the agent state online
and produce accurate predictions.",2
564,"Semantic representation is of great benefit to the video text tracking(VTT)
task that requires simultaneously classifying, detecting, and tracking texts in
the video. Most existing approaches tackle this task by appearance similarity
in continuous frames, while ignoring the abundant semantic features. In this
paper, we explore to robustly track video text with contrastive learning of
semantic and visual representations. Correspondingly, we present an end-to-end
video text tracker with Semantic and Visual Representations(SVRep), which
detects and tracks texts by exploiting the visual and semantic relationships
between different texts in a video sequence. Besides, with a light-weight
architecture, SVRep achieves state-of-the-art performance while maintaining
competitive inference speed. Specifically, with a backbone of ResNet-18, SVRep
achieves an ${\rm ID_{F1}}$ of $\textbf{65.9\%}$, running at $\textbf{16.7}$
FPS, on the ICDAR2015(video) dataset with $\textbf{8.6\%}$ improvement than the
previous state-of-the-art methods.",2
565,"Autism Spectrum Disorder (ASD) is found to be a major concern among various
occupational therapists. The foremost challenge of this neurodevelopmental
disorder lies in the fact of analyzing and exploring various symptoms of the
children at their early stage of development. Such early identification could
prop up the therapists and clinicians to provide proper assistive support to
make the children lead an independent life. Facial expressions and emotions
perceived by the children could contribute to such early intervention of
autism. In this regard, the paper implements in identifying basic facial
expression and exploring their emotions upon a time variant factor. The
emotions are analyzed by incorporating the facial expression identified through
CNN using 68 landmark points plotted on the frontal face with a prediction
network formed by RNN known as RCNN-FER system. The paper adopts R-CNN to take
the advantage of increased accuracy and performance with decreased time
complexity in predicting emotion as a textual network analysis. The papers
proves better accuracy in identifying the emotion in autistic children when
compared over simple machine learning models built for such identifications
contributing to autistic society.",2
566,"Various computer simulations regarding, e.g., the weather or structural
mechanics, solve complex problems on a two-dimensional domain. They mostly do
so by splitting the input domain into a finite set of smaller and simpler
elements on which the simulation can be run fast and efficiently. This process
of splitting can be automatized by using subdivision schemes.
  Given the wide range of simulation problems to be tackled, an equally wide
range of subdivision schemes is available. They create subdivisions that are
(mainly) comprised of triangles, quadrilaterals, or hexagons. Furthermore, they
ensure that (almost) all vertices have the same number of neighboring vertices.
  This paper illustrates a subdivision scheme that splits the input domain into
pentagons. Repeated application of the scheme gives rise to fractal-like
structures. Furthermore, the resulting subdivided domain admits to certain
weaving patterns. These patterns are subsequently generalized to several other
subdivision schemes.
  As a final contribution, we provide paper models illustrating the weaving
patterns induced by the pentagonal subdivision scheme. Furthermore, we present
a jigsaw puzzle illustrating both the subdivision process and the induced
weaving pattern. These transform the visual and abstract mathematical
algorithms into tactile objects that offer exploration possibilities aside from
the visual.",2
567,"Fine-tuning pre-trained language models for downstream tasks has become a
norm for NLP. Recently it is found that intermediate training based on
high-level inference tasks such as Question Answering (QA) can improve the
performance of some language models for target tasks. However it is not clear
if intermediate training generally benefits various language models. In this
paper, using the SQuAD-2.0 QA task for intermediate training for target text
classification tasks, we experimented on eight tasks for single-sequence
classification and eight tasks for sequence-pair classification using two base
and two compact language models. Our experiments show that QA-based
intermediate training generates varying transfer performance across different
language models, except for similar QA tasks.",2
568,"Learner corpus collects language data produced by L2 learners, that is second
or foreign-language learners. This resource is of great relevance for second
language acquisition research, foreign-language teaching, and automatic
grammatical error correction. However, there is little focus on learner corpus
for Chinese as Foreign Language (CFL) learners. Therefore, we propose to
construct a large-scale, multidimensional annotated Chinese learner corpus. To
construct the corpus, we first obtain a large number of topic-rich texts
generated by CFL learners. Then we design an annotation scheme including a
sentence acceptability score as well as grammatical error and fluency-based
corrections. We build a crowdsourcing platform to perform the annotation
effectively (https://yaclc.wenmind.net). We name the corpus YACLC (Yet Another
Chinese Learner Corpus) and release it as part of the CUGE benchmark
(http://cuge.baai.ac.cn). By analyzing the original sentences and annotations
in the corpus, we found that YACLC has a considerable size and very high
annotation quality. We hope this corpus can further enhance the studies on
Chinese International Education and Chinese automatic grammatical error
correction.",2
569,"We consider the following problem: Given a set S of at most n elements from a
universe of size m, represent it in memory as a bit string so that membership
queries of the form ""Is x in S?"" can be answered by making at most t probes
into the bit string. Let s(m,n,t) be the minimum number of bits needed by any
such scheme. We obtain new upper bounds for s(m,n,t=2), which match or improve
all the previously known bounds. We also consider the quantum version of this
problem and obtain improved upper bounds.",2
570,"The most standard description of symmetries of a mathematical structure
produces a group. However, when the definition of this structure is motivated
by physics, or information theory, etc., the respective symmetry objects might
become more sophisticated: quasigroups, loops, quantum groups, ... In this
paper, we introduce and study quantum symmetries of very general categorical
structures: operads. Its initial motivation were spaces of probability
distributions on finite sets. We also investigate here how structures of
quantum information, such as quantum states and some constructions of quantum
codes are algebras over operads.",2
571,"The objective of this paper is to assess the performances of dimensionality
reduction techniques to establish a link between cryptocurrencies. We have
focused our analysis on the two most traded cryptocurrencies: Bitcoin and
Ethereum. To perform our analysis, we took log returns and added some
covariates to build our data set. We first introduced the pearson correlation
coefficient in order to have a preliminary assessment of the link between
Bitcoin and Ethereum. We then reduced the dimension of our data set using
canonical correlation analysis and principal component analysis. After
performing an analysis of the links between Bitcoin and Ethereum with both
statistical techniques, we measured their performance on forecasting Ethereum
returns with Bitcoin s features.",2
572,"While recent work on conjugate gradient methods and Lanczos decompositions
have achieved scalable Gaussian process inference with highly accurate point
predictions, in several implementations these iterative methods appear to
struggle with numerical instabilities in learning kernel hyperparameters, and
poor test likelihoods. By investigating CG tolerance, preconditioner rank, and
Lanczos decomposition rank, we provide a particularly simple prescription to
correct these issues: we recommend that one should use a small CG tolerance
($\epsilon \leq 0.01$) and a large root decomposition size ($r \geq 5000$).
Moreover, we show that L-BFGS-B is a compelling optimizer for Iterative GPs,
achieving convergence with fewer gradient updates.",2
573,"In this paper, discrete linear quadratic regulator (DLQR) and iterative
linear quadratic regulator (ILQR) methods based on high-order Runge-Kutta (RK)
discretization are proposed for solving linear and nonlinear quadratic optimal
control problems respectively. As discovered in [W. Hager, Runge-Kutta method
in optimal control and the discrete adjoint system, Numer. Math.,2000, pp.
247-282], direct approach with RK discretization is equivalent with indirect
approach based on symplectic partitioned Runge-Kutta (SPRK) integration. In
this paper, we will reconstruct this equivalence by the analogue of continuous
and discrete dynamic programming. Then, based on the equivalence, we discuss
the issue that the internal-stage controls produced by direct approach may have
lower order accuracy than the RK method used. We propose order conditions for
internal-stage controls and then demonstrate that third or fourth order
explicit RK discretization cannot avoid the order reduction phenomenon. To
overcome this obstacle, we calculate node control instead of internal-stage
controls in DLQR and ILQR methods. And numerical examples will illustrate the
validity of our methods. Another advantage of our methods is high computational
efficiency which comes from the usage of feedback technique. In this paper, we
also demonstrate that ILQR is essentially a quasi-Newton method with linear
convergence rate.",2
574,"The second edition of Deep Learning Interviews is home to hundreds of
fully-solved problems, from a wide range of key topics in AI. It is designed to
both rehearse interview or exam specific topics and provide machine learning
MSc / PhD. students, and those awaiting an interview a well-organized overview
of the field. The problems it poses are tough enough to cut your teeth on and
to dramatically improve your skills-but they're framed within thought-provoking
questions and engaging stories. That is what makes the volume so specifically
valuable to students and job seekers: it provides them with the ability to
speak confidently and quickly on any relevant topic, to answer technical
questions clearly and correctly, and to fully understand the purpose and
meaning of interview questions and answers. Those are powerful, indispensable
advantages to have when walking into the interview room. The book's contents is
a large inventory of numerous topics relevant to DL job interviews and graduate
level exams. That places this work at the forefront of the growing trend in
science to teach a core set of practical mathematical and computational skills.
It is widely accepted that the training of every computer scientist must
include the fundamental theorems of ML, and AI appears in the curriculum of
nearly every university. This volume is designed as an excellent reference for
graduates of such programs.",2
575,"Recent years have witnessed an unprecedented surge of interest, from social
networks to drug discovery, in learning representations of graph-structured
data. However, graph neural networks, the machine learning models for handling
graph-structured data, face significant challenges when running on conventional
digital hardware, including von Neumann bottleneck incurred by physically
separated memory and processing units, slowdown of Moore's law due to
transistor scaling limit, and expensive training cost. Here we present a novel
hardware-software co-design, the random resistor array-based echo state graph
neural network, which addresses these challenges. The random resistor arrays
not only harness low-cost, nanoscale and stackable resistors for highly
efficient in-memory computing using simple physical laws, but also leverage the
intrinsic stochasticity of dielectric breakdown to implement random projections
in hardware for an echo state network that effectively minimizes the training
cost thanks to its fixed and random weights. The system demonstrates
state-of-the-art performance on both graph classification using the MUTAG and
COLLAB datasets and node classification using the CORA dataset, achieving
34.2x, 93.2x, and 570.4x improvement of energy efficiency and 98.27%, 99.46%,
and 95.12% reduction of training cost compared to conventional graph learning
on digital hardware, respectively, which may pave the way for the next
generation AI system for graph learning.",2
576,"Objective: The paper focuses on development of robust and accurate processing
solutions for continuous and cuff-less blood pressure (BP) monitoring. In this
regard, a robust deep learning-based framework is proposed for computation of
low latency, continuous, and calibration-free upper and lower bounds on the
systolic and diastolic BP. Method: Referred to as the BP-Net, the proposed
framework is a novel convolutional architecture that provides longer effective
memory while achieving superior performance due to incorporation of casual
dialated convolutions and residual connections. To utilize the real potential
of deep learning in extraction of intrinsic features (deep features) and
enhance the long-term robustness, the BP-Net uses raw Electrocardiograph (ECG)
and Photoplethysmograph (PPG) signals without extraction of any form of
hand-crafted features as it is common in existing solutions. Results: By
capitalizing on the fact that datasets used in recent literature are not
unified and properly defined, a benchmark dataset is constructed from the
MIMIC-I and MIMIC-III databases obtained from PhysioNet. The proposed BP-Net is
evaluated based on this benchmark dataset demonstrating promising performance
and shows superior generalizable capacity. Conclusion: The proposed BP-Net
architecture is more accurate than canonical recurrent networks and enhances
the long-term robustness of the BP estimation task. Significance: The proposed
BP-Net architecture addresses key drawbacks of existing BP estimation
solutions, i.e., relying heavily on extraction of hand-crafted features, such
as pulse arrival time (PAT), and; Lack of robustness. Finally, the constructed
BP-Net dataset provides a unified base for evaluation and comparison of deep
learning-based BP estimation algorithms.",2
577,"We present an open-source toolkit for neural machine translation (NMT). The
new toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along
with many other improvements detailed below, in order to create a
self-contained, simple to use, consistent and comprehensive framework for
Machine Translation tasks of various domains. It is tooled to support both
bilingual and multilingual translation tasks, starting from building the model
from respective corpora, to inferring new predictions or packaging the model to
serving-capable JIT format.",2
578,"Turbulence simulation with classical numerical solvers requires very
high-resolution grids to accurately resolve dynamics. Here we train learned
simulators at low spatial and temporal resolutions to capture turbulent
dynamics generated at high resolution. We show that our proposed model can
simulate turbulent dynamics more accurately than classical numerical solvers at
the same low resolutions across various scientifically relevant metrics. Our
model is trained end-to-end from data and is capable of learning a range of
challenging chaotic and turbulent dynamics at low resolution, including
trajectories generated by the state-of-the-art Athena++ engine. We show that
our simpler, general-purpose architecture outperforms various more specialized,
turbulence-specific architectures from the learned turbulence simulation
literature. In general, we see that learned simulators yield unstable
trajectories; however, we show that tuning training noise and temporal
downsampling solves this problem. We also find that while generalization beyond
the training distribution is a challenge for learned models, training noise,
convolutional architectures, and added loss constraints can help. Broadly, we
conclude that our learned simulator outperforms traditional solvers run on
coarser grids, and emphasize that simple design choices can offer stability and
robust generalization.",2
579,"Nowadays, intelligent systems and services are getting increasingly popular
as they provide data-driven solutions to diverse real-world problems, thanks to
recent breakthroughs in Artificial Intelligence (AI) and Machine Learning (ML).
However, machine learning meets software engineering not only with promising
potentials but also with some inherent challenges. Despite some recent research
efforts, we still do not have a clear understanding of the challenges of
developing ML-based applications and the current industry practices. Moreover,
it is unclear where software engineering researchers should focus their efforts
to better support ML application developers. In this paper, we report about a
survey that aimed to understand the challenges and best practices of ML
application development. We synthesize the results obtained from 80
practitioners (with diverse skills, experience, and application domains) into
17 findings; outlining challenges and best practices for ML application
development. Practitioners involved in the development of ML-based software
systems can leverage the summarized best practices to improve the quality of
their system. We hope that the reported challenges will inform the research
community about topics that need to be investigated to improve the engineering
process and the quality of ML-based applications.",2
580,"Domain adaptation tasks such as cross-domain sentiment classification aim to
utilize existing labeled data in the source domain and unlabeled or few labeled
data in the target domain to improve the performance in the target domain via
reducing the shift between the data distributions. Existing cross-domain
sentiment classification methods need to distinguish pivots, i.e., the
domain-shared sentiment words, and non-pivots, i.e., the domain-specific
sentiment words, for excellent adaptation performance. In this paper, we first
design a Category Attention Network (CAN), and then propose a model named
CAN-CNN to integrate CAN and a Convolutional Neural Network (CNN). On the one
hand, the model regards pivots and non-pivots as unified category attribute
words and can automatically capture them to improve the domain adaptation
performance; on the other hand, the model makes an attempt at interpretability
to learn the transferred category attribute words. Specifically, the
optimization objective of our model has three different components: 1) the
supervised classification loss; 2) the distributions loss of category feature
weights; 3) the domain invariance loss. Finally, the proposed model is
evaluated on three public sentiment analysis datasets and the results
demonstrate that CAN-CNN can outperform other various baseline methods.",2
581,"Besides entity-centric knowledge, usually organized as Knowledge Graph (KG),
events are also an essential kind of knowledge in the world, which trigger the
spring up of event-centric knowledge representation form like Event KG (EKG).
It plays an increasingly important role in many machine learning and artificial
intelligence applications, such as intelligent search, question-answering,
recommendation, and text generation. This paper provides a comprehensive survey
of EKG from history, ontology, instance, and application views. Specifically,
to characterize EKG thoroughly, we focus on its history, definitions, schema
induction, acquisition, related representative graphs/systems, and
applications. The development processes and trends are studied therein. We
further summarize perspective directions to facilitate future research on EKG.",2
582,"In the last decade, many deep learning models have been well trained and made
a great success in various fields of machine intelligence, especially for
computer vision and natural language processing. To better leverage the
potential of these well-trained models in intra-domain or cross-domain transfer
learning situations, knowledge distillation (KD) and domain adaptation (DA) are
proposed and become research highlights. They both aim to transfer useful
information from a well-trained model with original training data. However, the
original data is not always available in many cases due to privacy, copyright
or confidentiality. Recently, the data-free knowledge transfer paradigm has
attracted appealing attention as it deals with distilling valuable knowledge
from well-trained models without requiring to access to the training data. In
particular, it mainly consists of the data-free knowledge distillation (DFKD)
and source data-free domain adaptation (SFDA). On the one hand, DFKD aims to
transfer the intra-domain knowledge of original data from a cumbersome teacher
network to a compact student network for model compression and efficient
inference. On the other hand, the goal of SFDA is to reuse the cross-domain
knowledge stored in a well-trained source model and adapt it to a target
domain. In this paper, we provide a comprehensive survey on data-free knowledge
transfer from the perspectives of knowledge distillation and unsupervised
domain adaptation, to help readers have a better understanding of the current
research status and ideas. Applications and challenges of the two areas are
briefly reviewed, respectively. Furthermore, we provide some insights to the
subject of future research.",2
583,"In the classical theory of cubic interpolation splines there exists an
algorithm which works with only $O\left( n\right)$ arithmetic operations. Also,
the smoothing cubic splines may be computed via the algorithm of Reinsch which
reduces their computation to interpolation cubic splines and also performs with
$O\left( n\right)$ arithmetic operations. In this paper it is shown that many
features of the polynomial cubic spline setting carry over to the larger class
of $L$-splines where $L$ is a linear differential operator of order $4$ with
constant coefficients. Criteria are given such that the associated matrix $R$
is strictly diagonally dominant which implies the existence of a fast algorithm
for interpolation.",2
584,"Human mobility data accumulated from Point-of-Interest (POI) check-ins
provides great opportunity for user behavior understanding. However, data
quality issues (e.g., geolocation information missing, unreal check-ins, data
sparsity) in real-life mobility data limit the effectiveness of existing
POI-oriented studies, e.g., POI recommendation and location prediction, when
applied to real applications. To this end, in this paper, we develop a model,
named Bi-STDDP, which can integrate bi-directional spatio-temporal dependence
and users' dynamic preferences, to identify the missing POI check-in where a
user has visited at a specific time. Specifically, we first utilize
bi-directional global spatial and local temporal information of POIs to capture
the complex dependence relationships. Then, target temporal pattern in
combination with user and POI information are fed into a multi-layer network to
capture users' dynamic preferences. Moreover, the dynamic preferences are
transformed into the same space as the dependence relationships to form the
final model. Finally, the proposed model is evaluated on three large-scale
real-world datasets and the results demonstrate significant improvements of our
model compared with state-of-the-art methods. Also, it is worth noting that the
proposed model can be naturally extended to address POI recommendation and
location prediction tasks with competitive performances.",2
585,"Conventional methods for the image-text generation tasks mainly tackle the
naturally bidirectional generation tasks separately, focusing on designing
task-specific frameworks to improve the quality and fidelity of the generated
samples. Recently, Vision-Language Pre-training models have greatly improved
the performance of the image-to-text generation tasks, but large-scale
pre-training models for text-to-image synthesis task are still under-developed.
In this paper, we propose ERNIE-ViLG, a unified generative pre-training
framework for bidirectional image-text generation with transformer model. Based
on the image quantization models, we formulate both image generation and text
generation as autoregressive generative tasks conditioned on the text/image
input. The bidirectional image-text generative modeling eases the semantic
alignments across vision and language. For the text-to-image generation
process, we further propose an end-to-end training method to jointly learn the
visual sequence generator and the image reconstructor. To explore the landscape
of large-scale pre-training for bidirectional text-image generation, we train a
10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million
(Chinese) image-text pairs which achieves state-of-the-art performance for both
text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for
text-to-image synthesis and best results on COCO-CN and AIC-ICC for image
captioning.",2
586,"The applicability of the swarm robots to perform foraging tasks is inspired
by their compact size and cost. A considerable amount of energy is required to
perform such tasks, especially if the tasks are continuous and/or repetitive.
Real-world situations in which robots perform tasks continuously while staying
alive (survivability) and maximizing production (performance) require energy
awareness. This paper proposes an energy-conscious distributed task allocation
algorithm to solve continuous tasks (e.g., unlimited foraging) for cooperative
robots to achieve highly effective missions. We consider efficiency as a
function of the energy consumed by the robot during exploration and collection
when food is returned to the collection bin. Finally, the proposed
energy-efficient algorithm minimizes the total transit time to the charging
station and time consumed while recharging and maximizes the robot's lifetime
to perform maximum tasks to enhance the overall efficiency of collaborative
robots. We evaluated the proposed solution against a typical greedy
benchmarking strategy (assigning the closest collection bin to the available
robot and recharging the robot at maximum) for efficiency and performance in
various scenarios. The proposed approach significantly improved performance and
efficiency over the baseline approach.",2
587,"Each grid block in a 3D geological model requires a rock type that represents
all physical and chemical properties of that block. The properties that
classify rock types are lithology, permeability, and capillary pressure.
Scientists and engineers determined these properties using conventional
laboratory measurements, which embedded destructive methods to the sample or
altered some of its properties (i.e., wettability, permeability, and porosity)
because the measurements process includes sample crushing, fluid flow, or fluid
saturation. Lately, Digital Rock Physics (DRT) has emerged to quantify these
properties from micro-Computerized Tomography (uCT) and Magnetic Resonance
Imaging (MRI) images. However, the literature did not attempt rock typing in a
wholly digital context. We propose performing Digital Rock Typing (DRT) by: (1)
integrating the latest DRP advances in a novel process that honors digital rock
properties determination, while; (2) digitalizing the latest rock typing
approaches in carbonate, and (3) introducing a novel carbonate rock typing
process that utilizes computer vision capabilities to provide more insight
about the heterogeneous carbonate rock texture.",2
588,"Many concurrent dictionary implementations are designed and optimized for
read-mostly workloads with uniformly distributed keys, and often perform poorly
on update-heavy workloads. In this work, we first present a concurrent
(a,b)-tree, the OCC-ABtree, which outperforms its fastest competitor by up to
2x on uniform update-heavy workloads, and is competitive on other workloads. We
then turn our attention to skewed update-heavy workloads (which feature many
inserts/deletes on the same key) and introduce the Elim-ABtree, which uses a
new optimization called publishing elimination. In publishing elimination,
concurrent inserts and deletes to a key are reordered to eliminate them. This
reduces the number of writes in the data structure. The Elim-ABtree achieves up
to 2.5x the performance of its fastest competitor (including the OCC-ABtree).
The OCC-ABtree and Elim-ABtree are linearizable. We also introduce durable
linearizable versions (for systems with Intel Optane DCPMM non-volatile main
memory) that are nearly as fast.",2
589,"We study relationship between first order multiplicative linear logic (MLL1),
which has been known to provide representations to different categorial
grammars, and the recently introduced extended tensor type calculus (ETTC). We
identify a fragment of MLL1, which seems sufficient for many grammar
representations, and establish a correspondence between ETTC and this fragment.
The system ETTC, thus, can be seen as an alternative syntax and intrinsic
deductive system together with a geometric representation for the latter. We
also give a natural deduction formulation of ETTC, which might be convenient.",2
590,"""Benign overfitting"", where classifiers memorize noisy training data yet
still achieve a good generalization performance, has drawn great attention in
the machine learning community. To explain this surprising phenomenon, a series
of works have provided theoretical justification in over-parameterized linear
regression, classification, and kernel methods. However, it is not clear if
benign overfitting still occurs in the presence of adversarial examples, i.e.,
examples with tiny and intentional perturbations to fool the classifiers. In
this paper, we show that benign overfitting indeed occurs in adversarial
training, a principled approach to defend against adversarial examples. In
detail, we prove the risk bounds of the adversarially trained linear classifier
on the mixture of sub-Gaussian data under $\ell_p$ adversarial perturbations.
Our result suggests that under moderate perturbations, adversarially trained
linear classifiers can achieve the near-optimal standard and adversarial risks,
despite overfitting the noisy training data. Numerical experiments validate our
theoretical findings.",2
591,"This paper proposes an efficient federated distillation learning system
(EFDLS) for multi-task time series classification (TSC). EFDLS consists of a
central server and multiple mobile users, where different users may run
different TSC tasks. EFDLS has two novel components, namely a feature-based
student-teacher (FBST) framework and a distance-based weights matching (DBWM)
scheme. Within each user, the FBST framework transfers knowledge from its
teacher's hidden layers to its student's hidden layers via knowledge
distillation, with the teacher and student having identical network structure.
For each connected user, its student model's hidden layers' weights are
uploaded to the EFDLS server periodically. The DBWM scheme is deployed on the
server, with the least square distance used to measure the similarity between
the weights of two given models. This scheme finds a partner for each connected
user such that the user's and its partner's weights are the closest among all
the weights uploaded. The server exchanges and sends back the user's and its
partner's weights to these two users which then load the received weights to
their teachers' hidden layers. Experimental results show that the proposed
EFDLS achieves excellent performance on a set of selected UCR2018 datasets
regarding top-1 accuracy.",2
592,"In this paper we present our workflow to automatically reconstruct 3D
building models based on 2D building polygons and a LiDAR point cloud. The
workflow generates models at different levels of detail (LoDs) to support data
requirements of different applications from one consistent source. Specific
attention has been paid to make the workflow robust to quickly run a new
iteration in case of improvements in an algorithm or in case new input data
become available. The quality of the reconstructed data highly depends on the
quality of the input data and is monitored in several steps of the process. A
3D viewer has been developed to view and download the openly available 3D data
at different LoDs in different formats. The workflow has been applied to all 10
million buildings of The Netherlands. The 3D service will be updated after new
input data becomes available.",2
593,"Corrupted labels and class imbalance are commonly encountered in practically
collected training data, which easily leads to over-fitting of deep neural
networks (DNNs). Existing approaches alleviate these issues by adopting a
sample re-weighting strategy, which is to re-weight sample by designing
weighting function. However, it is only applicable for training data containing
only either one type of data biases. In practice, however, biased samples with
corrupted labels and of tailed classes commonly co-exist in training data. How
to handle them simultaneously is a key but under-explored problem. In this
paper, we find that these two types of biased samples, though have similar
transient loss, have distinguishable trend and characteristics in loss curves,
which could provide valuable priors for sample weight assignment. Motivated by
this, we delve into the loss curves and propose a novel probe-and-allocate
training strategy: In the probing stage, we train the network on the whole
biased training data without intervention, and record the loss curve of each
sample as an additional attribute; In the allocating stage, we feed the
resulting attribute to a newly designed curve-perception network, named
CurveNet, to learn to identify the bias type of each sample and assign proper
weights through meta-learning adaptively. The training speed of meta learning
also blocks its application. To solve it, we propose a method named skip layer
meta optimization (SLMO) to accelerate training speed by skipping the bottom
layers. Extensive synthetic and real experiments well validate the proposed
method, which achieves state-of-the-art performance on multiple challenging
benchmarks.",2
594,"We study the forecasting problem for traffic with dynamic, possibly
periodical, and joint spatial-temporal dependency between regions. Given the
aggregated inflow and outflow traffic of regions in a city from time slots 0 to
t-1, we predict the traffic at time t at any region. Prior arts in the area
often consider the spatial and temporal dependencies in a decoupled manner or
are rather computationally intensive in training with a large number of
hyper-parameters to tune. We propose ST-TIS, a novel, lightweight, and accurate
Spatial-Temporal Transformer with information fusion and region sampling for
traffic forecasting. ST-TIS extends the canonical Transformer with information
fusion and region sampling. The information fusion module captures the complex
spatial-temporal dependency between regions. The region sampling module is to
improve the efficiency and prediction accuracy, cutting the computation
complexity for dependency learning from $O(n^2)$ to $O(n\sqrt{n})$, where n is
the number of regions. With far fewer parameters than state-of-the-art models,
the offline training of our model is significantly faster in terms of tuning
and computation (with a reduction of up to $90\%$ on training time and network
parameters). Notwithstanding such training efficiency, extensive experiments
show that ST-TIS is substantially more accurate in online prediction than
state-of-the-art approaches (with an average improvement of up to $9.5\%$ on
RMSE, and $12.4\%$ on MAPE).",2
595,"Brain signals constitute the information that are processed by millions of
brain neurons (nerve cells and brain cells). These brain signals can be
recorded and analyzed using various of non-invasive techniques such as the
Electroencephalograph (EEG), Magneto-encephalograph (MEG) as well as
brain-imaging techniques such as Magnetic Resonance Imaging (MRI), Computed
Tomography (CT) and others, which will be discussed briefly in this paper. This
paper discusses about the currently emerging techniques such as the usage of
different Deep Learning (DL) algorithms for the analysis of these brain signals
and how these algorithms will be helpful in determining the neurological status
of a person by applying the signal decoding strategy.",2
596,"There is a general trend of applying reinforcement learning (RL) techniques
for traffic signal control (TSC). Recently, most studies pay attention to the
neural network design and rarely concentrate on the state representation. Does
the design of state representation has a good impact on TSC? In this paper, we
(1) propose an effective state representation as queue length of vehicles with
intensive knowledge; (2) present a TSC method called MaxQueue based on our
state representation approach; (3) develop a general RL-based TSC template
called QL-XLight with queue length as state and reward and generate QL-FRAP,
QL-CoLight, and QL-DQN by our QL-XLight template based on traditional and
latest RL models.Through comprehensive experiments on multiple real-world
datasets, we demonstrate that: (1) our MaxQueue method outperforms the latest
RL based methods; (2) QL-FRAP and QL-CoLight achieves a new state-of-the-art
(SOTA). In general, state representation with intensive knowledge is also
essential for TSC methods. Our code is released on Github.",2
597,"This paper quantifies the quality of heatmap-based eXplainable AI methods
w.r.t image classification problem. Here, a heatmap is considered desirable if
it improves the probability of predicting the correct classes. Different XAI
heatmap-based methods are empirically shown to improve classification
confidence to different extents depending on the datasets, e.g. Saliency works
best on ImageNet and Deconvolution on ChestX-Ray Pneumonia dataset. The novelty
includes a new gap distribution that shows a stark difference between correct
and wrong predictions. Finally, the generative augmentative explanation is
introduced, a method to generate heatmaps maps capable of improving predictive
confidence to a high level.",2
598,"Inferring reward functions from demonstrations and pairwise preferences are
auspicious approaches for aligning Reinforcement Learning (RL) agents with
human intentions. However, state-of-the art methods typically focus on learning
a single reward model, thus rendering it difficult to trade off different
reward functions from multiple experts. We propose Multi-Objective Reinforced
Active Learning (MORAL), a novel method for combining diverse demonstrations of
social norms into a Pareto-optimal policy. Through maintaining a distribution
over scalarization weights, our approach is able to interactively tune a deep
RL agent towards a variety of preferences, while eliminating the need for
computing multiple policies. We empirically demonstrate the effectiveness of
MORAL in two scenarios, which model a delivery and an emergency task that
require an agent to act in the presence of normative conflicts. Overall, we
consider our research a step towards multi-objective RL with learned rewards,
bridging the gap between current reward learning and machine ethics literature.",2
599,"Reconfigurable intelligent surface (RIS) is very promising for wireless
networks to achieve high energy efficiency, extended coverage, improved
capacity, massive connectivity, etc. To unleash the full potentials of
RIS-aided communications, acquiring accurate channel state information is
crucial, which however is very challenging. For RIS-aided multiple-input and
multiple-output (MIMO) communications, the existing channel estimation methods
have computational complexity growing rapidly with the number of RIS units $N$
(e.g., in the order of $N^2$ or $N^3$) and/or have special requirements on the
matrices involved (e.g., the matrices need to be sparse for algorithm
convergence to achieve satisfactory performance), which hinder their
applications. In this work, instead of using the conventional signal model in
the literature, we derive a new signal model obtained through proper
vectorization and reduction operations. Then, leveraging the unitary
approximate message passing (UAMP), we develop a more efficient channel
estimator that has complexity linear with $N$ and does not have special
requirements on the relevant matrices, thanks to the robustness of UAMP. These
facilitate the applications of the proposed algorithm to a general RIS-aided
MIMO system with a larger $N$. Moreover, extensive numerical results show that
the proposed estimator delivers much better performance and/or requires
significantly less number of training symbols, thereby leading to notable
reductions in both training overhead and latency.",2
